{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Index Analyse et g\u00e9olocalisation des arr\u00eat\u00e9s de p\u00e9rils sur le territoire de la m\u00e9tropole Aix-Marseille Provence. Installation Installation commune: Ubuntu (natif ou Windows Subsystem for Linux) Dans le terminal WSL (ou l'invite de commande Ubuntu): Installer le mod\u00e8le de Tesseract pour le fran\u00e7ais sudo apt update sudo apt install tesseract-ocr-fra Installer Mambaforge : curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\" bash Mambaforge-$(uname)-$(uname -m).sh Cr\u00e9er un environnement virtuel conda \u00e0 partir du fichier de sp\u00e9cifications environment-prod.yml sh mamba env create --file environment-prod.yml Installer ocrmypdf (maintenant que ses d\u00e9pendances ont \u00e9t\u00e9 install\u00e9es \u00e0 la cr\u00e9ation de l'environnement). conda activate geo-arretes # installer ocrmypdf qui ne pouvait pas \u00eatre install\u00e9 en m\u00eame temps que ses d\u00e9pendances... mamba install ocrmypdf # depuis le dossier o\u00f9 se trouve le code source du projet pip install -e . # d\u00e9sactiver et r\u00e9activer l'environnement virtuel car tesseract, install\u00e9 par ocrmypdf, # a d\u00e9pos\u00e9 ses fichiers de langage dans un sous-dossier # `$HOME/mambaforge/envs/geo-arretes/share/tessdata` # (sinon ils ne seront visibles...) conda deactivate R\u00e9solution de probl\u00e8mes A la cr\u00e9ation de l'environnement conda, si l'installation d'un paquet \u00e9choue avec une erreur \u00e9trange, eg. le paquet vs2015_runtime , suivre la proc\u00e9dure dans https://stackoverflow.com/a/65728405 Sur des serveurs Windows 2019 ou plus anciens, il peut \u00eatre n\u00e9cessaire d'installer le WSL via une proc\u00e9dure manuelle d\u00e9crite ici . En r\u00e9sum\u00e9 il faut : Activer la fonctionnalit\u00e9 Windows \"Windows Subsystem for Linux\" via la commande Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux . Un red\u00e9marrage du syst\u00e8me est n\u00e9cessaire. T\u00e9l\u00e9charger la distribution Ubuntu 20.04 Renommer le fichier .AppxBundle t\u00e9l\u00e9charg\u00e9 en .zip : Rename-Item .\\CanonicalGroupLimited.UbunutuonWindows_2004.2021.825.0.AppxBundle .\\ubuntu.zip Extraire le fichier zip dans un dossier ubuntu : Expand-Archive .\\ubuntu.zip .\\ubuntu Se d\u00e9placer dans le dossier ubuntu : cd .\\ubuntu Lancer l'archive correspondant \u00e0 votre architecture (par exemple, x64) : Expand-Archive .\\Ubuntu_2004.2021.825.0_x64.zip .\\ubuntu Ajouter le dossier Ubuntu \u00e0 la variable d'environnement PATH : $userenv = [System.Environment]::GetEnvironmentVariable(\"Path\", \"User\") puis [System.Environment]::SetEnvironmentVariable(\"PATH\", $userenv + \";D:\\Logiciels\\Ubuntu\\Ubuntu\", \"User\") Red\u00e9marrer le terminal PowerShell, l'ouvrir en tant qu'administrateur Lancer le fichier ubuntu.exe contenu dans le dossier Ubuntu : .\\ubuntu\\ubuntu\\ubuntu.exe Utilisation Dans le terminal WSL (ou l'invite de commande Ubuntu): Les dossier d'entr\u00e9es et de sorties sont configurables dans le fichier scripts/process.sh . conda activate geo-arretes scripts/process.sh Project layout data/ # Datasets raw/ # The original, immutable data dump. processed/ # The final, canonical data sets for modeling. interim/ # Intermediate data that has been transformed. external/ # Data from third party sources. docs/ # Documentation index.md # The documentation homepage. ... # Other markdown pages, images and other files . ... # that follow the same structure as the project. notebooks/ # Jupyter notebooks explore_actes.ipynb # TODO test_pds_image.ipynb # TODO src/ # Source code of this project. domain_knowledge/ # Regex pattern and dictionaries used for data extraction. preprocess/ # Functions for preprocessing PDF files. process/ # Functions for processing PDF files. quality/ # Functions for quality control. utils/ # Utility functions. .gitignore # Specifies intentionally untracked files to ignore. environment-prod.yml # Conda environment file for production. environment.yml # Conda environment file for development. LICENSE # MIT Licence. README.md # The top level README for developers using this project. setup.py # Make this project pip installable with `pip install -e`","title":"Index"},{"location":"#index","text":"Analyse et g\u00e9olocalisation des arr\u00eat\u00e9s de p\u00e9rils sur le territoire de la m\u00e9tropole Aix-Marseille Provence.","title":"Index"},{"location":"#installation","text":"","title":"Installation"},{"location":"#installation-commune-ubuntu-natif-ou-windows-subsystem-for-linux","text":"Dans le terminal WSL (ou l'invite de commande Ubuntu): Installer le mod\u00e8le de Tesseract pour le fran\u00e7ais sudo apt update sudo apt install tesseract-ocr-fra Installer Mambaforge : curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\" bash Mambaforge-$(uname)-$(uname -m).sh Cr\u00e9er un environnement virtuel conda \u00e0 partir du fichier de sp\u00e9cifications environment-prod.yml sh mamba env create --file environment-prod.yml Installer ocrmypdf (maintenant que ses d\u00e9pendances ont \u00e9t\u00e9 install\u00e9es \u00e0 la cr\u00e9ation de l'environnement). conda activate geo-arretes # installer ocrmypdf qui ne pouvait pas \u00eatre install\u00e9 en m\u00eame temps que ses d\u00e9pendances... mamba install ocrmypdf # depuis le dossier o\u00f9 se trouve le code source du projet pip install -e . # d\u00e9sactiver et r\u00e9activer l'environnement virtuel car tesseract, install\u00e9 par ocrmypdf, # a d\u00e9pos\u00e9 ses fichiers de langage dans un sous-dossier # `$HOME/mambaforge/envs/geo-arretes/share/tessdata` # (sinon ils ne seront visibles...) conda deactivate","title":"Installation commune: Ubuntu (natif ou Windows Subsystem for Linux)"},{"location":"#resolution-de-problemes","text":"A la cr\u00e9ation de l'environnement conda, si l'installation d'un paquet \u00e9choue avec une erreur \u00e9trange, eg. le paquet vs2015_runtime , suivre la proc\u00e9dure dans https://stackoverflow.com/a/65728405 Sur des serveurs Windows 2019 ou plus anciens, il peut \u00eatre n\u00e9cessaire d'installer le WSL via une proc\u00e9dure manuelle d\u00e9crite ici . En r\u00e9sum\u00e9 il faut : Activer la fonctionnalit\u00e9 Windows \"Windows Subsystem for Linux\" via la commande Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux . Un red\u00e9marrage du syst\u00e8me est n\u00e9cessaire. T\u00e9l\u00e9charger la distribution Ubuntu 20.04 Renommer le fichier .AppxBundle t\u00e9l\u00e9charg\u00e9 en .zip : Rename-Item .\\CanonicalGroupLimited.UbunutuonWindows_2004.2021.825.0.AppxBundle .\\ubuntu.zip Extraire le fichier zip dans un dossier ubuntu : Expand-Archive .\\ubuntu.zip .\\ubuntu Se d\u00e9placer dans le dossier ubuntu : cd .\\ubuntu Lancer l'archive correspondant \u00e0 votre architecture (par exemple, x64) : Expand-Archive .\\Ubuntu_2004.2021.825.0_x64.zip .\\ubuntu Ajouter le dossier Ubuntu \u00e0 la variable d'environnement PATH : $userenv = [System.Environment]::GetEnvironmentVariable(\"Path\", \"User\") puis [System.Environment]::SetEnvironmentVariable(\"PATH\", $userenv + \";D:\\Logiciels\\Ubuntu\\Ubuntu\", \"User\") Red\u00e9marrer le terminal PowerShell, l'ouvrir en tant qu'administrateur Lancer le fichier ubuntu.exe contenu dans le dossier Ubuntu : .\\ubuntu\\ubuntu\\ubuntu.exe","title":"R\u00e9solution de probl\u00e8mes"},{"location":"#utilisation","text":"Dans le terminal WSL (ou l'invite de commande Ubuntu): Les dossier d'entr\u00e9es et de sorties sont configurables dans le fichier scripts/process.sh . conda activate geo-arretes scripts/process.sh","title":"Utilisation"},{"location":"#project-layout","text":"data/ # Datasets raw/ # The original, immutable data dump. processed/ # The final, canonical data sets for modeling. interim/ # Intermediate data that has been transformed. external/ # Data from third party sources. docs/ # Documentation index.md # The documentation homepage. ... # Other markdown pages, images and other files . ... # that follow the same structure as the project. notebooks/ # Jupyter notebooks explore_actes.ipynb # TODO test_pds_image.ipynb # TODO src/ # Source code of this project. domain_knowledge/ # Regex pattern and dictionaries used for data extraction. preprocess/ # Functions for preprocessing PDF files. process/ # Functions for processing PDF files. quality/ # Functions for quality control. utils/ # Utility functions. .gitignore # Specifies intentionally untracked files to ignore. environment-prod.yml # Conda environment file for production. environment.yml # Conda environment file for development. LICENSE # MIT Licence. README.md # The top level README for developers using this project. setup.py # Make this project pip installable with `pip install -e`","title":"Project layout"},{"location":"Code%20Source/domain_knowledge/","text":"Domain Knowledge Definit les expressions r\u00e9guli\u00e8res et les dictionnaires utilis\u00e9s pour l'extraction des donn\u00e9es. Actes Traces de t\u00e9l\u00e9transmission de documents par @ctes. Tampon et page d'accus\u00e9 de r\u00e9ception. is_accusedereception_page(page_txt) D\u00e9tecte si une page contient un accus\u00e9 de r\u00e9ception. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient un tampon de transmission src\\domain_knowledge\\actes.py def is_accusedereception_page(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un accus\u00e9 de r\u00e9ception. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un tampon de transmission \"\"\" return P_ACCUSE.search(page_txt) is not None is_stamped_page(page_txt) D\u00e9tecte si une page contient un tampon (encadr\u00e9) de transmission @actes. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient un tampon de transmission src\\domain_knowledge\\actes.py def is_stamped_page(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un tampon (encadr\u00e9) de transmission @actes. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un tampon de transmission \"\"\" return P_STAMP.search(page_txt) is not None Adresse Reconnaissance et traitement des adresses. create_adresse_normalisee(adr_num, adr_ind, adr_voie, adr_compl, adr_cpostal, adr_ville) Cr\u00e9er une adresse normalis\u00e9e. L'adresse normalis\u00e9e rassemble les champs extraits de l'adresse brute, et ailleurs dans le document si n\u00e9cessaire (eg. autorit\u00e9 prenant l'arr\u00eat\u00e9, template). Le compl\u00e9ment d'adresse est ignor\u00e9. Parameters adr_num: str Num\u00e9ro de l'adresse adr_ind: str Indice de l'adresse adr_voie: str Nom de la voie (incluant le type) adr_compl: str Compl\u00e9ment d'adresse adr_cpostal: str Code postal adr_ville: str Commune Returns adr_norm: str Adresse normalis\u00e9e src\\domain_knowledge\\adresse.py def create_adresse_normalisee( adr_num: str, adr_ind: str, adr_voie: str, adr_compl: str, adr_cpostal: str, adr_ville: str, ) -> str: \"\"\"Cr\u00e9er une adresse normalis\u00e9e. L'adresse normalis\u00e9e rassemble les champs extraits de l'adresse brute, et ailleurs dans le document si n\u00e9cessaire (eg. autorit\u00e9 prenant l'arr\u00eat\u00e9, template). Le compl\u00e9ment d'adresse est ignor\u00e9. Parameters ---------- adr_num: str Num\u00e9ro de l'adresse adr_ind: str Indice de l'adresse adr_voie: str Nom de la voie (incluant le type) adr_compl: str Compl\u00e9ment d'adresse adr_cpostal: str Code postal adr_ville: str Commune Returns ------- adr_norm: str Adresse normalis\u00e9e \"\"\" # logique BAN: tous les champs sont s\u00e9par\u00e9s par une espace, sauf le num\u00e9ro de voie et l'indice de r\u00e9p\u00e9tition: # (ex: 6a rue Victor Hugo 13001 Marseille) (rmq_iteration) adr_num_ind = \"\".join( x for x in [ adr_num, adr_ind, ] if pd.notna(x) ) # tout concat\u00e9ner, sauf le compl\u00e9ment d'adresse adr_norm = \" \".join( x for x in [ adr_num_ind, adr_voie, # adr_compl, # TODO normaliser: accents etc? adr_cpostal, adr_ville, ] if pd.notna(x) ) return adr_norm normalize_adresse(adresse) Normalise les champs d'adresse. Les formes normales de chaque champ sont: - indice de r\u00e9p\u00e9tition en minuscules, - voie en minuscules, - ville en forme canonique tir\u00e9e du fichier des codes communes INSEE. Les espaces superflues ont normalement \u00e9t\u00e9 supprim\u00e9es en amont. Parameters adresse: Dict[str, str] Adresse dont les champs sont bruts. Returns adresse_norm: Dict[str, str] Adresse dont les champs sont normalis\u00e9s. src\\domain_knowledge\\adresse.py def normalize_adresse(adresse: Dict[str, str]) -> Dict[str, str]: \"\"\"Normalise les champs d'adresse. Les formes normales de chaque champ sont: - indice de r\u00e9p\u00e9tition en minuscules, - voie en minuscules, - ville en forme canonique tir\u00e9e du fichier des codes communes INSEE. Les espaces superflues ont normalement \u00e9t\u00e9 supprim\u00e9es en amont. Parameters ---------- adresse: Dict[str, str] Adresse dont les champs sont bruts. Returns ------- adresse_norm: Dict[str, str] Adresse dont les champs sont normalis\u00e9s. \"\"\" # normalisation simple des champs (redondant et inutile ?) adresse_norm = { k: ( normalize_string(v, num=True, apos=True, hyph=True, spaces=True) if pd.notna(v) else v ) for k, v in adresse.items() } # indice de r\u00e9p\u00e9tition: mettre en minuscules if pd.notna(adresse_norm[\"ind\"]): adresse_norm[\"ind\"] = adresse_norm[\"ind\"].lower() # voie: mettre en minuscules if pd.notna(adresse_norm[\"voie\"]): adresse_norm[\"voie\"] = adresse_norm[\"voie\"].lower() # compl\u00e9ment: mettre en minuscules (2023-06-26 FL) if pd.notna(adresse_norm[\"compl\"]): adresse_norm[\"compl\"] = adresse_norm[\"compl\"].lower() # code postal: tel quel (d\u00e9j\u00e0 normalis\u00e9 en amont: espace entre code d\u00e9partement et le reste) # ville: enlever le num\u00e9ro d'arrondissement (Marseille), ex: \"Marseille 1er\" ; # la forme a d\u00e9j\u00e0 \u00e9t\u00e9 normalis\u00e9e (ou apr\u00e8s? TODO si apr\u00e8s, remettre la normalisation ici?) if pd.notna(adresse_norm[\"ville\"]): adresse_norm[\"ville\"] = re.sub( P_ARRONDISSEMENTS, \"\", adresse_norm[\"ville\"] ).strip() # return adresse_norm process_adresse_brute(adr_ad_brute) Extraire une ou plusieurs adresses d'une adresse brute. Chaque adresse comporte diff\u00e9rents champs: num\u00e9ro, indicateur, voie, (\u00e9ventuellement complement d'adresse,) code postal, commune. Parameters adr_ad_brute: str Adresse brute Returns adresses: list(dict) Liste d'adresses src\\domain_knowledge\\adresse.py def process_adresse_brute(adr_ad_brute: str) -> List[Dict]: \"\"\"Extraire une ou plusieurs adresses d'une adresse brute. Chaque adresse comporte diff\u00e9rents champs: num\u00e9ro, indicateur, voie, (\u00e9ventuellement complement d'adresse,) code postal, commune. Parameters ---------- adr_ad_brute: str Adresse brute Returns ------- adresses: list(dict) Liste d'adresses \"\"\" if adr_ad_brute is None: adr_fields = { \"adr_num\": None, \"adr_ind\": None, \"adr_voie\": None, \"adr_compl\": None, \"adr_cpostal\": None, \"adr_ville\": None, } # TODO liste contenant un seul dict aux champs tous None, ou liste vide (\u00e0 g\u00e9rer) ? return [adr_fields] adresses = [] # ajouter une but\u00e9e droite pour le lookahead # FIXME contournement sale adr_ad_brute = adr_ad_brute + \" - \" m_adresse = P_ADRESSE_NG.match(adr_ad_brute) # was: \".search()\" if m_adresse: logging.warning(f\"process_adresse_brute: match: {m_adresse.groupdict()}\") # si aucune adresse extraite, on renvoie aussi une liste contenant une unique adresse vide if not m_adresse: logging.error(f\"aucune adresse extraite de {adr_ad_brute} par P_ADRESSE_NG\") # TODO factoriser avec le cas adr_ad_brute is None adr_fields = { \"adr_num\": None, \"adr_ind\": None, \"adr_voie\": None, \"adr_compl\": None, \"adr_cpostal\": None, \"adr_ville\": None, } # TODO liste contenant un seul dict aux champs tous None, ou liste vide (\u00e0 g\u00e9rer) ? return [adr_fields] logging.warning( f\"process_adresse_brute: {m_adresse.group(0)}\\n{m_adresse.groups()}\\n{m_adresse.groupdict()}\" ) # r\u00e9cup\u00e9rer les champs communs \u00e0 toutes les adresses group\u00e9es: compl\u00e9ment, # code postal et commune adr_compl = \" / \".join( m_adresse[x].strip() for x in [\"compl_ini\", \"compl_fin\"] if m_adresse[x] ) # FIXME concat? if adr_compl: logging.warning( f\"compl\u00e9ment d'adresse trouv\u00e9, pr\u00e9: {m_adresse['compl_ini']} ; post: {m_adresse['compl_fin']} dans adr_ad_brute: {adr_ad_brute}\" ) cpostal = m_adresse[\"code_postal\"] if cpostal: # code postal: supprimer une \u00e9ventuelle espace apr\u00e8s le d\u00e9partement (ex: 13 001) cpostal = cpostal.replace(\" \", \"\") commune = m_adresse[\"commune\"] if commune: # commune: remplacer par la forme canonique (pour les communes AMP) commune = normalize_ville(commune) # traitement sp\u00e9cifique pour la voie: type + nom (legacy?) # adr_voie = m_adresse[\"voie\"].strip() # if adr_voie == \"\": # adr_voie = None # extraire la ou les adresses courtes, et les s\u00e9parer s'il y en a plusieurs # on est oblig\u00e9 de r\u00e9extraire depuis l'adresse brute, car le RE_VOIE est d\u00e9fini # avec un contexte droit (positive lookahead) # (p\u00e9nible, mais pour le moment \u00e7a fonctionne comme \u00e7a) adr_lists = list(P_NUM_IND_VOIE_LIST.finditer(adr_ad_brute)) # obligatoire: une liste d'adresses courtes (ou une adresse courte) try: assert len(adr_lists) >= 1 except AssertionError: raise ValueError(f\"Aucune adresse courte d\u00e9tect\u00e9e dans {adr_ad_brute}\") # on v\u00e9rifie qu'on travaille exactement au m\u00eame endroit, pour se positionner au bon endroit try: assert adr_lists[0].group(0) == m_adresse[\"num_ind_voie_list\"] except AssertionError: logging.warning( f\"Probl\u00e8me sur {m_adresse.groupdict()}\\nadr_list.group(0): {adr_lists[0].group(0)} ; {m_adresse['num_ind_voie_list']}\" ) \"\"\"raise ValueError( f\"Probl\u00e8me sur {m_adresse.groupdict()}\\nadr_list.group(0): {adr_lists[0].group(0)} ; {m_adresse['num_ind_voie_list']}\" )\"\"\" for adr_list in adr_lists: # on ne peut pas compl\u00e8tement verrouiller avec adr_list.end(), car il manquerait \u00e0 nouveau le contexte droit (grmpf) adrs = list(P_NUM_IND_VOIE_NG.finditer(adr_ad_brute, adr_list.start())) if not adrs: raise ValueError(f\"Aucune adresse NUM_IND_VOIE trouv\u00e9e dans {adr_list}\") for adr in adrs: # pour chaque adresse courte, # - r\u00e9cup\u00e9rer la voie voie = adr[\"voie\"] # - r\u00e9cup\u00e9rer la liste (optionnelle) de num\u00e9ros et d'indicateurs (optionnels) num_ind_list = adr[\"num_ind_list\"] if not num_ind_list: # pas de liste de num\u00e9ros et indicateurs: logging.warning(f\"adresse courte en voie seule: {adr.group(0)}\") # ajouter une adresse sans num\u00e9ro (ni indicateur) adr_fields = { \"adr_num\": None, \"adr_ind\": None, \"adr_voie\": voie, \"adr_compl\": adr_compl, \"adr_cpostal\": cpostal, \"adr_ville\": commune, } adresses.append(adr_fields) else: # on a une liste de num\u00e9ros (et \u00e9ventuellement indicateurs) num_inds = list(P_NUM_IND.finditer(num_ind_list)) if len(num_inds) > 1: logging.warning(f\"plusieurs num\u00e9ros et indicateurs: {num_inds}\") for num_ind in num_inds: # pour chaque num\u00e9ro et \u00e9ventuel indicateur num_ind_str = num_ind.group(0) # extraire le num\u00e9ro m_nums = list(P_NUM_VOIE.finditer(num_ind_str)) assert len(m_nums) == 1 num = m_nums[0].group(0) # extraire le ou les \u00e9ventuels indicateurs m_inds = list(P_IND_VOIE.finditer(num_ind_str)) if not m_inds: # pas d'indicateur: adresse avec juste un num\u00e9ro adr_fields = { \"adr_num\": num, \"adr_ind\": None, \"adr_voie\": voie, \"adr_compl\": adr_compl, \"adr_cpostal\": cpostal, \"adr_ville\": commune, } adresses.append(adr_fields) else: # au moins un indicateur if len(m_inds) > 1: logging.warning(f\"plusieurs indicateurs: {m_inds}\") for m_ind in m_inds: # pour chaque indicateur, adresse avec num\u00e9ro et indicateur ind = m_ind.group(0) adr_fields = { \"adr_num\": num, \"adr_ind\": ind, \"adr_voie\": voie, \"adr_compl\": adr_compl, \"adr_cpostal\": cpostal, \"adr_ville\": commune, } adresses.append(adr_fields) # WIP code postal disparait if (cpostal is None) and P_CP.search(adr_ad_brute): # WIP survient pour les adresses doubles: la fin de la 2e adresse est envoy\u00e9e en commune # TODO d\u00e9tecter et analyser sp\u00e9cifiquement les adresses doubles logging.warning( f\"aucun code postal extrait de {adr_ad_brute}: {m_adresse.groupdict()}\" ) # end WIP code postal return adresses Agence immobili\u00e8re Reconnaissance des noms d'agences immobili\u00e8res. Certains noms de syndics incluent \"syndic\", les capturer explicitement avant le motif g\u00e9n\u00e9ral permet d'\u00e9viter les conflits. Lister les syndics connus peut acc\u00e9l\u00e9rer et mieux focaliser la capture. normalize_nom_cabinet(nom_cab) Normalise un nom de cabinet. La version actuelle requiert une d\u00e9claration explicite dans LISTE_NOMS_CABINETS, mais des traitements de normalisation standard pourraient \u00eatre d\u00e9finis en compl\u00e9ment. Parameters nom_cab: str Nom du cabinet ou de l'agence. Returns nom_nor: str Nom normalis\u00e9. src\\domain_knowledge\\agences_immo.py def normalize_nom_cabinet(nom_cab: str) -> str: \"\"\"Normalise un nom de cabinet. La version actuelle requiert une d\u00e9claration explicite dans LISTE_NOMS_CABINETS, mais des traitements de normalisation standard pourraient \u00eatre d\u00e9finis en compl\u00e9ment. Parameters ---------- nom_cab: str Nom du cabinet ou de l'agence. Returns ------- nom_nor: str Nom normalis\u00e9. \"\"\" if nom_cab is None: return None # for re_nom, norm in LISTE_NOMS_CABINETS.items(): # d\u00e8s qu'on a un match sur un nom de cabinet, on renvoie la forme normalis\u00e9e if re.search(re_nom, nom_cab, flags=(re.IGNORECASE | re.MULTILINE)): return norm else: # si aucun match, on renvoie le nom en entr\u00e9e tel quel return nom_cab Arr\u00e9t\u00e9s de collectivit\u00e9 territoriale Structure d'un arr\u00eat\u00e9 de collectivit\u00e9 territoriale. contains_arrete(page_txt) D\u00e9tecte si une page contient ARRET(E|ONS). Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient ARRET(E|ONS) src\\domain_knowledge\\arrete.py def contains_arrete(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient ARRET(E|ONS). Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient ARRET(E|ONS) \"\"\" return P_ARRETONS.search(page_txt) is not None contains_article(page_txt) D\u00e9tecte si une page contient un Article. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient un Article src\\domain_knowledge\\arrete.py def contains_article(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un Article. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un Article \"\"\" return P_ARTICLE.search(page_txt) is not None contains_considerant(page_txt) D\u00e9tecte si une page contient un CONSIDERANT. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient un CONSIDERANT src\\domain_knowledge\\arrete.py def contains_considerant(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un CONSIDERANT. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un CONSIDERANT \"\"\" return P_CONSIDERANT.search(page_txt) is not None contains_vu(page_txt) D\u00e9tecte si une page contient un VU. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient un VU src\\domain_knowledge\\arrete.py def contains_vu(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un VU. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un VU \"\"\" return P_VU.search(page_txt) is not None get_commune_maire(page_txt) Extrait le nom de la commune pr\u00e9c\u00e9d\u00e9 de la mention du maire. Parameters page_txt: str Texte d'une page de document Returns nom_commune: str | None Nom de la commune si le texte contient une mention du maire, None sinon. src\\domain_knowledge\\arrete.py def get_commune_maire(page_txt: str) -> bool: \"\"\"Extrait le nom de la commune pr\u00e9c\u00e9d\u00e9 de la mention du maire. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- nom_commune: str | None Nom de la commune si le texte contient une mention du maire, None sinon. \"\"\" if match_mc := P_MAIRE_COMMUNE.search(page_txt): com_maire = match_mc.group(\"commune\") # nettoyage de la valeur r\u00e9cup\u00e9r\u00e9e if m_com_cln := M_MAIRE_COMMUNE_CLEANUP.search(com_maire): com_maire = m_com_cln.group(\"maire_commune\") return com_maire else: return None get_date(page_txt) R\u00e9cup\u00e8re la date de l'arr\u00eat\u00e9. Actuellement, correspond \u00e0 la date de signature, en fin d'arr\u00eat\u00e9. Parameters page_txt: str Texte d'une page de document Returns doc_date: str Date du document si trouv\u00e9e, None sinon. src\\domain_knowledge\\arrete.py def get_date(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re la date de l'arr\u00eat\u00e9. Actuellement, correspond \u00e0 la date de signature, en fin d'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_date: str Date du document si trouv\u00e9e, None sinon. \"\"\" if m_date_d := P_DATE_SIGNAT.search(page_txt): return m_date_d.group(\"arr_date\") else: return None get_nom(page_txt) R\u00e9cup\u00e8re le nom de l'arr\u00eat\u00e9. Parameters page_txt: str Texte d'une page de document Returns doc_nom: str Nom de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\arrete.py def get_nom(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re le nom de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_nom: str Nom de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" # TODO nettoyer \u00e0 gauche (\"Dossier suivi par\") et \u00e0 droite: \"\\nNous,\" if m_nom := P_NOM_ARR.search(page_txt): return m_nom.group(\"nom_arr\") else: return None get_num(page_txt) R\u00e9cup\u00e8re le num\u00e9ro de l'arr\u00eat\u00e9. Parameters page_txt: str Texte d'une page de document Returns doc_num: str Num\u00e9ro de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\arrete.py def get_num(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re le num\u00e9ro de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_num: str Num\u00e9ro de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" if m_num := P_NUM_ARR.search(page_txt): return m_num.group(\"num_arr\") elif m_num_fb := P_NUM_ARR_FALLBACK.search(page_txt): return m_num_fb.group(\"num_arr\") else: return None R\u00e9f\u00e9rences cadastrales Reconnaissance et analyse de r\u00e9f\u00e9rences cadastrales. generate_refcadastrale_norm(codeinsee, refcad, arr_pdf, adr_cpostal) G\u00e9n\u00e8re une r\u00e9f\u00e9rence cadastrale normalis\u00e9e \u00e0 une entr\u00e9e. Parameters codeinsee: string Code INSEE de la commune. refcad: string R\u00e9f\u00e9rence cadastrale brute. arr_pdf: string Nom du fichier PDF (pour exception) adr_cpostal: string Code postal de la commune Returns refcad: string R\u00e9f\u00e9rence cadastrale normalis\u00e9e. src\\domain_knowledge\\cadastre.py def generate_refcadastrale_norm( codeinsee: str, refcad: str, arr_pdf: str, adr_cpostal: str ) -> str: \"\"\"G\u00e9n\u00e8re une r\u00e9f\u00e9rence cadastrale normalis\u00e9e \u00e0 une entr\u00e9e. Parameters ---------- codeinsee: string Code INSEE de la commune. refcad: string R\u00e9f\u00e9rence cadastrale brute. arr_pdf: string Nom du fichier PDF (pour exception) adr_cpostal: string Code postal de la commune Returns ------- refcad: string R\u00e9f\u00e9rence cadastrale normalis\u00e9e. \"\"\" # ajouter le pr\u00e9fixe du code insee # TODO cas particulier pour Marseille: code commune par ardt + code quartier if pd.isna(codeinsee): codeinsee = \"\" # TODO v\u00e9rifier si le comportement qui en d\u00e9coule est ok (identifiant court, \u00e0 compl\u00e9ter manuellement par le code insee) # prendre la r\u00e9f\u00e9rence locale (commune) if pd.isna(refcad): refcad = None elif m_mars := P_CAD_MARSEILLE_NG.search(refcad): # on ne garde que le 1er match # TODO g\u00e9rer 2 ou plusieurs r\u00e9f\u00e9rences cadastrales # Marseille: code insee arrondissement + code quartier (3 chiffres) + section + parcelle arrt = m_mars[\"arrt\"] if not arrt and not (codeinsee and codeinsee != \"13055\"): # ni arrondissement ni code INSEE (diff\u00e9rent de celui de tout Marseille)=> g\u00e9n\u00e9rer une r\u00e9f\u00e9rence cadastrale courte refcad = f\"{m_mars['quar']}{m_mars['sec']:>02}{m_mars['num']:>04}\" logging.error( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale incompl\u00e8te (num\u00e9ro d'arrondissement manquant \u00e0 Marseille): {refcad}\" ) else: # arrondissement ou code INSEE if codeinsee and codeinsee != \"13055\": # on a bien un code INSEE (diff\u00e9rent du code INSEE pour tout Marseille), on peut donc l'utiliser if arrt: # si on a aussi un num\u00e9ro d'arrondissement, on v\u00e9rifie que les deux sont coh\u00e9rents try: assert codeinsee[-3:] == arrt except AssertionError: # FIXME am\u00e9liorer le warning ; \u00e9crire une expectation sur le dataset final # 2023-03-06: 16 conflits logging.warning( f\"{arr_pdf}: conflit entre code INSEE ({codeinsee}, via code postal {adr_cpostal}) et r\u00e9f\u00e9rence cadastrale {arrt}\" ) else: # on n'a un code d'arrondissement: reconstruire un code INSEE codeinsee = f\"13{arrt}\" # g\u00e9n\u00e9rer une r\u00e9f\u00e9rence cadastrale compl\u00e8te refcad = ( f\"{codeinsee}{m_mars['quar']}{m_mars['sec']:>02}{m_mars['num']:>04}\" ) # le code de section devrait \u00eatre en majuscules ; \u00e9mettre un warning sinon # TODO ajouter au rapport d'erreur (r\u00e9f normalis\u00e9e produite + str en entr\u00e9e) if not m_mars[\"sec\"].isupper(): logging.warning( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale suspecte (code de section): {refcad}\" ) elif m_autr := P_CAD_AUTRES_NG.search(refcad): # hors Marseille: code insee commune + 000 + section + parcelle codequartier = \"000\" refcad = f\"{codeinsee}{codequartier}{m_autr['sec']:>02}{m_autr['num']:>04}\" # 2 v\u00e9rifications: # - le code de section devrait \u00eatre en majuscules ; \u00e9mettre un warning sinon # TODO ajouter au rapport d'erreur (r\u00e9f normalis\u00e9e produite + str en entr\u00e9e) if not m_autr[\"sec\"].isupper(): logging.warning( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale suspecte (code de section): {refcad}\" ) # - si le code INSEE est en fait un code INSEE d'un arrondissement de Marseille, # mais aucun code quartier n'a \u00e9t\u00e9 rep\u00e9r\u00e9 donc la r\u00e9f\u00e9rence cadastrale lue est # une r\u00e9f\u00e9rence courte, comme dans d'autres communes # TODO ajouter au rapport d'erreur (r\u00e9f normalis\u00e9e produite + str en entr\u00e9e) if codeinsee and (int(codeinsee) in range(13201, 13216)): logging.warning( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale suspecte (Marseille sans code quartier): {refcad}\" ) else: refcad = None return refcad get_parcelles(page_txt) R\u00e9cup\u00e8re la ou les r\u00e9f\u00e9rences de parcelles cadastrales. Parameters page_txt: str Texte d'une page de document Returns id_parcelles: List[str] R\u00e9f\u00e9rences d'une ou plusieurs parcelles cadastrales si d\u00e9tect\u00e9es dans le texte, liste vide sinon. src\\domain_knowledge\\cadastre.py def get_parcelles(page_txt: str) -> List[str]: \"\"\"R\u00e9cup\u00e8re la ou les r\u00e9f\u00e9rences de parcelles cadastrales. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- id_parcelles: List[str] R\u00e9f\u00e9rences d'une ou plusieurs parcelles cadastrales si d\u00e9tect\u00e9es dans le texte, liste vide sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW id_parcelles = [] # r\u00e9sultat # WIP chercher le ou les empans distincts contenant au moins une r\u00e9f\u00e9rence \u00e0 une parcelle if matches := list(P_PARCELLE.finditer(page_txt)): logging.warning( f\"{len(matches)} empans PARCELLE: {[x.group(0) for x in matches]}\" ) # WIP extraire plusieurs r\u00e9f\u00e9rences for m_parc in matches: # liste des identifiants de parcelles m_cad_str = m_parc.group(\"cadastre_id\") # WIP if m_parcs_mrs := list( P_CAD_MARSEILLE_NG.finditer( page_txt, m_parc.start(\"cadastre_id\"), m_parc.end(\"cadastre_id\") ) ): # essayer d'abord de rep\u00e9rer des r\u00e9f\u00e9rences Marseille, plus longues et qui peuvent g\u00e9n\u00e9rer de faux positifs si analys\u00e9es # comme des r\u00e9f\u00e9rences hors Marseille (ex: \"208837 D0607 ET 208837 D0290\" => \"ET 2088\" serait rep\u00e9r\u00e9 comme une parcelle...) m_parcs = m_parcs_mrs if len(m_parcs) > 1: logging.warning( f\"{len(m_parcs)} parcelles (Marseille 1) dans {m_cad_str}: {[x.group(0) for x in m_parcs]}\" ) elif m_parcs_aut := list( P_CAD_SECNUM.finditer( page_txt, m_parc.start(\"cadastre_id\"), m_parc.end(\"cadastre_id\") ) ): # sinon essayer de rep\u00e9rer des r\u00e9f\u00e9rences d'autres communes m_parcs = m_parcs_aut if len(m_parcs) > 1: logging.warning( f\"{len(m_parcs)} parcelles (toutes communes) dans {m_cad_str}: {[x.group(0) for x in m_parcs]}\" ) else: raise ValueError( f\"Pas de r\u00e9f\u00e9rence retrouv\u00e9e dans la zone? {m_cad_str}\" ) # end WIP # RESUME HERE ! 2023-04-28 id_parcelles.append(m_cad_str) elif ( False ): # matches := list(P_PARCELLE_MARSEILLE_NOCONTEXT.finditer(page_txt)): # WIP logging.warning( f\"{len(matches)} empans PARC_MRS: {[x.group(0) for x in matches]}\" ) for m_parc_mrs in matches: # liste des identifiants de parcelles m_cad_str = m_parc_mrs.group(\"cadastre_id\") # WIP m_parcs = list( P_CAD_MARSEILLE_NG.finditer( page_txt, m_parc_mrs.start(\"cadastre_id\"), m_parc_mrs.end(\"cadastre_id\"), ) ) if len(m_parcs) > 1: logging.warning( f\"{len(m_parcs)} parcelles (Marseille 2) dans {m_cad_str}: {[x.group(0) for x in m_parcs]}\" ) # end WIP id_parcelles.append(m_cad_str) return id_parcelles Cadre r\u00e9glementaire R\u00e9f\u00e9rences au cadre r\u00e9glementaire. contains_cc(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code Civil. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code Civil. src\\domain_knowledge\\cadre_reglementaire.py def contains_cc(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code Civil. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code Civil. \"\"\" return P_CC.search(page_txt) is not None contains_cc_art(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil. src\\domain_knowledge\\cadre_reglementaire.py def contains_cc_art(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil. \"\"\" return P_CC_ART.search(page_txt) is not None contains_cch(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation. \"\"\" return P_CCH.search(page_txt) is not None contains_cch_L111(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L111(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L111.search(page_txt) is not None contains_cch_L511(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L511(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L511.search(page_txt) is not None contains_cch_L521(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L521(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L521.search(page_txt) is not None contains_cch_L541(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L541(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L541.search(page_txt) is not None contains_cch_R511(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_R511(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_R511.search(page_txt) is not None contains_cgct(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. src\\domain_knowledge\\cadre_reglementaire.py def contains_cgct(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. \"\"\" return P_CGCT.search(page_txt) is not None contains_cgct_art(page_txt) D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. Parameters page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. src\\domain_knowledge\\cadre_reglementaire.py def contains_cgct_art(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. \"\"\" return P_CGCT_ART.search(page_txt) is not None parse_refs_reglement(txt_body, span_beg, span_end) Rep\u00e8re dans un texte des r\u00e9f\u00e9rences au cadre r\u00e9glementaire. Parameters txt_body: string Corps de texte \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser. Returns content: list Liste d'empans de r\u00e9f\u00e9rences src\\domain_knowledge\\cadre_reglementaire.py def parse_refs_reglement(txt_body: str, span_beg: int, span_end: int) -> list: \"\"\"Rep\u00e8re dans un texte des r\u00e9f\u00e9rences au cadre r\u00e9glementaire. Parameters ---------- txt_body: string Corps de texte \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser. Returns ------- content: list Liste d'empans de r\u00e9f\u00e9rences \"\"\" content = [] for p_reg, typ_reg in REG_TYP: if matches := p_reg.finditer(txt_body, span_beg, span_end): for match in matches: content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(0), \"span_typ\": typ_reg, } ) return content Code g\u00e9ographiques Acc\u00e8s aux codes g\u00e9ographiques (codes INSEE, codes postaux) des communes. TODO cr\u00e9er des modules similaires pour les autres bases de connaissances: * les variantes de graphies des communes (TODO), * une liste de syndics (TODO). get_codeinsee(nom_commune, cpostal) R\u00e9cup\u00e9rer le code INSEE d'une commune. Le code postal est utilis\u00e9 pour les arrondissements de Marseille. Parameters nom_commune: string Nom de la commune cpostal: string or None Code postal, utile pour les arrondissements de Marseille Returns codeinsee: string Code INSEE de la commune. src\\domain_knowledge\\codes_geo.py def get_codeinsee(nom_commune: str, cpostal: str) -> str: \"\"\"R\u00e9cup\u00e9rer le code INSEE d'une commune. Le code postal est utilis\u00e9 pour les arrondissements de Marseille. Parameters ---------- nom_commune: string Nom de la commune cpostal: string or None Code postal, utile pour les arrondissements de Marseille Returns ------- codeinsee: string Code INSEE de la commune. \"\"\" if pd.isna(nom_commune): return None nom_commune = ( nom_commune.strip() ) # TODO s'assurer que strip() est fait en amont, \u00e0 l'extraction de la donn\u00e9e ? # v\u00e9rifier que nom_commune est une graphie d'une commune de la m\u00e9tropole try: assert P_COMMUNES_AMP_ALLFORMS.match(nom_commune) or nom_commune in ( \"la Gardanne\", ) # FIXME: arr\u00eat\u00e9s mal lus except AssertionError: # TODO d\u00e9tecter et exclure les communes hors M\u00e9tropole en amont? logging.warning( f\"Impossible de d\u00e9terminer le code INSEE pour {nom_commune}, hors m\u00e9tropole?\" ) # raise return None if ( nom_commune.lower().startswith(\"marseille\") and pd.notna(cpostal) and (cpostal in CP_MARSEILLE) ): # NB: c'est une approximation ! # TODO expectation: aucun codeinsee 13055 dans le dataset final (ou presque) codeinsee = \"132\" + cpostal[-2:] else: # TODO \u00e9prouver et am\u00e9liorer la robustesse codeinsee = COM2INSEE.get(simplify_commune(nom_commune), None) if not codeinsee: logging.warning( f\"get_codeinsee: pas de code trouv\u00e9 pour {(nom_commune, cpostal)} (simplify_commune={simplify_commune(nom_commune)}).\" ) return codeinsee get_codepostal(nom_commune, codeinsee) R\u00e9cup\u00e9rer le code postal d'une commune \u00e0 partir de son code INSEE. Attention, risque d'erreurs car certaines communes \u00e9tendues sont couvertes par plusieurs codes postaux: Marseille (1 par arrondissement, chaque arrondissement a aussi son COG) mais aussi Aix-en-Provence (1 COG mais 6 codes postaux: 13080, 13090, 13098, 13100, 13290, 13540), Martigues (codes postaux: 13117, 13500). TODO Le nom de la commune est-il utile? Parameters nom_commune: string Nom de la commune (inutile?) codeinsee: string or None Code INSEE. Returns cpostal: string Code postal de la commune. src\\domain_knowledge\\codes_geo.py def get_codepostal(nom_commune: str, codeinsee: str) -> str: \"\"\"R\u00e9cup\u00e9rer le code postal d'une commune \u00e0 partir de son code INSEE. Attention, risque d'erreurs car certaines communes \u00e9tendues sont couvertes par plusieurs codes postaux: Marseille (1 par arrondissement, chaque arrondissement a aussi son COG) mais aussi Aix-en-Provence (1 COG mais 6 codes postaux: 13080, 13090, 13098, 13100, 13290, 13540), Martigues (codes postaux: 13117, 13500). TODO Le nom de la commune est-il utile? Parameters ---------- nom_commune: string Nom de la commune (inutile?) codeinsee: string or None Code INSEE. Returns ------- cpostal: string Code postal de la commune. \"\"\" if pd.isna(nom_commune): return None nom_commune = ( nom_commune.strip() ) # TODO s'assurer que strip() est fait en amont, \u00e0 l'extraction de la donn\u00e9e ? # v\u00e9rifier que nom_commune est une graphie d'une commune de la m\u00e9tropole try: assert P_COMMUNES_AMP_ALLFORMS.match(nom_commune) or nom_commune in ( \"la Gardanne\", ) # FIXME: arr\u00eat\u00e9s mal lus except AssertionError: # TODO d\u00e9tecter et exclure les communes hors M\u00e9tropole en amont? logging.warning( f\"Impossible de d\u00e9terminer le code INSEE pour {nom_commune}, hors m\u00e9tropole?\" ) # raise return None if ( nom_commune.lower().startswith(\"marseille\") and pd.notna(codeinsee) and (codeinsee.startswith(\"132\")) # FIXME g\u00e9n\u00e9raliser/am\u00e9liorer? ): # NB: c'est une approximation ! # TODO expectation: aucun codeinsee 13055 dans le dataset final (ou presque) cpostal = \"130\" + codeinsee[-2:] # 2023-03-18: a priori, cela ne devrait rien changer car le code INSEE est d\u00e9termin\u00e9 \u00e0 partir du code postal pour les arrondissements de Marseille elif pd.notna(codeinsee) and (simplify_commune(nom_commune), codeinsee) in ( (\"aixenprovence\", \"13001\"), (\"martigues\", \"13056\"), ): cpostal = None # pour que create_adresse_normalisee() n'ait \u00e0 g\u00e9rer des valeurs pd.<NA> dont la valeur bool\u00e9enne est ambigue (alors que None est faux) logging.warning( f\"get_codepostal: abstention, plusieurs codes postaux possibles pour {(nom_commune, codeinsee)}.\" ) else: # TODO \u00e9prouver et am\u00e9liorer la robustesse cpostal = INSEE2POST.get(codeinsee, None) if pd.isna(cpostal): cpostal = None # pour que create_adresse_normalisee() n'ait \u00e0 g\u00e9rer des valeurs pd.<NA> dont la valeur bool\u00e9enne est ambigue (alors que None est faux) logging.warning( f\"get_codepostal: pas de code trouv\u00e9 pour {(nom_commune, codeinsee)}.\" ) return cpostal load_codes_insee_amp() Charger les codes INSEE des communes Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence. Returns df_insee: pd.DataFrame Liste des communes avec leur code INSEE. src\\domain_knowledge\\codes_geo.py def load_codes_insee_amp() -> pd.DataFrame: \"\"\"Charger les codes INSEE des communes Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence. Returns ------- df_insee: pd.DataFrame Liste des communes avec leur code INSEE. \"\"\" df_insee = pd.read_csv(FP_INSEE, dtype=DTYPE_INSEE) return df_insee load_codes_postaux_amp() Charger les codes postaux des communes, associ\u00e9s aux codes INSEE. Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence. Attention, le fichier actuel (2023-03-18) utilise un s\u00e9parateur \";\". Returns df_cpostal: pd.DataFrame Liste des codes postaux par (code INSEE de) commune. src\\domain_knowledge\\codes_geo.py def load_codes_postaux_amp() -> pd.DataFrame: \"\"\"Charger les codes postaux des communes, associ\u00e9s aux codes INSEE. Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence. Attention, le fichier actuel (2023-03-18) utilise un s\u00e9parateur \";\". Returns ------- df_cpostal: pd.DataFrame Liste des codes postaux par (code INSEE de) commune. \"\"\" df_cpostal = pd.read_csv(FP_CPOSTAL, dtype=DTYPE_CPOSTAL, sep=\";\") return df_cpostal normalize_ville(raw_ville) Normalise un nom de ville. Les formes reconnues par S_RE_COMMUNES_VARS sont r\u00e9\u00e9crites dans la forme canonique tir\u00e9e de DF_INSEE[\"commune\"] . Pour les villes absentes de cette ressource externe, le nom est renvoy\u00e9 tel quel. Parameters raw_ville: str Nom brut de la ville, extrait du document. Returns nor_ville: str Forme normale, canonique, du nom de ville. src\\domain_knowledge\\codes_geo.py def normalize_ville(raw_ville: str) -> str: \"\"\"Normalise un nom de ville. Les formes reconnues par `S_RE_COMMUNES_VARS` sont r\u00e9\u00e9crites dans la forme canonique tir\u00e9e de `DF_INSEE[\"commune\"]`. Pour les villes absentes de cette ressource externe, le nom est renvoy\u00e9 tel quel. Parameters ---------- raw_ville: str Nom brut de la ville, extrait du document. Returns ------- nor_ville: str Forme normale, canonique, du nom de ville. \"\"\" for p_ville, norm_ville in VILLE_PAT_NORM: if p_ville.match(raw_ville): return norm_ville else: # si toutes les possibilit\u00e9s ont \u00e9t\u00e9 \u00e9puis\u00e9es, # renvoyer la valeur en entr\u00e9e return raw_ville simplify_commune(com) Simplifier le nom d'une commune pour faciliter le matching. Parameters com: str Nom de la commune Returns com_simple: str Nom de la commune simplifi\u00e9 src\\domain_knowledge\\codes_geo.py def simplify_commune(com: str) -> str: \"\"\"Simplifier le nom d'une commune pour faciliter le matching. Parameters ---------- com: str Nom de la commune Returns ------- com_simple: str Nom de la commune simplifi\u00e9 \"\"\" # FIXME utiliser unicodedata.normalize return ( com.lower() .replace(\"\u00e0\", \"a\") .replace(\"\u00e7\", \"c\") .replace(\"\u00e9\", \"e\") .replace(\"\u00e8\", \"e\") .replace(\"\u00ea\", \"e\") .replace(\"\u00ee\", \"i\") .replace(\"\u00ef\", \"i\") .replace(\"\u00f4\", \"o\") .replace(\"\u00fb\", \"u\") .replace(\"\u00ff\", \"y\") .replace(\"-\", \"\") .replace(\" \", \"\") ) Relations entre documents. Les r\u00e9f\u00e9rence \u00e0 des documents pr\u00e9c\u00e9dents sont \u00e9nonc\u00e9es dans les \"Vu\". Mod\u00e8les de documents des arr\u00eat\u00e9s. Motifs de reconnaissance des en-t\u00eates, pieds-de-page et annexes. TODO - [ ] exploiter les \u00e9l\u00e9ments de template (discriminants) pour d\u00e9terminer la ville (en compl\u00e9ment des autres emplacements: autorit\u00e9, signature) Logement Rep\u00e9rage et extraction de donn\u00e9es propres aux arr\u00eat\u00e9s sur le logement. Propri\u00e9taire, gestionnaire, syndic ou administrateur, adresse de l'immeuble concern\u00e9. get_adr_doc(page_txt) Extrait la ou les adresses vis\u00e9es par l'arr\u00eat\u00e9. Parameters page_txt: str Texte d'une page de document Returns adresses: List[dict] La ou les adresses vis\u00e9es par l'arr\u00eat\u00e9, si trouv\u00e9es dans la page de texte. Pour chaque zone d'adresse brute, la ou les adresses extraites. src\\domain_knowledge\\logement.py def get_adr_doc(page_txt: str) -> bool: \"\"\"Extrait la ou les adresses vis\u00e9es par l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- adresses: List[dict] La ou les adresses vis\u00e9es par l'arr\u00eat\u00e9, si trouv\u00e9es dans la page de texte. Pour chaque zone d'adresse brute, la ou les adresses extraites. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # WIP au pr\u00e9alable, neutraliser les adresses des services municipaux if serv_mun := P_ADR_SERVICES_MUNI.search(page_txt): logging.warning(f\"service municipal remplac\u00e9: {serv_mun}\") page_txt = re.sub(P_ADR_SERVICES_MUNI, \"SERVICE_MUNICIPAL\", page_txt) adresses = [] if matches_adr := list(P_ADR_DOC.finditer(page_txt)): for m_adr in matches_adr: logging.warning( f\"adr_doc: {m_adr.group(0)}\\n{m_adr.groups()}\\n{m_adr.groupdict()}\" ) adr_brute = m_adr.group(\"adresse\") logging.warning(f\"adr_brute brute: {adr_brute}\") # nettoyer la valeur r\u00e9cup\u00e9r\u00e9e # - couper sur certains contextes droits if False: adr_brute = re.sub( RE_ADR_CLEANUP, \"\", adr_brute, flags=(re.MULTILINE | re.IGNORECASE) ) # RESUME HERE 2023-04-11 il faut r\u00e9ussir \u00e0 se passer du cleanup # - enlever l'\u00e9ventuelle ponctuation finale if adr_brute.endswith((\".\", \",\")): adr_brute = adr_brute[:-1] # - normaliser les graphies, les espaces etc adr_brute = normalize_string( adr_brute, num=True, apos=True, hyph=True, spaces=True ) # - extraire la ou les adresses pr\u00e9cises, d\u00e9compos\u00e9e en champs # (num\u00e9ro, indicateur, voie...) # WIP on prend le texte de la page, born\u00e9 \u00e0 gauche avec le d\u00e9but de l'adresse # mais pas born\u00e9 \u00e0 droite pour avoir le contexte droit (n\u00e9cessaire pour # les adresses courtes, car le nom de voie est born\u00e9 par un lookahead) try: adresses_proc = process_adresse_brute(adr_brute) except AssertionError: print( f\"get_adr_doc: process_adresse_brute({adr_brute})\\nmatch complet={m_adr.group(0)}\\ngroups={m_adr.groups()}\\ngroupdict={m_adr.groupdict()}\" ) raise adresses.append( { \"adresse_brute\": adr_brute, \"adresses\": adresses_proc, } ) return adresses get_gest(page_txt) D\u00e9tecte si une page contient un nom de gestionnaire immobilier. Parameters page_txt: str Texte d'une page de document Returns syndic: str Nom de gestionnaire si d\u00e9tect\u00e9, None sinon. src\\domain_knowledge\\logement.py def get_gest(page_txt: str) -> str: \"\"\"D\u00e9tecte si une page contient un nom de gestionnaire immobilier. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- syndic: str Nom de gestionnaire si d\u00e9tect\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW match = P_GEST.search(page_txt) return match.group(\"gestio\") if match is not None else None get_proprio(page_txt) Extrait le nom et l'adresse du propri\u00e9taire. Parameters page_txt: str Texte d'une page de document Returns syndic: str Nom et adresse du propri\u00e9taire si d\u00e9tect\u00e9, None sinon. src\\domain_knowledge\\logement.py def get_proprio(page_txt: str) -> bool: \"\"\"Extrait le nom et l'adresse du propri\u00e9taire. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- syndic: str Nom et adresse du propri\u00e9taire si d\u00e9tect\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW # on essaie d'abord de d\u00e9tecter un mono-propri\u00e9taire (WIP) if match := P_PROPRIO_MONO.search(page_txt): logging.warning(f\"mono-propri\u00e9taire: {match}\\n{match.group(0)}\") return match.group(\"proprio\") # puis sinon les multi-propri\u00e9taires (TODO proprement) elif match := P_PROPRIO.search(page_txt): logging.warning(f\"mono- ou multi-propri\u00e9taire: {match}\\n{match.group(0)}\") return match.group(\"proprio\") else: return None get_syndic(page_txt) D\u00e9tecte si une page contient un nom de syndic. Parameters page_txt: str Texte d'une page de document Returns syndic: str Nom de syndic si d\u00e9tect\u00e9, None sinon. src\\domain_knowledge\\logement.py def get_syndic(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un nom de syndic. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- syndic: str Nom de syndic si d\u00e9tect\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if m_synd := P_NOTIFIE_AU_SYNDIC_LI.search(page_txt): logging.warning( f\"Syndic: {m_synd.group(0)}\\n{m_synd.group('syndic')} / {m_synd.group('syndic_post')}\" ) return m_synd.group(\"syndic\") elif m_synd := P_SYNDIC.search(page_txt): logging.warning( f\"Syndic: {m_synd.group(0)}\\n{m_synd.group('syndic_pre')} / {m_synd.group('syndic')} / {m_synd.group('syndic_post')}\" ) return m_synd.group(\"syndic\") else: return None Typologie Typologie des arr\u00eat\u00e9s de mise en s\u00e9curit\u00e9. get_classe(page_txt) R\u00e9cup\u00e8re la classification de l'arr\u00eat\u00e9. Parameters page_txt: str Texte d'une page de document Returns doc_class: str Classification de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_classe(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re la classification de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_class: str Classification de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW # on commence par reconna\u00eetre et effacer les faux positifs de mainlev\u00e9e: # mentions de notification ou d'affichage, dans les extraits des textes # r\u00e9glementaires # TODO remplacer ce traitement par une d\u00e9tection des extraits dans leur # totalit\u00e9 (annexes, \u00e9ventuellement paragraphes int\u00e9gr\u00e9s au corps de l'arr\u00eat\u00e9) ml_fps = list(P_ML_FP.finditer(page_txt)) for ml_fp in ml_fps: page_txt = ( page_txt[: ml_fp.start()] + \" \" * (ml_fp.end() - ml_fp.start()) + page_txt[ml_fp.end() :] ) # # NB: l'ordre d'application des r\u00e8gles de matching est important: # les mainlev\u00e9es incluent g\u00e9n\u00e9ralement l'intitul\u00e9 de l'arr\u00eat\u00e9 (ou du type d'arr\u00eat\u00e9) pr\u00e9c\u00e9dent if ( M_CLASS_ML.search(page_txt) or M_CLASS_ABRO_DE.search(page_txt) or M_CLASS_ABRO_INT.search(page_txt) ): return \"Arr\u00eat\u00e9 de mainlev\u00e9e\" elif ( M_CLASS_PS_PO_MOD.search(page_txt) or M_CLASS_MS_MOD.search(page_txt) or M_CLASS_PGI_MOD.search(page_txt) or M_CLASS_MSU_MOD.search(page_txt) or M_CLASS_ML_PA.search(page_txt) or P_CLASS_INT_MOD.search(page_txt) ): return \"Arr\u00eat\u00e9 de mise en s\u00e9curit\u00e9 modificatif\" elif ( M_CLASS_PS_PO.search(page_txt) or M_CLASS_MS.search(page_txt) or M_CLASS_PGI.search(page_txt) or M_CLASS_MSU.search(page_txt) or M_CLASS_DE.search(page_txt) or M_CLASS_INS.search(page_txt) or P_CLASS_INT.search(page_txt) ): return \"Arr\u00eat\u00e9 de mise en s\u00e9curit\u00e9\" else: return None get_demo(page_txt) D\u00e9termine si l'arr\u00eat\u00e9 porte une d\u00e9molition ou d\u00e9construction. Parameters page_txt: str Texte d'une page de document Returns doc_demol_deconst: str D\u00e9molition ou d\u00e9construction si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_demo(page_txt: str) -> bool: \"\"\"D\u00e9termine si l'arr\u00eat\u00e9 porte une d\u00e9molition ou d\u00e9construction. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_demol_deconst: str D\u00e9molition ou d\u00e9construction si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if page_txt is None: return None elif P_DEMO.search(page_txt): return \"oui\" else: return \"non\" get_equ_com(page_txt) D\u00e9termine si l'arr\u00eat\u00e9 porte sur la s\u00e9curit\u00e9 des \u00e9quipements communs. Parameters page_txt: str Texte d'une page de document Returns doc_equ_com: str S\u00e9curit\u00e9 des \u00e9quipements communs si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_equ_com(page_txt: str) -> bool: \"\"\"D\u00e9termine si l'arr\u00eat\u00e9 porte sur la s\u00e9curit\u00e9 des \u00e9quipements communs. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_equ_com: str S\u00e9curit\u00e9 des \u00e9quipements communs si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if page_txt is None: return None elif P_EQU_COM.search(page_txt): return \"oui\" else: return \"non\" get_int_hab(page_txt) D\u00e9termine si l'arr\u00eat\u00e9 porte interdiction d'habiter et d'occuper. Parameters page_txt: str Texte d'une page de document Returns doc_int_hab: str Interdiction d'habiter si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_int_hab(page_txt: str) -> bool: \"\"\"D\u00e9termine si l'arr\u00eat\u00e9 porte interdiction d'habiter et d'occuper. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_int_hab: str Interdiction d'habiter si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if page_txt is None: return None elif P_INT_HAB.search(page_txt): return \"oui\" else: return \"non\" get_urgence(page_txt) R\u00e9cup\u00e8re le caract\u00e8re d'urgence de l'arr\u00eat\u00e9. Parameters page_txt: str Texte d'une page de document Returns doc_class: str Caract\u00e8re d'urgence de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_urgence(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re le caract\u00e8re d'urgence de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_class: str Caract\u00e8re d'urgence de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if ( M_CLASS_PS_PO.search(page_txt) or M_CLASS_PS_PO_MOD.search(page_txt) or M_CLASS_MS.search(page_txt) or M_CLASS_MS_MOD.search(page_txt) ): return \"non\" elif ( M_CLASS_PGI.search(page_txt) or M_CLASS_PGI_MOD.search(page_txt) or M_CLASS_MSU.search(page_txt) or M_CLASS_MSU_MOD.search(page_txt) ): return \"oui\" elif ( M_CLASS_ML_PA.search(page_txt) or M_CLASS_DE.search(page_txt) or M_CLASS_ABRO_DE.search(page_txt) or M_CLASS_INS.search(page_txt) or P_CLASS_INT.search(page_txt) ): # FIXME ajouter la prise en compte des articles cit\u00e9s pour d\u00e9terminer l'urgence # (pas pour le moment car l'info n'est pas fiable, les articles peuvent \u00eatre cit\u00e9s # en paquet) return None elif M_CLASS_ML.search(page_txt) or M_CLASS_ABRO_INT.search(page_txt): return None else: return None","title":"Domain Knowledge"},{"location":"Code%20Source/domain_knowledge/#domain-knowledge","text":"Definit les expressions r\u00e9guli\u00e8res et les dictionnaires utilis\u00e9s pour l'extraction des donn\u00e9es.","title":"Domain Knowledge"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.actes--actes","text":"Traces de t\u00e9l\u00e9transmission de documents par @ctes. Tampon et page d'accus\u00e9 de r\u00e9ception.","title":"Actes"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.actes.is_accusedereception_page","text":"D\u00e9tecte si une page contient un accus\u00e9 de r\u00e9ception.","title":"is_accusedereception_page()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.actes.is_accusedereception_page--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient un tampon de transmission src\\domain_knowledge\\actes.py def is_accusedereception_page(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un accus\u00e9 de r\u00e9ception. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un tampon de transmission \"\"\" return P_ACCUSE.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.actes.is_stamped_page","text":"D\u00e9tecte si une page contient un tampon (encadr\u00e9) de transmission @actes.","title":"is_stamped_page()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.actes.is_stamped_page--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient un tampon de transmission src\\domain_knowledge\\actes.py def is_stamped_page(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un tampon (encadr\u00e9) de transmission @actes. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un tampon de transmission \"\"\" return P_STAMP.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse--adresse","text":"Reconnaissance et traitement des adresses.","title":"Adresse"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.create_adresse_normalisee","text":"Cr\u00e9er une adresse normalis\u00e9e. L'adresse normalis\u00e9e rassemble les champs extraits de l'adresse brute, et ailleurs dans le document si n\u00e9cessaire (eg. autorit\u00e9 prenant l'arr\u00eat\u00e9, template). Le compl\u00e9ment d'adresse est ignor\u00e9.","title":"create_adresse_normalisee()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.create_adresse_normalisee--parameters","text":"adr_num: str Num\u00e9ro de l'adresse adr_ind: str Indice de l'adresse adr_voie: str Nom de la voie (incluant le type) adr_compl: str Compl\u00e9ment d'adresse adr_cpostal: str Code postal adr_ville: str Commune","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.create_adresse_normalisee--returns","text":"adr_norm: str Adresse normalis\u00e9e src\\domain_knowledge\\adresse.py def create_adresse_normalisee( adr_num: str, adr_ind: str, adr_voie: str, adr_compl: str, adr_cpostal: str, adr_ville: str, ) -> str: \"\"\"Cr\u00e9er une adresse normalis\u00e9e. L'adresse normalis\u00e9e rassemble les champs extraits de l'adresse brute, et ailleurs dans le document si n\u00e9cessaire (eg. autorit\u00e9 prenant l'arr\u00eat\u00e9, template). Le compl\u00e9ment d'adresse est ignor\u00e9. Parameters ---------- adr_num: str Num\u00e9ro de l'adresse adr_ind: str Indice de l'adresse adr_voie: str Nom de la voie (incluant le type) adr_compl: str Compl\u00e9ment d'adresse adr_cpostal: str Code postal adr_ville: str Commune Returns ------- adr_norm: str Adresse normalis\u00e9e \"\"\" # logique BAN: tous les champs sont s\u00e9par\u00e9s par une espace, sauf le num\u00e9ro de voie et l'indice de r\u00e9p\u00e9tition: # (ex: 6a rue Victor Hugo 13001 Marseille) (rmq_iteration) adr_num_ind = \"\".join( x for x in [ adr_num, adr_ind, ] if pd.notna(x) ) # tout concat\u00e9ner, sauf le compl\u00e9ment d'adresse adr_norm = \" \".join( x for x in [ adr_num_ind, adr_voie, # adr_compl, # TODO normaliser: accents etc? adr_cpostal, adr_ville, ] if pd.notna(x) ) return adr_norm","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.normalize_adresse","text":"Normalise les champs d'adresse. Les formes normales de chaque champ sont: - indice de r\u00e9p\u00e9tition en minuscules, - voie en minuscules, - ville en forme canonique tir\u00e9e du fichier des codes communes INSEE. Les espaces superflues ont normalement \u00e9t\u00e9 supprim\u00e9es en amont.","title":"normalize_adresse()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.normalize_adresse--parameters","text":"adresse: Dict[str, str] Adresse dont les champs sont bruts.","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.normalize_adresse--returns","text":"adresse_norm: Dict[str, str] Adresse dont les champs sont normalis\u00e9s. src\\domain_knowledge\\adresse.py def normalize_adresse(adresse: Dict[str, str]) -> Dict[str, str]: \"\"\"Normalise les champs d'adresse. Les formes normales de chaque champ sont: - indice de r\u00e9p\u00e9tition en minuscules, - voie en minuscules, - ville en forme canonique tir\u00e9e du fichier des codes communes INSEE. Les espaces superflues ont normalement \u00e9t\u00e9 supprim\u00e9es en amont. Parameters ---------- adresse: Dict[str, str] Adresse dont les champs sont bruts. Returns ------- adresse_norm: Dict[str, str] Adresse dont les champs sont normalis\u00e9s. \"\"\" # normalisation simple des champs (redondant et inutile ?) adresse_norm = { k: ( normalize_string(v, num=True, apos=True, hyph=True, spaces=True) if pd.notna(v) else v ) for k, v in adresse.items() } # indice de r\u00e9p\u00e9tition: mettre en minuscules if pd.notna(adresse_norm[\"ind\"]): adresse_norm[\"ind\"] = adresse_norm[\"ind\"].lower() # voie: mettre en minuscules if pd.notna(adresse_norm[\"voie\"]): adresse_norm[\"voie\"] = adresse_norm[\"voie\"].lower() # compl\u00e9ment: mettre en minuscules (2023-06-26 FL) if pd.notna(adresse_norm[\"compl\"]): adresse_norm[\"compl\"] = adresse_norm[\"compl\"].lower() # code postal: tel quel (d\u00e9j\u00e0 normalis\u00e9 en amont: espace entre code d\u00e9partement et le reste) # ville: enlever le num\u00e9ro d'arrondissement (Marseille), ex: \"Marseille 1er\" ; # la forme a d\u00e9j\u00e0 \u00e9t\u00e9 normalis\u00e9e (ou apr\u00e8s? TODO si apr\u00e8s, remettre la normalisation ici?) if pd.notna(adresse_norm[\"ville\"]): adresse_norm[\"ville\"] = re.sub( P_ARRONDISSEMENTS, \"\", adresse_norm[\"ville\"] ).strip() # return adresse_norm","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.process_adresse_brute","text":"Extraire une ou plusieurs adresses d'une adresse brute. Chaque adresse comporte diff\u00e9rents champs: num\u00e9ro, indicateur, voie, (\u00e9ventuellement complement d'adresse,) code postal, commune.","title":"process_adresse_brute()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.process_adresse_brute--parameters","text":"adr_ad_brute: str Adresse brute","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.adresse.process_adresse_brute--returns","text":"adresses: list(dict) Liste d'adresses src\\domain_knowledge\\adresse.py def process_adresse_brute(adr_ad_brute: str) -> List[Dict]: \"\"\"Extraire une ou plusieurs adresses d'une adresse brute. Chaque adresse comporte diff\u00e9rents champs: num\u00e9ro, indicateur, voie, (\u00e9ventuellement complement d'adresse,) code postal, commune. Parameters ---------- adr_ad_brute: str Adresse brute Returns ------- adresses: list(dict) Liste d'adresses \"\"\" if adr_ad_brute is None: adr_fields = { \"adr_num\": None, \"adr_ind\": None, \"adr_voie\": None, \"adr_compl\": None, \"adr_cpostal\": None, \"adr_ville\": None, } # TODO liste contenant un seul dict aux champs tous None, ou liste vide (\u00e0 g\u00e9rer) ? return [adr_fields] adresses = [] # ajouter une but\u00e9e droite pour le lookahead # FIXME contournement sale adr_ad_brute = adr_ad_brute + \" - \" m_adresse = P_ADRESSE_NG.match(adr_ad_brute) # was: \".search()\" if m_adresse: logging.warning(f\"process_adresse_brute: match: {m_adresse.groupdict()}\") # si aucune adresse extraite, on renvoie aussi une liste contenant une unique adresse vide if not m_adresse: logging.error(f\"aucune adresse extraite de {adr_ad_brute} par P_ADRESSE_NG\") # TODO factoriser avec le cas adr_ad_brute is None adr_fields = { \"adr_num\": None, \"adr_ind\": None, \"adr_voie\": None, \"adr_compl\": None, \"adr_cpostal\": None, \"adr_ville\": None, } # TODO liste contenant un seul dict aux champs tous None, ou liste vide (\u00e0 g\u00e9rer) ? return [adr_fields] logging.warning( f\"process_adresse_brute: {m_adresse.group(0)}\\n{m_adresse.groups()}\\n{m_adresse.groupdict()}\" ) # r\u00e9cup\u00e9rer les champs communs \u00e0 toutes les adresses group\u00e9es: compl\u00e9ment, # code postal et commune adr_compl = \" / \".join( m_adresse[x].strip() for x in [\"compl_ini\", \"compl_fin\"] if m_adresse[x] ) # FIXME concat? if adr_compl: logging.warning( f\"compl\u00e9ment d'adresse trouv\u00e9, pr\u00e9: {m_adresse['compl_ini']} ; post: {m_adresse['compl_fin']} dans adr_ad_brute: {adr_ad_brute}\" ) cpostal = m_adresse[\"code_postal\"] if cpostal: # code postal: supprimer une \u00e9ventuelle espace apr\u00e8s le d\u00e9partement (ex: 13 001) cpostal = cpostal.replace(\" \", \"\") commune = m_adresse[\"commune\"] if commune: # commune: remplacer par la forme canonique (pour les communes AMP) commune = normalize_ville(commune) # traitement sp\u00e9cifique pour la voie: type + nom (legacy?) # adr_voie = m_adresse[\"voie\"].strip() # if adr_voie == \"\": # adr_voie = None # extraire la ou les adresses courtes, et les s\u00e9parer s'il y en a plusieurs # on est oblig\u00e9 de r\u00e9extraire depuis l'adresse brute, car le RE_VOIE est d\u00e9fini # avec un contexte droit (positive lookahead) # (p\u00e9nible, mais pour le moment \u00e7a fonctionne comme \u00e7a) adr_lists = list(P_NUM_IND_VOIE_LIST.finditer(adr_ad_brute)) # obligatoire: une liste d'adresses courtes (ou une adresse courte) try: assert len(adr_lists) >= 1 except AssertionError: raise ValueError(f\"Aucune adresse courte d\u00e9tect\u00e9e dans {adr_ad_brute}\") # on v\u00e9rifie qu'on travaille exactement au m\u00eame endroit, pour se positionner au bon endroit try: assert adr_lists[0].group(0) == m_adresse[\"num_ind_voie_list\"] except AssertionError: logging.warning( f\"Probl\u00e8me sur {m_adresse.groupdict()}\\nadr_list.group(0): {adr_lists[0].group(0)} ; {m_adresse['num_ind_voie_list']}\" ) \"\"\"raise ValueError( f\"Probl\u00e8me sur {m_adresse.groupdict()}\\nadr_list.group(0): {adr_lists[0].group(0)} ; {m_adresse['num_ind_voie_list']}\" )\"\"\" for adr_list in adr_lists: # on ne peut pas compl\u00e8tement verrouiller avec adr_list.end(), car il manquerait \u00e0 nouveau le contexte droit (grmpf) adrs = list(P_NUM_IND_VOIE_NG.finditer(adr_ad_brute, adr_list.start())) if not adrs: raise ValueError(f\"Aucune adresse NUM_IND_VOIE trouv\u00e9e dans {adr_list}\") for adr in adrs: # pour chaque adresse courte, # - r\u00e9cup\u00e9rer la voie voie = adr[\"voie\"] # - r\u00e9cup\u00e9rer la liste (optionnelle) de num\u00e9ros et d'indicateurs (optionnels) num_ind_list = adr[\"num_ind_list\"] if not num_ind_list: # pas de liste de num\u00e9ros et indicateurs: logging.warning(f\"adresse courte en voie seule: {adr.group(0)}\") # ajouter une adresse sans num\u00e9ro (ni indicateur) adr_fields = { \"adr_num\": None, \"adr_ind\": None, \"adr_voie\": voie, \"adr_compl\": adr_compl, \"adr_cpostal\": cpostal, \"adr_ville\": commune, } adresses.append(adr_fields) else: # on a une liste de num\u00e9ros (et \u00e9ventuellement indicateurs) num_inds = list(P_NUM_IND.finditer(num_ind_list)) if len(num_inds) > 1: logging.warning(f\"plusieurs num\u00e9ros et indicateurs: {num_inds}\") for num_ind in num_inds: # pour chaque num\u00e9ro et \u00e9ventuel indicateur num_ind_str = num_ind.group(0) # extraire le num\u00e9ro m_nums = list(P_NUM_VOIE.finditer(num_ind_str)) assert len(m_nums) == 1 num = m_nums[0].group(0) # extraire le ou les \u00e9ventuels indicateurs m_inds = list(P_IND_VOIE.finditer(num_ind_str)) if not m_inds: # pas d'indicateur: adresse avec juste un num\u00e9ro adr_fields = { \"adr_num\": num, \"adr_ind\": None, \"adr_voie\": voie, \"adr_compl\": adr_compl, \"adr_cpostal\": cpostal, \"adr_ville\": commune, } adresses.append(adr_fields) else: # au moins un indicateur if len(m_inds) > 1: logging.warning(f\"plusieurs indicateurs: {m_inds}\") for m_ind in m_inds: # pour chaque indicateur, adresse avec num\u00e9ro et indicateur ind = m_ind.group(0) adr_fields = { \"adr_num\": num, \"adr_ind\": ind, \"adr_voie\": voie, \"adr_compl\": adr_compl, \"adr_cpostal\": cpostal, \"adr_ville\": commune, } adresses.append(adr_fields) # WIP code postal disparait if (cpostal is None) and P_CP.search(adr_ad_brute): # WIP survient pour les adresses doubles: la fin de la 2e adresse est envoy\u00e9e en commune # TODO d\u00e9tecter et analyser sp\u00e9cifiquement les adresses doubles logging.warning( f\"aucun code postal extrait de {adr_ad_brute}: {m_adresse.groupdict()}\" ) # end WIP code postal return adresses","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.agences_immo--agence-immobiliere","text":"Reconnaissance des noms d'agences immobili\u00e8res. Certains noms de syndics incluent \"syndic\", les capturer explicitement avant le motif g\u00e9n\u00e9ral permet d'\u00e9viter les conflits. Lister les syndics connus peut acc\u00e9l\u00e9rer et mieux focaliser la capture.","title":"Agence immobili\u00e8re"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.agences_immo.normalize_nom_cabinet","text":"Normalise un nom de cabinet. La version actuelle requiert une d\u00e9claration explicite dans LISTE_NOMS_CABINETS, mais des traitements de normalisation standard pourraient \u00eatre d\u00e9finis en compl\u00e9ment.","title":"normalize_nom_cabinet()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.agences_immo.normalize_nom_cabinet--parameters","text":"nom_cab: str Nom du cabinet ou de l'agence.","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.agences_immo.normalize_nom_cabinet--returns","text":"nom_nor: str Nom normalis\u00e9. src\\domain_knowledge\\agences_immo.py def normalize_nom_cabinet(nom_cab: str) -> str: \"\"\"Normalise un nom de cabinet. La version actuelle requiert une d\u00e9claration explicite dans LISTE_NOMS_CABINETS, mais des traitements de normalisation standard pourraient \u00eatre d\u00e9finis en compl\u00e9ment. Parameters ---------- nom_cab: str Nom du cabinet ou de l'agence. Returns ------- nom_nor: str Nom normalis\u00e9. \"\"\" if nom_cab is None: return None # for re_nom, norm in LISTE_NOMS_CABINETS.items(): # d\u00e8s qu'on a un match sur un nom de cabinet, on renvoie la forme normalis\u00e9e if re.search(re_nom, nom_cab, flags=(re.IGNORECASE | re.MULTILINE)): return norm else: # si aucun match, on renvoie le nom en entr\u00e9e tel quel return nom_cab","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete--arretes-de-collectivite-territoriale","text":"Structure d'un arr\u00eat\u00e9 de collectivit\u00e9 territoriale.","title":"Arr\u00e9t\u00e9s de collectivit\u00e9 territoriale"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_arrete","text":"D\u00e9tecte si une page contient ARRET(E|ONS).","title":"contains_arrete()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_arrete--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_arrete--returns","text":"has_stamp: bool True si le texte contient ARRET(E|ONS) src\\domain_knowledge\\arrete.py def contains_arrete(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient ARRET(E|ONS). Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient ARRET(E|ONS) \"\"\" return P_ARRETONS.search(page_txt) is not None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_article","text":"D\u00e9tecte si une page contient un Article.","title":"contains_article()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_article--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_article--returns","text":"has_stamp: bool True si le texte contient un Article src\\domain_knowledge\\arrete.py def contains_article(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un Article. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un Article \"\"\" return P_ARTICLE.search(page_txt) is not None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_considerant","text":"D\u00e9tecte si une page contient un CONSIDERANT.","title":"contains_considerant()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_considerant--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_considerant--returns","text":"has_stamp: bool True si le texte contient un CONSIDERANT src\\domain_knowledge\\arrete.py def contains_considerant(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un CONSIDERANT. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un CONSIDERANT \"\"\" return P_CONSIDERANT.search(page_txt) is not None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_vu","text":"D\u00e9tecte si une page contient un VU.","title":"contains_vu()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_vu--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.contains_vu--returns","text":"has_stamp: bool True si le texte contient un VU src\\domain_knowledge\\arrete.py def contains_vu(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un VU. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient un VU \"\"\" return P_VU.search(page_txt) is not None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_commune_maire","text":"Extrait le nom de la commune pr\u00e9c\u00e9d\u00e9 de la mention du maire.","title":"get_commune_maire()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_commune_maire--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_commune_maire--returns","text":"nom_commune: str | None Nom de la commune si le texte contient une mention du maire, None sinon. src\\domain_knowledge\\arrete.py def get_commune_maire(page_txt: str) -> bool: \"\"\"Extrait le nom de la commune pr\u00e9c\u00e9d\u00e9 de la mention du maire. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- nom_commune: str | None Nom de la commune si le texte contient une mention du maire, None sinon. \"\"\" if match_mc := P_MAIRE_COMMUNE.search(page_txt): com_maire = match_mc.group(\"commune\") # nettoyage de la valeur r\u00e9cup\u00e9r\u00e9e if m_com_cln := M_MAIRE_COMMUNE_CLEANUP.search(com_maire): com_maire = m_com_cln.group(\"maire_commune\") return com_maire else: return None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_date","text":"R\u00e9cup\u00e8re la date de l'arr\u00eat\u00e9. Actuellement, correspond \u00e0 la date de signature, en fin d'arr\u00eat\u00e9.","title":"get_date()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_date--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_date--returns","text":"doc_date: str Date du document si trouv\u00e9e, None sinon. src\\domain_knowledge\\arrete.py def get_date(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re la date de l'arr\u00eat\u00e9. Actuellement, correspond \u00e0 la date de signature, en fin d'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_date: str Date du document si trouv\u00e9e, None sinon. \"\"\" if m_date_d := P_DATE_SIGNAT.search(page_txt): return m_date_d.group(\"arr_date\") else: return None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_nom","text":"R\u00e9cup\u00e8re le nom de l'arr\u00eat\u00e9.","title":"get_nom()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_nom--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_nom--returns","text":"doc_nom: str Nom de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\arrete.py def get_nom(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re le nom de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_nom: str Nom de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" # TODO nettoyer \u00e0 gauche (\"Dossier suivi par\") et \u00e0 droite: \"\\nNous,\" if m_nom := P_NOM_ARR.search(page_txt): return m_nom.group(\"nom_arr\") else: return None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_num","text":"R\u00e9cup\u00e8re le num\u00e9ro de l'arr\u00eat\u00e9.","title":"get_num()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_num--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.arrete.get_num--returns","text":"doc_num: str Num\u00e9ro de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\arrete.py def get_num(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re le num\u00e9ro de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_num: str Num\u00e9ro de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" if m_num := P_NUM_ARR.search(page_txt): return m_num.group(\"num_arr\") elif m_num_fb := P_NUM_ARR_FALLBACK.search(page_txt): return m_num_fb.group(\"num_arr\") else: return None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadastre--references-cadastrales","text":"Reconnaissance et analyse de r\u00e9f\u00e9rences cadastrales.","title":"R\u00e9f\u00e9rences cadastrales"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadastre.generate_refcadastrale_norm","text":"G\u00e9n\u00e8re une r\u00e9f\u00e9rence cadastrale normalis\u00e9e \u00e0 une entr\u00e9e.","title":"generate_refcadastrale_norm()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadastre.generate_refcadastrale_norm--parameters","text":"codeinsee: string Code INSEE de la commune. refcad: string R\u00e9f\u00e9rence cadastrale brute. arr_pdf: string Nom du fichier PDF (pour exception) adr_cpostal: string Code postal de la commune","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadastre.generate_refcadastrale_norm--returns","text":"refcad: string R\u00e9f\u00e9rence cadastrale normalis\u00e9e. src\\domain_knowledge\\cadastre.py def generate_refcadastrale_norm( codeinsee: str, refcad: str, arr_pdf: str, adr_cpostal: str ) -> str: \"\"\"G\u00e9n\u00e8re une r\u00e9f\u00e9rence cadastrale normalis\u00e9e \u00e0 une entr\u00e9e. Parameters ---------- codeinsee: string Code INSEE de la commune. refcad: string R\u00e9f\u00e9rence cadastrale brute. arr_pdf: string Nom du fichier PDF (pour exception) adr_cpostal: string Code postal de la commune Returns ------- refcad: string R\u00e9f\u00e9rence cadastrale normalis\u00e9e. \"\"\" # ajouter le pr\u00e9fixe du code insee # TODO cas particulier pour Marseille: code commune par ardt + code quartier if pd.isna(codeinsee): codeinsee = \"\" # TODO v\u00e9rifier si le comportement qui en d\u00e9coule est ok (identifiant court, \u00e0 compl\u00e9ter manuellement par le code insee) # prendre la r\u00e9f\u00e9rence locale (commune) if pd.isna(refcad): refcad = None elif m_mars := P_CAD_MARSEILLE_NG.search(refcad): # on ne garde que le 1er match # TODO g\u00e9rer 2 ou plusieurs r\u00e9f\u00e9rences cadastrales # Marseille: code insee arrondissement + code quartier (3 chiffres) + section + parcelle arrt = m_mars[\"arrt\"] if not arrt and not (codeinsee and codeinsee != \"13055\"): # ni arrondissement ni code INSEE (diff\u00e9rent de celui de tout Marseille)=> g\u00e9n\u00e9rer une r\u00e9f\u00e9rence cadastrale courte refcad = f\"{m_mars['quar']}{m_mars['sec']:>02}{m_mars['num']:>04}\" logging.error( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale incompl\u00e8te (num\u00e9ro d'arrondissement manquant \u00e0 Marseille): {refcad}\" ) else: # arrondissement ou code INSEE if codeinsee and codeinsee != \"13055\": # on a bien un code INSEE (diff\u00e9rent du code INSEE pour tout Marseille), on peut donc l'utiliser if arrt: # si on a aussi un num\u00e9ro d'arrondissement, on v\u00e9rifie que les deux sont coh\u00e9rents try: assert codeinsee[-3:] == arrt except AssertionError: # FIXME am\u00e9liorer le warning ; \u00e9crire une expectation sur le dataset final # 2023-03-06: 16 conflits logging.warning( f\"{arr_pdf}: conflit entre code INSEE ({codeinsee}, via code postal {adr_cpostal}) et r\u00e9f\u00e9rence cadastrale {arrt}\" ) else: # on n'a un code d'arrondissement: reconstruire un code INSEE codeinsee = f\"13{arrt}\" # g\u00e9n\u00e9rer une r\u00e9f\u00e9rence cadastrale compl\u00e8te refcad = ( f\"{codeinsee}{m_mars['quar']}{m_mars['sec']:>02}{m_mars['num']:>04}\" ) # le code de section devrait \u00eatre en majuscules ; \u00e9mettre un warning sinon # TODO ajouter au rapport d'erreur (r\u00e9f normalis\u00e9e produite + str en entr\u00e9e) if not m_mars[\"sec\"].isupper(): logging.warning( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale suspecte (code de section): {refcad}\" ) elif m_autr := P_CAD_AUTRES_NG.search(refcad): # hors Marseille: code insee commune + 000 + section + parcelle codequartier = \"000\" refcad = f\"{codeinsee}{codequartier}{m_autr['sec']:>02}{m_autr['num']:>04}\" # 2 v\u00e9rifications: # - le code de section devrait \u00eatre en majuscules ; \u00e9mettre un warning sinon # TODO ajouter au rapport d'erreur (r\u00e9f normalis\u00e9e produite + str en entr\u00e9e) if not m_autr[\"sec\"].isupper(): logging.warning( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale suspecte (code de section): {refcad}\" ) # - si le code INSEE est en fait un code INSEE d'un arrondissement de Marseille, # mais aucun code quartier n'a \u00e9t\u00e9 rep\u00e9r\u00e9 donc la r\u00e9f\u00e9rence cadastrale lue est # une r\u00e9f\u00e9rence courte, comme dans d'autres communes # TODO ajouter au rapport d'erreur (r\u00e9f normalis\u00e9e produite + str en entr\u00e9e) if codeinsee and (int(codeinsee) in range(13201, 13216)): logging.warning( f\"{arr_pdf}: r\u00e9f\u00e9rence cadastrale suspecte (Marseille sans code quartier): {refcad}\" ) else: refcad = None return refcad","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadastre.get_parcelles","text":"R\u00e9cup\u00e8re la ou les r\u00e9f\u00e9rences de parcelles cadastrales.","title":"get_parcelles()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadastre.get_parcelles--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadastre.get_parcelles--returns","text":"id_parcelles: List[str] R\u00e9f\u00e9rences d'une ou plusieurs parcelles cadastrales si d\u00e9tect\u00e9es dans le texte, liste vide sinon. src\\domain_knowledge\\cadastre.py def get_parcelles(page_txt: str) -> List[str]: \"\"\"R\u00e9cup\u00e8re la ou les r\u00e9f\u00e9rences de parcelles cadastrales. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- id_parcelles: List[str] R\u00e9f\u00e9rences d'une ou plusieurs parcelles cadastrales si d\u00e9tect\u00e9es dans le texte, liste vide sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW id_parcelles = [] # r\u00e9sultat # WIP chercher le ou les empans distincts contenant au moins une r\u00e9f\u00e9rence \u00e0 une parcelle if matches := list(P_PARCELLE.finditer(page_txt)): logging.warning( f\"{len(matches)} empans PARCELLE: {[x.group(0) for x in matches]}\" ) # WIP extraire plusieurs r\u00e9f\u00e9rences for m_parc in matches: # liste des identifiants de parcelles m_cad_str = m_parc.group(\"cadastre_id\") # WIP if m_parcs_mrs := list( P_CAD_MARSEILLE_NG.finditer( page_txt, m_parc.start(\"cadastre_id\"), m_parc.end(\"cadastre_id\") ) ): # essayer d'abord de rep\u00e9rer des r\u00e9f\u00e9rences Marseille, plus longues et qui peuvent g\u00e9n\u00e9rer de faux positifs si analys\u00e9es # comme des r\u00e9f\u00e9rences hors Marseille (ex: \"208837 D0607 ET 208837 D0290\" => \"ET 2088\" serait rep\u00e9r\u00e9 comme une parcelle...) m_parcs = m_parcs_mrs if len(m_parcs) > 1: logging.warning( f\"{len(m_parcs)} parcelles (Marseille 1) dans {m_cad_str}: {[x.group(0) for x in m_parcs]}\" ) elif m_parcs_aut := list( P_CAD_SECNUM.finditer( page_txt, m_parc.start(\"cadastre_id\"), m_parc.end(\"cadastre_id\") ) ): # sinon essayer de rep\u00e9rer des r\u00e9f\u00e9rences d'autres communes m_parcs = m_parcs_aut if len(m_parcs) > 1: logging.warning( f\"{len(m_parcs)} parcelles (toutes communes) dans {m_cad_str}: {[x.group(0) for x in m_parcs]}\" ) else: raise ValueError( f\"Pas de r\u00e9f\u00e9rence retrouv\u00e9e dans la zone? {m_cad_str}\" ) # end WIP # RESUME HERE ! 2023-04-28 id_parcelles.append(m_cad_str) elif ( False ): # matches := list(P_PARCELLE_MARSEILLE_NOCONTEXT.finditer(page_txt)): # WIP logging.warning( f\"{len(matches)} empans PARC_MRS: {[x.group(0) for x in matches]}\" ) for m_parc_mrs in matches: # liste des identifiants de parcelles m_cad_str = m_parc_mrs.group(\"cadastre_id\") # WIP m_parcs = list( P_CAD_MARSEILLE_NG.finditer( page_txt, m_parc_mrs.start(\"cadastre_id\"), m_parc_mrs.end(\"cadastre_id\"), ) ) if len(m_parcs) > 1: logging.warning( f\"{len(m_parcs)} parcelles (Marseille 2) dans {m_cad_str}: {[x.group(0) for x in m_parcs]}\" ) # end WIP id_parcelles.append(m_cad_str) return id_parcelles","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire--cadre-reglementaire","text":"R\u00e9f\u00e9rences au cadre r\u00e9glementaire.","title":"Cadre r\u00e9glementaire"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cc","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code Civil.","title":"contains_cc()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cc--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code Civil. src\\domain_knowledge\\cadre_reglementaire.py def contains_cc(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code Civil. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code Civil. \"\"\" return P_CC.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cc_art","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil.","title":"contains_cc_art()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cc_art--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil. src\\domain_knowledge\\cadre_reglementaire.py def contains_cc_art(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code Civil. \"\"\" return P_CC_ART.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation.","title":"contains_cch()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code de la Construction et de l'Habitation. \"\"\" return P_CCH.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L111","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation.","title":"contains_cch_L111()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L111--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L111(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L111 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L111.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L511","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation.","title":"contains_cch_L511()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L511--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L511(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L511 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L511.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L521","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation.","title":"contains_cch_L521()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L521--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L521(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L521 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L521.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L541","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation.","title":"contains_cch_L541()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_L541--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_L541(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article L541 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_L541.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_R511","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation.","title":"contains_cch_R511()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cch_R511--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation. src\\domain_knowledge\\cadre_reglementaire.py def contains_cch_R511(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 l'article R511 du Code de la Construction et de l'Habitation. \"\"\" return P_CCH_R511.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cgct","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales.","title":"contains_cgct()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cgct--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. src\\domain_knowledge\\cadre_reglementaire.py def contains_cgct(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence au Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. \"\"\" return P_CGCT.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cgct_art","text":"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales.","title":"contains_cgct_art()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.contains_cgct_art--parameters","text":"page_txt: str Texte d'une page de document Returns has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. src\\domain_knowledge\\cadre_reglementaire.py def contains_cgct_art(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- has_stamp: bool True si le texte contient une r\u00e9f\u00e9rence \u00e0 des articles du Code G\u00e9n\u00e9ral des Collectivit\u00e9s Territoriales. \"\"\" return P_CGCT_ART.search(page_txt) is not None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.parse_refs_reglement","text":"Rep\u00e8re dans un texte des r\u00e9f\u00e9rences au cadre r\u00e9glementaire.","title":"parse_refs_reglement()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.parse_refs_reglement--parameters","text":"txt_body: string Corps de texte \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser.","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.cadre_reglementaire.parse_refs_reglement--returns","text":"content: list Liste d'empans de r\u00e9f\u00e9rences src\\domain_knowledge\\cadre_reglementaire.py def parse_refs_reglement(txt_body: str, span_beg: int, span_end: int) -> list: \"\"\"Rep\u00e8re dans un texte des r\u00e9f\u00e9rences au cadre r\u00e9glementaire. Parameters ---------- txt_body: string Corps de texte \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser. Returns ------- content: list Liste d'empans de r\u00e9f\u00e9rences \"\"\" content = [] for p_reg, typ_reg in REG_TYP: if matches := p_reg.finditer(txt_body, span_beg, span_end): for match in matches: content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(0), \"span_typ\": typ_reg, } ) return content","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo--code-geographiques","text":"Acc\u00e8s aux codes g\u00e9ographiques (codes INSEE, codes postaux) des communes. TODO cr\u00e9er des modules similaires pour les autres bases de connaissances: * les variantes de graphies des communes (TODO), * une liste de syndics (TODO).","title":"Code g\u00e9ographiques"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.get_codeinsee","text":"R\u00e9cup\u00e9rer le code INSEE d'une commune. Le code postal est utilis\u00e9 pour les arrondissements de Marseille.","title":"get_codeinsee()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.get_codeinsee--parameters","text":"nom_commune: string Nom de la commune cpostal: string or None Code postal, utile pour les arrondissements de Marseille","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.get_codeinsee--returns","text":"codeinsee: string Code INSEE de la commune. src\\domain_knowledge\\codes_geo.py def get_codeinsee(nom_commune: str, cpostal: str) -> str: \"\"\"R\u00e9cup\u00e9rer le code INSEE d'une commune. Le code postal est utilis\u00e9 pour les arrondissements de Marseille. Parameters ---------- nom_commune: string Nom de la commune cpostal: string or None Code postal, utile pour les arrondissements de Marseille Returns ------- codeinsee: string Code INSEE de la commune. \"\"\" if pd.isna(nom_commune): return None nom_commune = ( nom_commune.strip() ) # TODO s'assurer que strip() est fait en amont, \u00e0 l'extraction de la donn\u00e9e ? # v\u00e9rifier que nom_commune est une graphie d'une commune de la m\u00e9tropole try: assert P_COMMUNES_AMP_ALLFORMS.match(nom_commune) or nom_commune in ( \"la Gardanne\", ) # FIXME: arr\u00eat\u00e9s mal lus except AssertionError: # TODO d\u00e9tecter et exclure les communes hors M\u00e9tropole en amont? logging.warning( f\"Impossible de d\u00e9terminer le code INSEE pour {nom_commune}, hors m\u00e9tropole?\" ) # raise return None if ( nom_commune.lower().startswith(\"marseille\") and pd.notna(cpostal) and (cpostal in CP_MARSEILLE) ): # NB: c'est une approximation ! # TODO expectation: aucun codeinsee 13055 dans le dataset final (ou presque) codeinsee = \"132\" + cpostal[-2:] else: # TODO \u00e9prouver et am\u00e9liorer la robustesse codeinsee = COM2INSEE.get(simplify_commune(nom_commune), None) if not codeinsee: logging.warning( f\"get_codeinsee: pas de code trouv\u00e9 pour {(nom_commune, cpostal)} (simplify_commune={simplify_commune(nom_commune)}).\" ) return codeinsee","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.get_codepostal","text":"R\u00e9cup\u00e9rer le code postal d'une commune \u00e0 partir de son code INSEE. Attention, risque d'erreurs car certaines communes \u00e9tendues sont couvertes par plusieurs codes postaux: Marseille (1 par arrondissement, chaque arrondissement a aussi son COG) mais aussi Aix-en-Provence (1 COG mais 6 codes postaux: 13080, 13090, 13098, 13100, 13290, 13540), Martigues (codes postaux: 13117, 13500). TODO Le nom de la commune est-il utile?","title":"get_codepostal()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.get_codepostal--parameters","text":"nom_commune: string Nom de la commune (inutile?) codeinsee: string or None Code INSEE.","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.get_codepostal--returns","text":"cpostal: string Code postal de la commune. src\\domain_knowledge\\codes_geo.py def get_codepostal(nom_commune: str, codeinsee: str) -> str: \"\"\"R\u00e9cup\u00e9rer le code postal d'une commune \u00e0 partir de son code INSEE. Attention, risque d'erreurs car certaines communes \u00e9tendues sont couvertes par plusieurs codes postaux: Marseille (1 par arrondissement, chaque arrondissement a aussi son COG) mais aussi Aix-en-Provence (1 COG mais 6 codes postaux: 13080, 13090, 13098, 13100, 13290, 13540), Martigues (codes postaux: 13117, 13500). TODO Le nom de la commune est-il utile? Parameters ---------- nom_commune: string Nom de la commune (inutile?) codeinsee: string or None Code INSEE. Returns ------- cpostal: string Code postal de la commune. \"\"\" if pd.isna(nom_commune): return None nom_commune = ( nom_commune.strip() ) # TODO s'assurer que strip() est fait en amont, \u00e0 l'extraction de la donn\u00e9e ? # v\u00e9rifier que nom_commune est une graphie d'une commune de la m\u00e9tropole try: assert P_COMMUNES_AMP_ALLFORMS.match(nom_commune) or nom_commune in ( \"la Gardanne\", ) # FIXME: arr\u00eat\u00e9s mal lus except AssertionError: # TODO d\u00e9tecter et exclure les communes hors M\u00e9tropole en amont? logging.warning( f\"Impossible de d\u00e9terminer le code INSEE pour {nom_commune}, hors m\u00e9tropole?\" ) # raise return None if ( nom_commune.lower().startswith(\"marseille\") and pd.notna(codeinsee) and (codeinsee.startswith(\"132\")) # FIXME g\u00e9n\u00e9raliser/am\u00e9liorer? ): # NB: c'est une approximation ! # TODO expectation: aucun codeinsee 13055 dans le dataset final (ou presque) cpostal = \"130\" + codeinsee[-2:] # 2023-03-18: a priori, cela ne devrait rien changer car le code INSEE est d\u00e9termin\u00e9 \u00e0 partir du code postal pour les arrondissements de Marseille elif pd.notna(codeinsee) and (simplify_commune(nom_commune), codeinsee) in ( (\"aixenprovence\", \"13001\"), (\"martigues\", \"13056\"), ): cpostal = None # pour que create_adresse_normalisee() n'ait \u00e0 g\u00e9rer des valeurs pd.<NA> dont la valeur bool\u00e9enne est ambigue (alors que None est faux) logging.warning( f\"get_codepostal: abstention, plusieurs codes postaux possibles pour {(nom_commune, codeinsee)}.\" ) else: # TODO \u00e9prouver et am\u00e9liorer la robustesse cpostal = INSEE2POST.get(codeinsee, None) if pd.isna(cpostal): cpostal = None # pour que create_adresse_normalisee() n'ait \u00e0 g\u00e9rer des valeurs pd.<NA> dont la valeur bool\u00e9enne est ambigue (alors que None est faux) logging.warning( f\"get_codepostal: pas de code trouv\u00e9 pour {(nom_commune, codeinsee)}.\" ) return cpostal","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.load_codes_insee_amp","text":"Charger les codes INSEE des communes Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence.","title":"load_codes_insee_amp()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.load_codes_insee_amp--returns","text":"df_insee: pd.DataFrame Liste des communes avec leur code INSEE. src\\domain_knowledge\\codes_geo.py def load_codes_insee_amp() -> pd.DataFrame: \"\"\"Charger les codes INSEE des communes Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence. Returns ------- df_insee: pd.DataFrame Liste des communes avec leur code INSEE. \"\"\" df_insee = pd.read_csv(FP_INSEE, dtype=DTYPE_INSEE) return df_insee","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.load_codes_postaux_amp","text":"Charger les codes postaux des communes, associ\u00e9s aux codes INSEE. Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence. Attention, le fichier actuel (2023-03-18) utilise un s\u00e9parateur \";\".","title":"load_codes_postaux_amp()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.load_codes_postaux_amp--returns","text":"df_cpostal: pd.DataFrame Liste des codes postaux par (code INSEE de) commune. src\\domain_knowledge\\codes_geo.py def load_codes_postaux_amp() -> pd.DataFrame: \"\"\"Charger les codes postaux des communes, associ\u00e9s aux codes INSEE. Actuellement restreint \u00e0 la M\u00e9tropole Aix-Marseille Provence. Attention, le fichier actuel (2023-03-18) utilise un s\u00e9parateur \";\". Returns ------- df_cpostal: pd.DataFrame Liste des codes postaux par (code INSEE de) commune. \"\"\" df_cpostal = pd.read_csv(FP_CPOSTAL, dtype=DTYPE_CPOSTAL, sep=\";\") return df_cpostal","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.normalize_ville","text":"Normalise un nom de ville. Les formes reconnues par S_RE_COMMUNES_VARS sont r\u00e9\u00e9crites dans la forme canonique tir\u00e9e de DF_INSEE[\"commune\"] . Pour les villes absentes de cette ressource externe, le nom est renvoy\u00e9 tel quel.","title":"normalize_ville()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.normalize_ville--parameters","text":"raw_ville: str Nom brut de la ville, extrait du document.","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.normalize_ville--returns","text":"nor_ville: str Forme normale, canonique, du nom de ville. src\\domain_knowledge\\codes_geo.py def normalize_ville(raw_ville: str) -> str: \"\"\"Normalise un nom de ville. Les formes reconnues par `S_RE_COMMUNES_VARS` sont r\u00e9\u00e9crites dans la forme canonique tir\u00e9e de `DF_INSEE[\"commune\"]`. Pour les villes absentes de cette ressource externe, le nom est renvoy\u00e9 tel quel. Parameters ---------- raw_ville: str Nom brut de la ville, extrait du document. Returns ------- nor_ville: str Forme normale, canonique, du nom de ville. \"\"\" for p_ville, norm_ville in VILLE_PAT_NORM: if p_ville.match(raw_ville): return norm_ville else: # si toutes les possibilit\u00e9s ont \u00e9t\u00e9 \u00e9puis\u00e9es, # renvoyer la valeur en entr\u00e9e return raw_ville","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.simplify_commune","text":"Simplifier le nom d'une commune pour faciliter le matching.","title":"simplify_commune()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.simplify_commune--parameters","text":"com: str Nom de la commune","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.codes_geo.simplify_commune--returns","text":"com_simple: str Nom de la commune simplifi\u00e9 src\\domain_knowledge\\codes_geo.py def simplify_commune(com: str) -> str: \"\"\"Simplifier le nom d'une commune pour faciliter le matching. Parameters ---------- com: str Nom de la commune Returns ------- com_simple: str Nom de la commune simplifi\u00e9 \"\"\" # FIXME utiliser unicodedata.normalize return ( com.lower() .replace(\"\u00e0\", \"a\") .replace(\"\u00e7\", \"c\") .replace(\"\u00e9\", \"e\") .replace(\"\u00e8\", \"e\") .replace(\"\u00ea\", \"e\") .replace(\"\u00ee\", \"i\") .replace(\"\u00ef\", \"i\") .replace(\"\u00f4\", \"o\") .replace(\"\u00fb\", \"u\") .replace(\"\u00ff\", \"y\") .replace(\"-\", \"\") .replace(\" \", \"\") )","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.doc_relations--relations-entre-documents","text":"Les r\u00e9f\u00e9rence \u00e0 des documents pr\u00e9c\u00e9dents sont \u00e9nonc\u00e9es dans les \"Vu\".","title":"Relations entre documents."},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.doc_template--modeles-de-documents-des-arretes","text":"Motifs de reconnaissance des en-t\u00eates, pieds-de-page et annexes. TODO - [ ] exploiter les \u00e9l\u00e9ments de template (discriminants) pour d\u00e9terminer la ville (en compl\u00e9ment des autres emplacements: autorit\u00e9, signature)","title":"Mod\u00e8les de documents des arr\u00eat\u00e9s."},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement--logement","text":"Rep\u00e9rage et extraction de donn\u00e9es propres aux arr\u00eat\u00e9s sur le logement. Propri\u00e9taire, gestionnaire, syndic ou administrateur, adresse de l'immeuble concern\u00e9.","title":"Logement"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_adr_doc","text":"Extrait la ou les adresses vis\u00e9es par l'arr\u00eat\u00e9.","title":"get_adr_doc()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_adr_doc--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_adr_doc--returns","text":"adresses: List[dict] La ou les adresses vis\u00e9es par l'arr\u00eat\u00e9, si trouv\u00e9es dans la page de texte. Pour chaque zone d'adresse brute, la ou les adresses extraites. src\\domain_knowledge\\logement.py def get_adr_doc(page_txt: str) -> bool: \"\"\"Extrait la ou les adresses vis\u00e9es par l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- adresses: List[dict] La ou les adresses vis\u00e9es par l'arr\u00eat\u00e9, si trouv\u00e9es dans la page de texte. Pour chaque zone d'adresse brute, la ou les adresses extraites. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # WIP au pr\u00e9alable, neutraliser les adresses des services municipaux if serv_mun := P_ADR_SERVICES_MUNI.search(page_txt): logging.warning(f\"service municipal remplac\u00e9: {serv_mun}\") page_txt = re.sub(P_ADR_SERVICES_MUNI, \"SERVICE_MUNICIPAL\", page_txt) adresses = [] if matches_adr := list(P_ADR_DOC.finditer(page_txt)): for m_adr in matches_adr: logging.warning( f\"adr_doc: {m_adr.group(0)}\\n{m_adr.groups()}\\n{m_adr.groupdict()}\" ) adr_brute = m_adr.group(\"adresse\") logging.warning(f\"adr_brute brute: {adr_brute}\") # nettoyer la valeur r\u00e9cup\u00e9r\u00e9e # - couper sur certains contextes droits if False: adr_brute = re.sub( RE_ADR_CLEANUP, \"\", adr_brute, flags=(re.MULTILINE | re.IGNORECASE) ) # RESUME HERE 2023-04-11 il faut r\u00e9ussir \u00e0 se passer du cleanup # - enlever l'\u00e9ventuelle ponctuation finale if adr_brute.endswith((\".\", \",\")): adr_brute = adr_brute[:-1] # - normaliser les graphies, les espaces etc adr_brute = normalize_string( adr_brute, num=True, apos=True, hyph=True, spaces=True ) # - extraire la ou les adresses pr\u00e9cises, d\u00e9compos\u00e9e en champs # (num\u00e9ro, indicateur, voie...) # WIP on prend le texte de la page, born\u00e9 \u00e0 gauche avec le d\u00e9but de l'adresse # mais pas born\u00e9 \u00e0 droite pour avoir le contexte droit (n\u00e9cessaire pour # les adresses courtes, car le nom de voie est born\u00e9 par un lookahead) try: adresses_proc = process_adresse_brute(adr_brute) except AssertionError: print( f\"get_adr_doc: process_adresse_brute({adr_brute})\\nmatch complet={m_adr.group(0)}\\ngroups={m_adr.groups()}\\ngroupdict={m_adr.groupdict()}\" ) raise adresses.append( { \"adresse_brute\": adr_brute, \"adresses\": adresses_proc, } ) return adresses","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_gest","text":"D\u00e9tecte si une page contient un nom de gestionnaire immobilier.","title":"get_gest()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_gest--parameters","text":"page_txt: str Texte d'une page de document Returns syndic: str Nom de gestionnaire si d\u00e9tect\u00e9, None sinon. src\\domain_knowledge\\logement.py def get_gest(page_txt: str) -> str: \"\"\"D\u00e9tecte si une page contient un nom de gestionnaire immobilier. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- syndic: str Nom de gestionnaire si d\u00e9tect\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW match = P_GEST.search(page_txt) return match.group(\"gestio\") if match is not None else None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_proprio","text":"Extrait le nom et l'adresse du propri\u00e9taire.","title":"get_proprio()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_proprio--parameters","text":"page_txt: str Texte d'une page de document Returns syndic: str Nom et adresse du propri\u00e9taire si d\u00e9tect\u00e9, None sinon. src\\domain_knowledge\\logement.py def get_proprio(page_txt: str) -> bool: \"\"\"Extrait le nom et l'adresse du propri\u00e9taire. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- syndic: str Nom et adresse du propri\u00e9taire si d\u00e9tect\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW # on essaie d'abord de d\u00e9tecter un mono-propri\u00e9taire (WIP) if match := P_PROPRIO_MONO.search(page_txt): logging.warning(f\"mono-propri\u00e9taire: {match}\\n{match.group(0)}\") return match.group(\"proprio\") # puis sinon les multi-propri\u00e9taires (TODO proprement) elif match := P_PROPRIO.search(page_txt): logging.warning(f\"mono- ou multi-propri\u00e9taire: {match}\\n{match.group(0)}\") return match.group(\"proprio\") else: return None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_syndic","text":"D\u00e9tecte si une page contient un nom de syndic.","title":"get_syndic()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.logement.get_syndic--parameters","text":"page_txt: str Texte d'une page de document Returns syndic: str Nom de syndic si d\u00e9tect\u00e9, None sinon. src\\domain_knowledge\\logement.py def get_syndic(page_txt: str) -> bool: \"\"\"D\u00e9tecte si une page contient un nom de syndic. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- syndic: str Nom de syndic si d\u00e9tect\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if m_synd := P_NOTIFIE_AU_SYNDIC_LI.search(page_txt): logging.warning( f\"Syndic: {m_synd.group(0)}\\n{m_synd.group('syndic')} / {m_synd.group('syndic_post')}\" ) return m_synd.group(\"syndic\") elif m_synd := P_SYNDIC.search(page_txt): logging.warning( f\"Syndic: {m_synd.group(0)}\\n{m_synd.group('syndic_pre')} / {m_synd.group('syndic')} / {m_synd.group('syndic_post')}\" ) return m_synd.group(\"syndic\") else: return None","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite--typologie","text":"Typologie des arr\u00eat\u00e9s de mise en s\u00e9curit\u00e9.","title":"Typologie"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_classe","text":"R\u00e9cup\u00e8re la classification de l'arr\u00eat\u00e9.","title":"get_classe()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_classe--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_classe--returns","text":"doc_class: str Classification de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_classe(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re la classification de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_class: str Classification de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW # on commence par reconna\u00eetre et effacer les faux positifs de mainlev\u00e9e: # mentions de notification ou d'affichage, dans les extraits des textes # r\u00e9glementaires # TODO remplacer ce traitement par une d\u00e9tection des extraits dans leur # totalit\u00e9 (annexes, \u00e9ventuellement paragraphes int\u00e9gr\u00e9s au corps de l'arr\u00eat\u00e9) ml_fps = list(P_ML_FP.finditer(page_txt)) for ml_fp in ml_fps: page_txt = ( page_txt[: ml_fp.start()] + \" \" * (ml_fp.end() - ml_fp.start()) + page_txt[ml_fp.end() :] ) # # NB: l'ordre d'application des r\u00e8gles de matching est important: # les mainlev\u00e9es incluent g\u00e9n\u00e9ralement l'intitul\u00e9 de l'arr\u00eat\u00e9 (ou du type d'arr\u00eat\u00e9) pr\u00e9c\u00e9dent if ( M_CLASS_ML.search(page_txt) or M_CLASS_ABRO_DE.search(page_txt) or M_CLASS_ABRO_INT.search(page_txt) ): return \"Arr\u00eat\u00e9 de mainlev\u00e9e\" elif ( M_CLASS_PS_PO_MOD.search(page_txt) or M_CLASS_MS_MOD.search(page_txt) or M_CLASS_PGI_MOD.search(page_txt) or M_CLASS_MSU_MOD.search(page_txt) or M_CLASS_ML_PA.search(page_txt) or P_CLASS_INT_MOD.search(page_txt) ): return \"Arr\u00eat\u00e9 de mise en s\u00e9curit\u00e9 modificatif\" elif ( M_CLASS_PS_PO.search(page_txt) or M_CLASS_MS.search(page_txt) or M_CLASS_PGI.search(page_txt) or M_CLASS_MSU.search(page_txt) or M_CLASS_DE.search(page_txt) or M_CLASS_INS.search(page_txt) or P_CLASS_INT.search(page_txt) ): return \"Arr\u00eat\u00e9 de mise en s\u00e9curit\u00e9\" else: return None","title":"Returns"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_demo","text":"D\u00e9termine si l'arr\u00eat\u00e9 porte une d\u00e9molition ou d\u00e9construction.","title":"get_demo()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_demo--parameters","text":"page_txt: str Texte d'une page de document Returns doc_demol_deconst: str D\u00e9molition ou d\u00e9construction si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_demo(page_txt: str) -> bool: \"\"\"D\u00e9termine si l'arr\u00eat\u00e9 porte une d\u00e9molition ou d\u00e9construction. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_demol_deconst: str D\u00e9molition ou d\u00e9construction si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if page_txt is None: return None elif P_DEMO.search(page_txt): return \"oui\" else: return \"non\"","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_equ_com","text":"D\u00e9termine si l'arr\u00eat\u00e9 porte sur la s\u00e9curit\u00e9 des \u00e9quipements communs.","title":"get_equ_com()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_equ_com--parameters","text":"page_txt: str Texte d'une page de document Returns doc_equ_com: str S\u00e9curit\u00e9 des \u00e9quipements communs si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_equ_com(page_txt: str) -> bool: \"\"\"D\u00e9termine si l'arr\u00eat\u00e9 porte sur la s\u00e9curit\u00e9 des \u00e9quipements communs. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_equ_com: str S\u00e9curit\u00e9 des \u00e9quipements communs si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if page_txt is None: return None elif P_EQU_COM.search(page_txt): return \"oui\" else: return \"non\"","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_int_hab","text":"D\u00e9termine si l'arr\u00eat\u00e9 porte interdiction d'habiter et d'occuper.","title":"get_int_hab()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_int_hab--parameters","text":"page_txt: str Texte d'une page de document Returns doc_int_hab: str Interdiction d'habiter si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_int_hab(page_txt: str) -> bool: \"\"\"D\u00e9termine si l'arr\u00eat\u00e9 porte interdiction d'habiter et d'occuper. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_int_hab: str Interdiction d'habiter si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if page_txt is None: return None elif P_INT_HAB.search(page_txt): return \"oui\" else: return \"non\"","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_urgence","text":"R\u00e9cup\u00e8re le caract\u00e8re d'urgence de l'arr\u00eat\u00e9.","title":"get_urgence()"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_urgence--parameters","text":"page_txt: str Texte d'une page de document","title":"Parameters"},{"location":"Code%20Source/domain_knowledge/#src.domain_knowledge.typologie_securite.get_urgence--returns","text":"doc_class: str Caract\u00e8re d'urgence de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. src\\domain_knowledge\\typologie_securite.py def get_urgence(page_txt: str) -> bool: \"\"\"R\u00e9cup\u00e8re le caract\u00e8re d'urgence de l'arr\u00eat\u00e9. Parameters ---------- page_txt: str Texte d'une page de document Returns ------- doc_class: str Caract\u00e8re d'urgence de l'arr\u00eat\u00e9 si trouv\u00e9, None sinon. \"\"\" # NEW normalisation du texte page_txt = normalize_string(page_txt, num=True, apos=True, hyph=True, spaces=True) # end NEW if ( M_CLASS_PS_PO.search(page_txt) or M_CLASS_PS_PO_MOD.search(page_txt) or M_CLASS_MS.search(page_txt) or M_CLASS_MS_MOD.search(page_txt) ): return \"non\" elif ( M_CLASS_PGI.search(page_txt) or M_CLASS_PGI_MOD.search(page_txt) or M_CLASS_MSU.search(page_txt) or M_CLASS_MSU_MOD.search(page_txt) ): return \"oui\" elif ( M_CLASS_ML_PA.search(page_txt) or M_CLASS_DE.search(page_txt) or M_CLASS_ABRO_DE.search(page_txt) or M_CLASS_INS.search(page_txt) or P_CLASS_INT.search(page_txt) ): # FIXME ajouter la prise en compte des articles cit\u00e9s pour d\u00e9terminer l'urgence # (pas pour le moment car l'info n'est pas fiable, les articles peuvent \u00eatre cit\u00e9s # en paquet) return None elif M_CLASS_ML.search(page_txt) or M_CLASS_ABRO_INT.search(page_txt): return None else: return None","title":"Returns"},{"location":"Code%20Source/preprocess/","text":"Preprocess Fonctions de pr\u00e9traitements des fichiers PDFs. Convertir les fichiers PDF natifs en PDF/A. Utilise ocrmypdf sans appeler le moteur d'OCR. process_files(df_meta, out_pdf_dir, redo=False, keep_pdfa=False, verbose=0) Convertir les fichiers PDF natifs en PDF/A. Parameters df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A. src\\preprocess\\convert_native_pdf_to_pdfa.py def process_files( df_meta: pd.DataFrame, out_pdf_dir: Path, redo: bool = False, keep_pdfa: bool = False, verbose: int = 0, ) -> pd.DataFrame: \"\"\"Convertir les fichiers PDF natifs en PDF/A. Parameters ---------- df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A. \"\"\" if not keep_pdfa: # FIXME pas tr\u00e8s joli fullpath_pdfa = [None for x in df_meta.itertuples()] else: fullpath_pdfa = [] # for df_row in df_meta.itertuples(): # fichier d'origine fp_pdf_in = Path(df_row.fullpath) # fichier \u00e0 produire fp_pdf_out = out_pdf_dir / f\"{fp_pdf_in.name}\" # si le fichier \u00e0 produire existe d\u00e9j\u00e0 if fp_pdf_out.is_file(): if redo: # r\u00e9-ex\u00e9cution explicitement demand\u00e9e: \u00e9mettre une info et traiter le fichier # TODO comparer les versions d'ocrmypdf/tesseract/pikepdf dans les m\u00e9tadonn\u00e9es du PDF de sortie et les versions actuelles des d\u00e9pendances, # et si pertinent \u00e9mettre un message proposant de r\u00e9-analyser le PDF ? logging.info( f\"Re-traitement de {fp_pdf_in}, le fichier de sortie {fp_pdf_out} existant sera \u00e9cras\u00e9.\" ) else: # pas de r\u00e9-ex\u00e9cution demand\u00e9e: \u00e9mettre un warning et passer au fichier suivant logging.info( f\"{fp_pdf_in} est ignor\u00e9 car le fichier {fp_pdf_out} existe d\u00e9j\u00e0.\" ) fullpath_pdfa.append(fp_pdf_out) continue if df_row.processed_as == \"text\" and not df_row.exclude: # convertir le PDF natif (\"texte\") en PDF/A-2b logging.info(f\"Conversion en PDF/A d'un PDF texte: {fp_pdf_in}\") convert_pdf_to_pdfa(fp_pdf_in, fp_pdf_out, verbose=verbose) # TODO stocker la valeur de retour d'ocrmypdf dans une nouvelle colonne \"retcode_pdfa\" ? # stocker le chemin vers le fichier PDF/A produit fullpath_pdfa.append(fp_pdf_out) else: # ignorer le PDF non-natif (\"image\") ; # le fichier PDF/A sera produit lors de l'OCRisation fullpath_pdfa.append(None) # remplir le fichier CSV de sortie df_mmod = df_meta.assign( fullpath_pdfa=fullpath_pdfa, ) # forcer les types des nouvelles colonnes df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_PDFA) return df_mmod Convertir un fichier PDF en PDF/A (archivable). Utilise ocrmypdf. NB: Certaines m\u00e9tadonn\u00e9es du PDF sont perdues https://github.com/ocrmypdf/OCRmyPDF/issues/327 . convert_pdf_to_pdfa(fp_pdf_in, fp_pdf_out, verbose=0) Convertir un PDF en PDF/A. Utilise ocrmypdf sans appliquer d'OCR. Parameters verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity Returns returncode: int 0 si un fichier PDF/A a \u00e9t\u00e9 produit, 1 sinon. src\\preprocess\\convert_to_pdfa.py def convert_pdf_to_pdfa(fp_pdf_in: Path, fp_pdf_out: Path, verbose: int = 0) -> int: \"\"\"Convertir un PDF en PDF/A. Utilise ocrmypdf sans appliquer d'OCR. Parameters ---------- verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- returncode: int 0 si un fichier PDF/A a \u00e9t\u00e9 produit, 1 sinon. \"\"\" try: compl_proc = subprocess.run( [ \"ocrmypdf\", \"-l\", \"fra\", \"--skip-text\", fp_pdf_in, fp_pdf_out, \"--verbose\", str(verbose), ], capture_output=True, check=False, text=True, ) finally: logging.info(compl_proc.stdout) logging.info(compl_proc.stderr) if compl_proc.returncode == ExitCode.pdfa_conversion_failed: # <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> logging.warning( f\"Un PDF a \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9 mais la conversion en PDF/A a \u00e9chou\u00e9: {fp_pdf_out}\" ) # cela arrive quand les m\u00e9tadonn\u00e9es du PDF contiennent des caract\u00e8res que ghostscript consid\u00e8re incorrects # \"DEBUG ocrmypdf.subprocess.gs - GPL Ghostscript 9.54.0: Text string detected in DOCINFO cannot be represented in XMP for PDF/A1, discarding DOCINFO\" # ex: les m\u00e9tadonn\u00e9es PDF contiennent \"Microsoft\u00ae Word 2010\" # <https://stackoverflow.com/questions/57167784/ghostscript-wont-generate-pdf-a-with-utf16be-text-string-detected-in-docinfo> return compl_proc.returncode Sources de donn\u00e9es. D\u00e9terminer le type des fichiers PDF. Un fichier peut \u00eatre consid\u00e9r\u00e9 PDF natif (\"texte\") ou non (\"image\"). Le type est d\u00e9termin\u00e9 pour le fichier entier, sans rentrer dans les cas particuliers commme un PDF texte dans lequel une page num\u00e9ris\u00e9e a \u00e9t\u00e9 ins\u00e9r\u00e9e en tant qu'image. Actuellement, de tels fichiers sont probablement consid\u00e9r\u00e9s comme des fichiers PDF non natifs (\"image\"), en se basant sur les m\u00e9tadonn\u00e9es du fichier PDF. guess_pdf_type(df_row) Devine le type de PDF: natif (\"texte\") ou non (\"image\") Parameters df_row: NamedTuple M\u00e9tadonn\u00e9es et propri\u00e9t\u00e9s du fichier PDF. Returns pdf_type: string, one of {\"text\", \"image\"} Type de PDF: \"text\" pour les PDF natifs, \"image\" pour les autres qui devront \u00eatre OCRis\u00e9s. src\\preprocess\\determine_pdf_type.py def guess_pdf_type(df_row: NamedTuple) -> str: \"\"\"Devine le type de PDF: natif (\"texte\") ou non (\"image\") Parameters ---------- df_row: NamedTuple M\u00e9tadonn\u00e9es et propri\u00e9t\u00e9s du fichier PDF. Returns ------- pdf_type: string, one of {\"text\", \"image\"} Type de PDF: \"text\" pour les PDF natifs, \"image\" pour les autres qui devront \u00eatre OCRis\u00e9s. \"\"\" if pd.notna(df_row.guess_pdftext) and df_row.guess_pdftext: # forte pr\u00e9somption que c'est un PDF texte, d'apr\u00e8s les m\u00e9tadonn\u00e9es pdf_type = \"text\" elif pd.notna(df_row.guess_dernpage) and df_row.guess_dernpage: # (pour les PDF du stock) la derni\u00e8re page est un accus\u00e9 de r\u00e9ception de transmission \u00e0 @ctes, # donc les m\u00e9tadonn\u00e9es ont \u00e9t\u00e9 \u00e9cras\u00e9es et: # 1. il faut exclure la derni\u00e8re page (accus\u00e9 de r\u00e9ception de la transmission) puis # 2. si pdftotext parvient \u00e0 extraire du texte, alors c'est un PDF texte, sinon c'est un PDF image if df_row.retcode_txt == 0: pdf_type = \"text\" else: pdf_type = \"image\" elif pd.notna(df_row.guess_badocr) and df_row.guess_badocr: # le PDF contient une couche d'OCR produite par un logiciel moins performant: refaire l'OCR # # PDF image pdf_type = \"image\" else: # PDF image pdf_type = \"image\" return pdf_type process_files(df_meta) D\u00e9terminer le type des fichiers PDF. Un fichier PDF peut \u00eatre natif (\"texte\") ou non (\"image\"). Parameters df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. src\\preprocess\\determine_pdf_type.py def process_files( df_meta: pd.DataFrame, ) -> pd.DataFrame: \"\"\"D\u00e9terminer le type des fichiers PDF. Un fichier PDF peut \u00eatre natif (\"texte\") ou non (\"image\"). Parameters ---------- df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. \"\"\" processed_as = [] for df_row in df_meta.itertuples(): # fichier d'origine fp_pdf_in = Path(df_row.fullpath) # d\u00e9terminer le type de fichier: PDF natif (\"text\") ou non (\"image\") pdf_type = guess_pdf_type(df_row) processed_as.append(pdf_type) # remplir le fichier CSV de sortie df_mmod = df_meta.assign( processed_as=processed_as, ) # forcer les types des nouvelles colonnes df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_PDFTYPE) return df_mmod Extrait le texte natif des fichiers PDF avec pdfminer.six N\u00e9cessite d'installer pdfminer.six. Non utilis\u00e9 pour le moment, faute d'avoir identifi\u00e9 les bonnes valeurs des param\u00e8tres utilis\u00e9s pour l'analyse du layout, mais pourrait \u00eatre utile pour r\u00e9-analyser les arr\u00eat\u00e9s de certaines communes avec une mise en page compliqu\u00e9e. extract_native_text_pdfminer(fp_pdf_in, fp_txt_out, page_beg, page_end) Extrait le texte natif d'un PDF avec pdfminer.six. Si le texte extrait par pdfminer.six n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\" \", \" \" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\"). Parameters fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). Returns returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. src\\preprocess\\extract_native_text_pdfminer.py def extract_native_text_pdfminer( fp_pdf_in: Path, fp_txt_out: Path, page_beg: int, page_end: int ) -> int: \"\"\"Extrait le texte natif d'un PDF avec pdfminer.six. Si le texte extrait par pdfminer.six n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\"\\x0c\", \"\\f\" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\"). Parameters ---------- fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). Returns ------- returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. \"\"\" # pages \u00e0 extraire # les num\u00e9ros de page commencent \u00e0 1, mais pdftotext cr\u00e9e une liste de pages # qui commence \u00e0 l'index 0 page_beg_ix = page_beg - 1 # le num\u00e9ro de la derni\u00e8re page ne doit pas \u00eatre d\u00e9cal\u00e9 car la borne sup d'un slice est exclue # page_end_ix = page_end page_numbers = list(range(page_beg_ix, page_end)) # TODO v\u00e9rifier que le texte contient bien \"\\f\" en fin de page txt = extract_text(fp_pdf_in, page_numbers=page_numbers, laparams=LAPARAMS) # codec=\"utf-8\" ? \"latin-1\"? # with open(fp_pdf_in, \"rb\") as f, open(fp_txt_out, \"w\") as f_txt: # extract_text_to_fp(f, f_txt, page_numbers=page_numbers, output_type=\"text\") # normaliser le texte extrait en forme NFC norm_txt = unicodedata.normalize(\"NFC\", txt) # if norm_txt: # stocker le texte dans un fichier .txt with open(fp_txt_out, \"w\") as f_txt: f_txt.write(norm_txt) # code ok return 0 else: # code d'erreur return 1 Extrait le texte natif des fichiers PDF avec pdftotext https://github.com/jalan/pdftotext D\u00e9pendances Windows ( https://github.com/jalan/pdftotext#os-dependencies ): * Microsoft Visual C++ Build Tools: https://visualstudio.microsoft.com/fr/visual-cpp-build-tools/ * poppler ( conda install -c conda-forge poppler ) extract_native_text_pdftotext(fp_pdf_in, fp_txt_out, page_beg, page_end) Extrait le texte natif d'un PDF avec pdftotext. Si le texte extrait par pdftotext n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\" \", \" \" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\"). Parameters fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). Returns returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. src\\preprocess\\extract_native_text_pdftotext.py def extract_native_text_pdftotext( fp_pdf_in: Path, fp_txt_out: Path, page_beg: int, page_end: int ) -> int: \"\"\"Extrait le texte natif d'un PDF avec pdftotext. Si le texte extrait par pdftotext n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\"\\x0c\", \"\\f\" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\"). Parameters ---------- fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). Returns ------- returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. \"\"\" # les num\u00e9ros de page commencent \u00e0 1, mais pdftotext cr\u00e9e une liste de pages # qui commence \u00e0 l'index 0 page_beg_ix = page_beg - 1 # le num\u00e9ro de la derni\u00e8re page ne doit pas \u00eatre d\u00e9cal\u00e9 car la borne sup d'un slice est exclue # page_end_ix = page_end with open(fp_pdf_in, \"rb\") as f: try: pdf = pdftotext.PDF(f) except pdftotext.Error as e: logging.error(f\"erreur pdftotext: {e}\") return 1 # code d'erreur # pdftotext.PDF a getitem(), mais ne permet pas de r\u00e9cup\u00e9rer un slice # donc il faut cr\u00e9er un range et it\u00e9rer manuellement doc_txt = [pdf[i] for i in range(page_beg_ix, page_end)] # chaque page produite par pdftotext se termine par \"\\f\", # il faut enlever le dernier \"\\f\" pour avoir la m\u00eame # structure qu'en sortie d'ocrmypdf assert doc_txt[-1][-1] == \"\\f\" doc_txt[-1] = doc_txt[-1][:-1] # concat\u00e9ner le texte des pages txt = \"\".join(doc_txt) # .strip() ? # normaliser le texte extrait en forme NFC norm_txt = unicodedata.normalize(\"NFC\", txt) # stocker le texte dans un fichier .txt with open(fp_txt_out, \"w\") as f_txt: f_txt.write(norm_txt) return 0 # code ok Extraire le texte de fichiers PDF par OCR. Utilise ocrmypdf. extract_text_from_pdf_image(fp_pdf_in, fp_txt_out, fp_pdf_out, page_beg, page_end, redo_ocr=False, verbose=0) Extraire le texte d'un PDF image et convertir le fichier en PDF/A. Utilise ocrmypdf. On utilise \"-l fra\" pour am\u00e9liorer la reconnaissance de: \"\u00e0\", \"\u00e8\", \"\u00ea\", apostrophe, \"l'\", \"\u0153\", \"\u00f4\" etc. Parameters fp_pdf_in: Path Fichier PDF image \u00e0 traiter. fp_txt_out: Path Fichier TXT produit, contenant le texte extrait par OCR. fp_pdf_out: Path Fichier PDF/A produit, incluant le texte oc\u00e9ris\u00e9. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). redo_ocr: boolean, defaults to False Si True, refait l'OCR m\u00eame si une couche d'OCR est d\u00e9tect\u00e9e sur certaines pages. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity Returns returncode: int 0 si deux fichiers PDF/A et TXT ont \u00e9t\u00e9 produits, une autre valeur sinon https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy . src\\preprocess\\extract_text_ocr_ocrmypdf.py def extract_text_from_pdf_image( fp_pdf_in: Path, fp_txt_out: Path, fp_pdf_out: Path, page_beg: int, page_end: int, redo_ocr: bool = False, verbose: int = 0, ) -> int: \"\"\"Extraire le texte d'un PDF image et convertir le fichier en PDF/A. Utilise ocrmypdf. On utilise \"-l fra\" pour am\u00e9liorer la reconnaissance de: \"\u00e0\", \"\u00e8\", \"\u00ea\", apostrophe, \"l'\", \"\u0153\", \"\u00f4\" etc. Parameters ---------- fp_pdf_in: Path Fichier PDF image \u00e0 traiter. fp_txt_out: Path Fichier TXT produit, contenant le texte extrait par OCR. fp_pdf_out: Path Fichier PDF/A produit, incluant le texte oc\u00e9ris\u00e9. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). redo_ocr: boolean, defaults to False Si True, refait l'OCR m\u00eame si une couche d'OCR est d\u00e9tect\u00e9e sur certaines pages. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- returncode: int 0 si deux fichiers PDF/A et TXT ont \u00e9t\u00e9 produits, une autre valeur sinon <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> . \"\"\" # appeler ocrmypdf pour produire 2 fichiers: PDF/A-2b (inc. OCR) + sidecar (txt) cmd = ( [\"ocrmypdf\"] + [ # langue fran\u00e7aise \"-l\", \"fra\", # s\u00e9lection de pages \"--page\", f\"{page_beg}-{page_end}\", # TXT en sortie \"--sidecar\", fp_txt_out, # verbosit\u00e9 \"--verbose\", str(verbose), ] + ([\"--redo-ocr\"] if redo_ocr else []) + [ # PDF en entr\u00e9e fp_pdf_in, # PDF/A en sortie fp_pdf_out, ] ) try: compl_proc = subprocess.run( cmd, capture_output=True, check=False, text=True, ) finally: logging.info(compl_proc.stdout) logging.info(compl_proc.stderr) if compl_proc.returncode == ExitCode.pdfa_conversion_failed: # <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> # <https://ocrmypdf.readthedocs.io/en/v14.0.4/apiref.html#ocrmypdf.exceptions.ExitCode> logging.warning( f\"Un PDF a \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9 mais la conversion en PDF/A a \u00e9chou\u00e9: {fp_pdf_out}\" ) # cela arrive quand les m\u00e9tadonn\u00e9es du PDF contiennent des caract\u00e8res que ghostscript consid\u00e8re incorrects # \"DEBUG ocrmypdf.subprocess.gs - GPL Ghostscript 9.54.0: Text string detected in DOCINFO cannot be represented in XMP for PDF/A1, discarding DOCINFO\" # ex: les m\u00e9tadonn\u00e9es PDF contiennent \"Microsoft\u00ae Word 2010\" # <https://stackoverflow.com/questions/57167784/ghostscript-wont-generate-pdf-a-with-utf16be-text-string-detected-in-docinfo> return compl_proc.returncode Extraire le texte des fichiers PDF par OCR. Le texte des PDF non-natifs (\"PDF image\") est extrait avec ocrmypdf. ocrmypdf produit un fichier PDF/A incluant une couche de texte extrait par OCR, et un fichier \"sidecar\" contenant le texte extrait par l'OCR uniquement. Le fichier PDF/A est effac\u00e9, sauf mention contraire explicite par l'option \"keep_pdfa\". preprocess_pdf_file(df_row, fp_pdf_in, fp_pdf_out, fp_txt_out, verbose=0) Extraire le texte par OCR et g\u00e9n\u00e9rer des fichiers PDF/A et txt. Le fichier PDF est OCRis\u00e9 avec ocrmypdf, qui cr\u00e9e un PDF/A et un fichier texte \"sidecar\". La version actuelle est: ocrmypdf 14.0.3 / Tesseract OCR-PDF 5.2.0 (+ pikepdf 5.6.1). Parameters df_row: NamedTuple M\u00e9tadonn\u00e9es et informations sur le fichier PDF \u00e0 traiter. fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. fp_pdf_out: Path Chemin du fichier PDF converti en PDF/A (avec OCR le cas \u00e9ch\u00e9ant). fp_txt_out: Path Chemin du fichier txt contenant le texte extrait. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity Returns retcode: int Code de retour d'ocrmypdf https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy . src\\preprocess\\extract_text_ocr.py def preprocess_pdf_file( df_row: NamedTuple, fp_pdf_in: Path, fp_pdf_out: Path, fp_txt_out: Path, verbose: int = 0, ) -> int: \"\"\"Extraire le texte par OCR et g\u00e9n\u00e9rer des fichiers PDF/A et txt. Le fichier PDF est OCRis\u00e9 avec ocrmypdf, qui cr\u00e9e un PDF/A et un fichier texte \"sidecar\". La version actuelle est: ocrmypdf 14.0.3 / Tesseract OCR-PDF 5.2.0 (+ pikepdf 5.6.1). Parameters ---------- df_row: NamedTuple M\u00e9tadonn\u00e9es et informations sur le fichier PDF \u00e0 traiter. fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. fp_pdf_out: Path Chemin du fichier PDF converti en PDF/A (avec OCR le cas \u00e9ch\u00e9ant). fp_txt_out: Path Chemin du fichier txt contenant le texte extrait. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- retcode: int Code de retour d'ocrmypdf <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> . \"\"\" logging.info(f\"Ouverture du fichier {fp_pdf_in}\") # d\u00e9finir les pages \u00e0 traiter page_beg = 1 # exclure la derni\u00e8re page de l'OCRisation, si c'est un accus\u00e9 de r\u00e9ception de transmission \u00e0 @ctes # TODO g\u00e9rer les cas o\u00f9 l'AR de transmission n'est pas en derni\u00e8re page car suivi d'annexes page_end = ( df_row.nb_pages - 1 if (pd.notna(df_row.guess_dernpage) and df_row.guess_dernpage) else df_row.nb_pages ) # Si un PDF est susceptible de contenir des couches d'OCR de mauvaise qualit\u00e9, indiquer \u00e0 # ocrmypdf de les ignorer et de refaire l'OCR, avec \"--redo-ocr\" (CLI) / \"redo_ocr\" (ici) # (si cela ne fonctionne pas ou pas toujours, modifier le code pour remplacer \"--redo-ocr\" # par \"--force-ocr\" qui forcera la rasterization des pages avant de leur appliquer l'OCR) # FIXME ? force ocr pour les PDF avec une mauvaise OCR, eg. \"Image Capture Plus\" ? redo_ocr = True # redo_ocr = df_row.processed_as == \"image\" # toujours vrai? assert df_row.processed_as == \"image\" logging.info(f\"PDF image: {fp_pdf_in}\") retcode = extract_text_from_pdf_image( fp_pdf_in, fp_txt_out, fp_pdf_out, page_beg=page_beg, page_end=page_end, redo_ocr=redo_ocr, verbose=verbose, ) return retcode process_files(df_meta, out_pdf_dir, out_txt_dir, redo=False, keep_pdfa=False, verbose=0) Traiter un ensemble de fichiers PDF: convertir les PDF en PDF/A et extraire le texte. Parameters df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. out_txt_dir: Path Dossier de sortie pour les fichiers texte. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. src\\preprocess\\extract_text_ocr.py def process_files( df_meta: pd.DataFrame, out_pdf_dir: Path, out_txt_dir: Path, redo: bool = False, keep_pdfa: bool = False, verbose: int = 0, ) -> pd.DataFrame: \"\"\"Traiter un ensemble de fichiers PDF: convertir les PDF en PDF/A et extraire le texte. Parameters ---------- df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. out_txt_dir: Path Dossier de sortie pour les fichiers texte. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. \"\"\" retcode_ocr = [] fullpath_pdfa = [] fullpath_txt = [] for i, df_row in enumerate(df_meta.itertuples()): if i != 0 and i % 10 == 0: print(f\"{i}/{len(df_meta)} pdf trait\u00e9s\") # fichier d'origine fp_pdf_in = Path(df_row.fullpath) # fichiers \u00e0 produire fp_pdf_out = out_pdf_dir / f\"{fp_pdf_in.name}\" fp_txt = out_txt_dir / (f\"{fp_pdf_in.stem}\" + \".txt\") # sauter les fichiers identifi\u00e9s comme PDF texte, et les fichiers exclus # et simplement reporter les chemins vers les fichiers txt et \u00e9ventuellement PDF/A if df_row.processed_as == \"text\" or df_row.exclude: retcode_ocr.append(None) # valeur de retour ocrmypdf fullpath_pdfa.append(df_row.fullpath_pdfa) fullpath_txt.append(df_row.fullpath_txt) continue # sinon processed_as est \"image\" (et pas <NA>) assert df_row.processed_as == \"image\" # et le fichier n'est pas exclus (et pas <NA>) assert not df_row.exclude # si le fichier txt \u00e0 produire existe d\u00e9j\u00e0 if fp_txt.is_file(): if redo: # r\u00e9-ex\u00e9cution explicitement demand\u00e9e: \u00e9mettre une info et traiter le fichier # TODO comparer les versions d'ocrmypdf/tesseract/pikepdf dans les m\u00e9tadonn\u00e9es du PDF de sortie et les versions actuelles des d\u00e9pendances, # et si pertinent \u00e9mettre un message proposant de r\u00e9-analyser le PDF ? logging.info( f\"Re-traitement de {fp_pdf_in}, le fichier de sortie {fp_txt} existant sera \u00e9cras\u00e9.\" ) else: # pas de r\u00e9-ex\u00e9cution demand\u00e9e: \u00e9mettre un warning et passer au fichier suivant logging.info( f\"{fp_pdf_in} est ignor\u00e9 car le fichier {fp_txt} existe d\u00e9j\u00e0.\" ) retcode_ocr.append( None ) # valeur de retour ocrmypdf, impossible \u00e0 r\u00e9cup\u00e9rer sans refaire tourner la conversion if fp_pdf_out.is_file(): fullpath_pdfa.append(fp_pdf_out) else: fullpath_pdfa.append(None) fullpath_txt.append(fp_txt) continue # traiter le fichier: extraire le texte par OCR si n\u00e9cessaire, corriger et convertir le PDF d'origine en PDF/A-2b retcode = preprocess_pdf_file( df_row, fp_pdf_in, fp_pdf_out, fp_txt, verbose=verbose ) # stocker les chemins: fichier TXT (OCR), \u00e9ventuellement PDF/A retcode_ocr.append(retcode) # valeur de retour ocrmypdf fullpath_txt.append(fp_txt) if not keep_pdfa: os.remove(fp_pdf_out) fullpath_pdfa.append(None) else: fullpath_pdfa.append(fp_pdf_out) df_mmod = df_meta.assign( retcode_ocr=retcode_ocr, fullpath_pdfa=fullpath_pdfa, fullpath_txt=fullpath_txt, ) # forcer les types des nouvelles colonnes df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_OCR) return df_mmod Filtrer les fichiers PDF hors du champ de la base de donn\u00e9es. Annexes des arr\u00eat\u00e9s: plan de p\u00e9rim\u00e8tre de s\u00e9curit\u00e9, rapports d'expertise etc. TODO filtrer automatiquement \u00e0 partir du texte process_files(df_meta, df_txts) Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns df_mmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers, filtr\u00e9s. df_tmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. src\\preprocess\\filter_docs.py def process_files( df_meta: pd.DataFrame, df_txts: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters ---------- df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns ------- df_mmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers, filtr\u00e9s. df_tmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. \"\"\" df_mmod = df_meta.assign(exclude=(lambda x: x.pdf.isin(SET_EXCLUDE))) df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_FILT) df_tmod = df_txts.assign(exclude=(lambda x: x.pdf.isin(SET_EXCLUDE))) df_tmod = df_tmod.astype(dtype=DTYPE_NTXT_PAGES_FILT) return df_mmod, df_tmod Extraire les m\u00e9tadonn\u00e9es des fichiers PDF. Ce module utilise pikepdf. get_pdf_info(fp_pdf, digest='blake2b', verbose=False) Extraire les informations (dont m\u00e9tadonn\u00e9es) d'un fichier PDF. Utilise actuellement pikepdf. Parameters fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Algorithme de hachage \u00e0 utiliser https://docs.python.org/3/library/hashlib.html#hash-algorithms . verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF. Returns pdf_info : dict Informations (dont m\u00e9tadonn\u00e9es) du fichier PDF d'entr\u00e9e src\\preprocess\\pdf_info.py def get_pdf_info( fp_pdf: Path, digest: str = \"blake2b\", verbose: bool = False ) -> Dict[str, str | int]: \"\"\"Extraire les informations (dont m\u00e9tadonn\u00e9es) d'un fichier PDF. Utilise actuellement pikepdf. Parameters ---------- fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Algorithme de hachage \u00e0 utiliser <https://docs.python.org/3/library/hashlib.html#hash-algorithms> . verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF. Returns ------- pdf_info : dict Informations (dont m\u00e9tadonn\u00e9es) du fichier PDF d'entr\u00e9e \"\"\" logging.info(f\"Ouverture du fichier {fp_pdf}\") pdf_info = { # m\u00e9tadonn\u00e9es du fichier lui-m\u00eame \"pdf\": fp_pdf.name, # nom du fichier \"fullpath\": fp_pdf.resolve(), # chemin complet \"filesize\": fp_pdf.stat().st_size, # taille du fichier digest: get_file_digest(fp_pdf, digest=digest), # hash du fichier } # lire les m\u00e9tadonn\u00e9es du PDF avec pikepdf meta_pike = get_pdf_info_pikepdf(fp_pdf, verbose=verbose) # ajouter les m\u00e9tadonn\u00e9es PDF \u00e0 celles du fichier pdf_info.update(meta_pike) return pdf_info get_pdf_info_pikepdf(fp_pdf_in, verbose=False) Renvoie les infos du PDF en utilisant pikepdf. Les infos incluent un sous-ensemble des m\u00e9tadonn\u00e9es du PDF. Parameters fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF. Returns infos: dict Dictionnaire contenant les infos du PDF. src\\preprocess\\pdf_info.py def get_pdf_info_pikepdf(fp_pdf_in: Path, verbose: bool = False) -> dict: \"\"\"Renvoie les infos du PDF en utilisant pikepdf. Les infos incluent un sous-ensemble des m\u00e9tadonn\u00e9es du PDF. Parameters ---------- fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF. Returns ------- infos: dict Dictionnaire contenant les infos du PDF. \"\"\" with pikepdf.open(fp_pdf_in) as f_pdf: with f_pdf.open_metadata(set_pikepdf_as_editor=False) as meta: # lire les m\u00e9tadonn\u00e9es stock\u00e9es en XMP (\"nouveau\" format) meta_base = {k: v for k, v in meta.items()} if verbose: try: logging.info( f\"{fp_pdf_in.name}: m\u00e9tadonn\u00e9es XMP brutes: {f_pdf.Root.Metadata.read_bytes().decode()}\" ) except AttributeError: logging.warning( f\"{fp_pdf_in.name}: absence de m\u00e9tadonn\u00e9es XMP brutes\" ) logging.info(f\"{fp_pdf_in.name}: m\u00e9tadonn\u00e9es XMP: {meta_base}\") # lire les m\u00e9tadonn\u00e9es stock\u00e9es dans docinfo (ancien format) meta.load_from_docinfo(f_pdf.docinfo) # NB: load_from_docinfo() peut lever un UserWarning, qui est alors inclus dans le log de ce script # <https://github.com/pikepdf/pikepdf/blob/94c50cd408b214f7569a717c3409e36b7a996769/src/pikepdf/models/metadata.py#L438> # ex: \"UserWarning: The metadata field /MetadataDate with value 'pikepdf.String(\"D:20230117110535+01'00'\")' has no XMP equivalent, so it was discarded\" meta_doci = {k: v for k, v in meta.items()} if verbose: logging.info(f\"{fp_pdf_in.name}: docinfo: {repr(f_pdf.docinfo)}\") logging.info(f\"{fp_pdf_in.name}: m\u00e9tadonn\u00e9es XMP+docinfo: {meta_doci}\") # comparaison des m\u00e9tadonn\u00e9es: XMP seul vs XMP mis \u00e0 jour avec docinfo base_keys = set(meta_base.keys()) doci_keys = set(meta_doci.keys()) # v\u00e9rifier que la lecture de docinfo n'a pas supprim\u00e9 de champ aux m\u00e9tadonn\u00e9es XMP assert (base_keys - doci_keys) == set() # v\u00e9rifier que les champs charg\u00e9s depuis docinfo n'ont modifi\u00e9 aucune valeur de champ XMP # (pas de modification / \u00e9crasement, condition plus forte que supra) for key, value in meta_base.items(): if key.endswith(\"Date\"): # traitement sp\u00e9cifique pour les dates: gestion de diff\u00e9rents formats + tol\u00e9rance de 2h pour les timezones base_v = datetime.fromisoformat( value.replace(\"Z\", \"+00:00\") ).astimezone(tz=TZ_FRA) doci_v = datetime.fromisoformat( meta_doci[key].replace(\"Z\", \"+00:00\") ).astimezone(tz=TZ_FRA) # base_eq_doci = abs(base_v - doci_v) <= timedelta(hours=1) # si besoin de permissivit\u00e9 base_eq_doci = doci_v == base_v else: # comparaison par d\u00e9faut: \u00e9galit\u00e9 stricte base_v = value doci_v = meta_doci[key] base_eq_doci = doci_v == base_v if not base_eq_doci: logging.warning( f\"{fp_pdf_in}: metadata: {key}={base_v} (xmp) vs {doci_v} (docinfo)\" ) if verbose: logging.info(f\"{fp_pdf_in}: pike:finalmetadata: {meta}\") # s\u00e9lection des champs et fixation de leur ordre infos = { \"nb_pages\": len(f_pdf.pages), # nombre de pages # m\u00e9tadonn\u00e9es PDF \"creatortool\": meta.get(\"xmp:CreatorTool\", \"\"), # string \"producer\": meta.get(\"pdf:Producer\", \"\"), # string \"createdate\": meta.get(\"xmp:CreateDate\", None), # date \"modifydate\": meta.get(\"xmp:ModifyDate\", None), # date } # analyse des dates if infos[\"createdate\"] is not None: infos[\"createdate\"] = datetime.fromisoformat(infos[\"createdate\"]).astimezone( tz=TZ_FRA ) if infos[\"modifydate\"] is not None: infos[\"modifydate\"] = datetime.fromisoformat(infos[\"modifydate\"]).astimezone( tz=TZ_FRA ) # WIP regarder si des champs sont toujours/souvent/jamais renseign\u00e9s if meta.get(\"dc:format\", None) is not None: assert meta.get(\"dc:format\", None) == \"application/pdf\" # return infos Traiter les m\u00e9tadonn\u00e9es des fichiers PDF. Les traitements permettent de: * d\u00e9tecter les fichiers doublons, * d\u00e9terminer si le PDF est du texte ou image, * d\u00e9terminer si le PDF contient des tampons de t\u00e9l\u00e9transmission en haut des pages, * d\u00e9terminer si le PDF contient une derni\u00e8re page qui est l'accus\u00e9 de r\u00e9ception de la t\u00e9l\u00e9transmission. La liste des tiers de transmission agr\u00e9\u00e9s pour @ctes est sur https://www.collectivites-locales.gouv.fr/sites/default/files/migration/2019_09_13_liste_operateurs_transmission_0.pdf . guess_badocr(df_meta) D\u00e9termine si le fichier contient une couche OCR de pi\u00e8tre qualit\u00e9. Arrive quand le champ \"creatortool\" vaut \"Image Capture Plus\". Parameters df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_badocr\" src\\preprocess\\process_metadata.py def guess_badocr(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si le fichier contient une couche OCR de pi\u00e8tre qualit\u00e9. Arrive quand le champ \"creatortool\" vaut \"Image Capture Plus\". Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_badocr\" \"\"\" has_badocr = ( ( # \"Image Capture Plus\" df_meta[\"creatortool\"].str.strip() == \"Image Capture Plus\" ) | ( # \"Adobe PSL 1.2e for Canon\" (ou 1.1e, 1.3e) df_meta[\"producer\"] .str.strip() .str.startswith(\"Adobe PSL\") ) | ( # \"Canon\" (df_meta[\"producer\"].str.strip() == \"\") & (df_meta[\"creatortool\"].str.strip() == \"Canon\") ) ) df_mmod = df_meta.assign(guess_badocr=has_badocr) return df_mmod guess_dernpage_transmission(df_meta) D\u00e9termine si la derni\u00e8re page est un accus\u00e9 de r\u00e9ception de t\u00e9l\u00e9transmission. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission. Parameters df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_dernpage\" src\\preprocess\\process_metadata.py def guess_dernpage_transmission(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si la derni\u00e8re page est un accus\u00e9 de r\u00e9ception de t\u00e9l\u00e9transmission. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission. Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_dernpage\" \"\"\" has_dernpage = ( # @ctes (toute la p\u00e9riode?) df_meta[\"producer\"] == \"iText 2.1.7 by 1T3XT\" ) df_mmod = df_meta.assign(guess_dernpage=has_dernpage) return df_mmod guess_duplicates_meta(df_meta, hash_fn='blake2b') D\u00e9termine si les fichiers PDF sont des doublons \u00e0 partir de leurs m\u00e9tadonn\u00e9es. Parameters df_meta : pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF hash_fn : str Nom de la fonction de hachage, doit \u00eatre un nom de colonne du DataFrame de m\u00e9tadonn\u00e9es. Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF, avec des colonnes bool\u00e9ennes \"dup_*\" indiquant les fichiers doublons. src\\preprocess\\process_metadata.py def guess_duplicates_meta(df_meta: pd.DataFrame, hash_fn: str = \"blake2b\"): \"\"\"D\u00e9termine si les fichiers PDF sont des doublons \u00e0 partir de leurs m\u00e9tadonn\u00e9es. Parameters ---------- df_meta : pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF hash_fn : str Nom de la fonction de hachage, doit \u00eatre un nom de colonne du DataFrame de m\u00e9tadonn\u00e9es. Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF, avec des colonnes bool\u00e9ennes \"dup_*\" indiquant les fichiers doublons. \"\"\" # d\u00e9tection stricte: doublons sur toutes les infos (sauf \"pdf\" et \"fullpath\") # (trop de faux n\u00e9gatifs?) cols_dups_allinfo = [ # infos fichier \"filesize\", \"nb_pages\", # m\u00e9tadonn\u00e9es pdf \"creatortool\", \"producer\", \"createdate\", \"modifydate\", ] s_dups_allinfo = _guess_duplicates(df_meta, cols_dups_allinfo) df_mmod = df_meta.assign(dup_allinfo=s_dups_allinfo) # d\u00e9tection l\u00e2che: doublons sur la date de cr\u00e9ation # (trop de faux positifs) cols_dups_createdate = [\"createdate\"] s_dups_createdate = _guess_duplicates(df_mmod, cols_dups_createdate) df_mmod = df_mmod.assign(dup_createdate=s_dups_createdate) # d\u00e9tection bas\u00e9e sur le hachage des fichiers cols_dups_hash = [hash_fn] s_dups_hash = _guess_duplicates(df_mmod, cols_dups_hash) df_mmod = df_mmod.assign(dup_hash=s_dups_hash) # return df_mmod guess_pdftext(df_meta) D\u00e9termine si le fichier est un PDF texte (ou \"num\u00e9rique natif\"). Parameters df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_pdftext\" src\\preprocess\\process_metadata.py def guess_pdftext(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si le fichier est un PDF texte (ou \"num\u00e9rique natif\"). Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_pdftext\" \"\"\" is_pdftext = ( # \"Microsoft\u00ae Word 2010\", \"Microsoft\u00ae Word 2013\", \"Microsoft\u00ae Word pour Microsoft 365\" df_meta[\"creatortool\"].str.startswith(\"Microsoft\u00ae Word\") # \"Writer\" (OpenOffice, LibreOffice) | (df_meta[\"creatortool\"] == \"Writer\") ) df_mmod = df_meta.assign(guess_pdftext=is_pdftext) return df_mmod guess_tampon_transmission(df_meta) D\u00e9termine si le haut des pages contient des tampons de transmission \u00e9lectronique. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission. Parameters df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_tampon\" src\\preprocess\\process_metadata.py def guess_tampon_transmission(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si le haut des pages contient des tampons de transmission \u00e9lectronique. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission. Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_tampon\" \"\"\" has_stamp = ~( # faux positif: MS 365 + iText utilis\u00e9 pour ajouter la signature df_meta[\"producer\"] == \"Microsoft\u00ae Word pour Microsoft 365; modified using iText\u00ae 5.5.9 \u00a92000-2015 iText Group NV (AGPL-version)\" ) & ( df_meta[\"producer\"].str.endswith( # tampon en haut \u00e0 droite: tiers de t\u00e9l\u00e9transmission S2LOW (2019-06-14 - ..) et Berger Levrault (2021-02-08) \"; modified using iText\u00ae 7.1.5 \u00a92000-2019 iText Group NV (AGPL-version)\" ) | df_meta[\"producer\"].str.endswith( # tampon en haut \u00e0 droite: tiers de t\u00e9l\u00e9transmission S2LOW (.. - 2019-02-11) \"; modified using iText\u00ae 5.5.12 \u00a92000-2017 iText Group NV (AGPL-version)\" ) | df_meta[\"producer\"].str.endswith( # tampon en bas \u00e0 gauche (quel tiers?): \"; modified using iText\u00ae 5.5.9 \u00a92000-2015 iText Group NV (AGPL-version)\" (sans MS 365) \"; modified using iText\u00ae 5.5.9 \u00a92000-2015 iText Group NV (AGPL-version)\" ) ) # df_mmod = df_meta.assign(guess_tampon=has_stamp) return df_mmod Charge le texte des documents dans un DataFrame. Chaque ligne correspond \u00e0 une page d'un document. create_pages_dataframe(df_meta) Charger le texte des documents dans un DataFrame. Une entr\u00e9e par page de document. Parameters df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des documents. Returns df_txts: pd.DataFrame Tableau contenant le texte des documents, s\u00e9par\u00e9 par page. src\\preprocess\\separate_pages.py def create_pages_dataframe( df_meta: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Charger le texte des documents dans un DataFrame. Une entr\u00e9e par page de document. Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des documents. Returns ------- df_txts: pd.DataFrame Tableau contenant le texte des documents, s\u00e9par\u00e9 par page. \"\"\" page_txts = [] for df_row in df_meta.itertuples(): if pd.isna(df_row.fullpath_txt): # cr\u00e9er une entr\u00e9e vide par page du PDF pages = [ { # m\u00e9tadonn\u00e9es de la page \"pagenum\": i, # texte de la page \"pagetxt\": None, } for i in range(1, df_row.nb_pages + 1) ] else: # cr\u00e9er une entr\u00e9e par page de texte doc_txt = load_pages_text(df_row.fullpath_txt) # v\u00e9rifier que le fichier TXT contient autant de pages que le PDF try: assert len(doc_txt) == df_row.nb_pages except AssertionError: print(repr(df_row)) print( f\"{len(doc_txt)} pages de texte != {df_row.nb_pages} pages dans le fichier PDF\" ) raise # pour chaque page, charger le texte pages = [ { # m\u00e9tadonn\u00e9es de la page \"pagenum\": i, # texte de la page \"pagetxt\": page_txt, } for i, page_txt in enumerate(doc_txt, start=1) ] # dupliquer les m\u00e9tadonn\u00e9es du fichier PDF et du TXT, dans chaque entr\u00e9e de page doc_rows = [ {x: getattr(df_row, x) for x in COLS_DOC} | page # python >= 3.9 (dict union) for page in pages ] # v\u00e9rifier que le nombre de pages de texte extrait est inf\u00e9rieur ou \u00e9gal au nombre de pages du PDF # (certaines pages peuvent \u00eatre blanches, ne contenir que des images ou photos...) # TODO v\u00e9rifier redondance avec l'assertion ci-dessus? assert len(doc_rows) <= df_row.nb_pages page_txts.extend(doc_rows) df_txts = pd.DataFrame.from_records(page_txts) df_txts = df_txts.astype(dtype=DTYPE_NTXT_PAGES) return df_txts","title":"Preprocess"},{"location":"Code%20Source/preprocess/#preprocess","text":"Fonctions de pr\u00e9traitements des fichiers PDFs.","title":"Preprocess"},{"location":"Code%20Source/preprocess/#src.preprocess.convert_native_pdf_to_pdfa--convertir-les-fichiers-pdf-natifs-en-pdfa","text":"Utilise ocrmypdf sans appeler le moteur d'OCR.","title":"Convertir les fichiers PDF natifs en PDF/A."},{"location":"Code%20Source/preprocess/#src.preprocess.convert_native_pdf_to_pdfa.process_files","text":"Convertir les fichiers PDF natifs en PDF/A.","title":"process_files()"},{"location":"Code%20Source/preprocess/#src.preprocess.convert_native_pdf_to_pdfa.process_files--parameters","text":"df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.convert_native_pdf_to_pdfa.process_files--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A. src\\preprocess\\convert_native_pdf_to_pdfa.py def process_files( df_meta: pd.DataFrame, out_pdf_dir: Path, redo: bool = False, keep_pdfa: bool = False, verbose: int = 0, ) -> pd.DataFrame: \"\"\"Convertir les fichiers PDF natifs en PDF/A. Parameters ---------- df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A. \"\"\" if not keep_pdfa: # FIXME pas tr\u00e8s joli fullpath_pdfa = [None for x in df_meta.itertuples()] else: fullpath_pdfa = [] # for df_row in df_meta.itertuples(): # fichier d'origine fp_pdf_in = Path(df_row.fullpath) # fichier \u00e0 produire fp_pdf_out = out_pdf_dir / f\"{fp_pdf_in.name}\" # si le fichier \u00e0 produire existe d\u00e9j\u00e0 if fp_pdf_out.is_file(): if redo: # r\u00e9-ex\u00e9cution explicitement demand\u00e9e: \u00e9mettre une info et traiter le fichier # TODO comparer les versions d'ocrmypdf/tesseract/pikepdf dans les m\u00e9tadonn\u00e9es du PDF de sortie et les versions actuelles des d\u00e9pendances, # et si pertinent \u00e9mettre un message proposant de r\u00e9-analyser le PDF ? logging.info( f\"Re-traitement de {fp_pdf_in}, le fichier de sortie {fp_pdf_out} existant sera \u00e9cras\u00e9.\" ) else: # pas de r\u00e9-ex\u00e9cution demand\u00e9e: \u00e9mettre un warning et passer au fichier suivant logging.info( f\"{fp_pdf_in} est ignor\u00e9 car le fichier {fp_pdf_out} existe d\u00e9j\u00e0.\" ) fullpath_pdfa.append(fp_pdf_out) continue if df_row.processed_as == \"text\" and not df_row.exclude: # convertir le PDF natif (\"texte\") en PDF/A-2b logging.info(f\"Conversion en PDF/A d'un PDF texte: {fp_pdf_in}\") convert_pdf_to_pdfa(fp_pdf_in, fp_pdf_out, verbose=verbose) # TODO stocker la valeur de retour d'ocrmypdf dans une nouvelle colonne \"retcode_pdfa\" ? # stocker le chemin vers le fichier PDF/A produit fullpath_pdfa.append(fp_pdf_out) else: # ignorer le PDF non-natif (\"image\") ; # le fichier PDF/A sera produit lors de l'OCRisation fullpath_pdfa.append(None) # remplir le fichier CSV de sortie df_mmod = df_meta.assign( fullpath_pdfa=fullpath_pdfa, ) # forcer les types des nouvelles colonnes df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_PDFA) return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.convert_to_pdfa--convertir-un-fichier-pdf-en-pdfa-archivable","text":"Utilise ocrmypdf. NB: Certaines m\u00e9tadonn\u00e9es du PDF sont perdues https://github.com/ocrmypdf/OCRmyPDF/issues/327 .","title":"Convertir un fichier PDF en PDF/A (archivable)."},{"location":"Code%20Source/preprocess/#src.preprocess.convert_to_pdfa.convert_pdf_to_pdfa","text":"Convertir un PDF en PDF/A. Utilise ocrmypdf sans appliquer d'OCR.","title":"convert_pdf_to_pdfa()"},{"location":"Code%20Source/preprocess/#src.preprocess.convert_to_pdfa.convert_pdf_to_pdfa--parameters","text":"verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.convert_to_pdfa.convert_pdf_to_pdfa--returns","text":"returncode: int 0 si un fichier PDF/A a \u00e9t\u00e9 produit, 1 sinon. src\\preprocess\\convert_to_pdfa.py def convert_pdf_to_pdfa(fp_pdf_in: Path, fp_pdf_out: Path, verbose: int = 0) -> int: \"\"\"Convertir un PDF en PDF/A. Utilise ocrmypdf sans appliquer d'OCR. Parameters ---------- verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- returncode: int 0 si un fichier PDF/A a \u00e9t\u00e9 produit, 1 sinon. \"\"\" try: compl_proc = subprocess.run( [ \"ocrmypdf\", \"-l\", \"fra\", \"--skip-text\", fp_pdf_in, fp_pdf_out, \"--verbose\", str(verbose), ], capture_output=True, check=False, text=True, ) finally: logging.info(compl_proc.stdout) logging.info(compl_proc.stderr) if compl_proc.returncode == ExitCode.pdfa_conversion_failed: # <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> logging.warning( f\"Un PDF a \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9 mais la conversion en PDF/A a \u00e9chou\u00e9: {fp_pdf_out}\" ) # cela arrive quand les m\u00e9tadonn\u00e9es du PDF contiennent des caract\u00e8res que ghostscript consid\u00e8re incorrects # \"DEBUG ocrmypdf.subprocess.gs - GPL Ghostscript 9.54.0: Text string detected in DOCINFO cannot be represented in XMP for PDF/A1, discarding DOCINFO\" # ex: les m\u00e9tadonn\u00e9es PDF contiennent \"Microsoft\u00ae Word 2010\" # <https://stackoverflow.com/questions/57167784/ghostscript-wont-generate-pdf-a-with-utf16be-text-string-detected-in-docinfo> return compl_proc.returncode","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.data_sources--sources-de-donnees","text":"","title":"Sources de donn\u00e9es."},{"location":"Code%20Source/preprocess/#src.preprocess.determine_pdf_type--determiner-le-type-des-fichiers-pdf","text":"Un fichier peut \u00eatre consid\u00e9r\u00e9 PDF natif (\"texte\") ou non (\"image\"). Le type est d\u00e9termin\u00e9 pour le fichier entier, sans rentrer dans les cas particuliers commme un PDF texte dans lequel une page num\u00e9ris\u00e9e a \u00e9t\u00e9 ins\u00e9r\u00e9e en tant qu'image. Actuellement, de tels fichiers sont probablement consid\u00e9r\u00e9s comme des fichiers PDF non natifs (\"image\"), en se basant sur les m\u00e9tadonn\u00e9es du fichier PDF.","title":"D\u00e9terminer le type des fichiers PDF."},{"location":"Code%20Source/preprocess/#src.preprocess.determine_pdf_type.guess_pdf_type","text":"Devine le type de PDF: natif (\"texte\") ou non (\"image\")","title":"guess_pdf_type()"},{"location":"Code%20Source/preprocess/#src.preprocess.determine_pdf_type.guess_pdf_type--parameters","text":"df_row: NamedTuple M\u00e9tadonn\u00e9es et propri\u00e9t\u00e9s du fichier PDF.","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.determine_pdf_type.guess_pdf_type--returns","text":"pdf_type: string, one of {\"text\", \"image\"} Type de PDF: \"text\" pour les PDF natifs, \"image\" pour les autres qui devront \u00eatre OCRis\u00e9s. src\\preprocess\\determine_pdf_type.py def guess_pdf_type(df_row: NamedTuple) -> str: \"\"\"Devine le type de PDF: natif (\"texte\") ou non (\"image\") Parameters ---------- df_row: NamedTuple M\u00e9tadonn\u00e9es et propri\u00e9t\u00e9s du fichier PDF. Returns ------- pdf_type: string, one of {\"text\", \"image\"} Type de PDF: \"text\" pour les PDF natifs, \"image\" pour les autres qui devront \u00eatre OCRis\u00e9s. \"\"\" if pd.notna(df_row.guess_pdftext) and df_row.guess_pdftext: # forte pr\u00e9somption que c'est un PDF texte, d'apr\u00e8s les m\u00e9tadonn\u00e9es pdf_type = \"text\" elif pd.notna(df_row.guess_dernpage) and df_row.guess_dernpage: # (pour les PDF du stock) la derni\u00e8re page est un accus\u00e9 de r\u00e9ception de transmission \u00e0 @ctes, # donc les m\u00e9tadonn\u00e9es ont \u00e9t\u00e9 \u00e9cras\u00e9es et: # 1. il faut exclure la derni\u00e8re page (accus\u00e9 de r\u00e9ception de la transmission) puis # 2. si pdftotext parvient \u00e0 extraire du texte, alors c'est un PDF texte, sinon c'est un PDF image if df_row.retcode_txt == 0: pdf_type = \"text\" else: pdf_type = \"image\" elif pd.notna(df_row.guess_badocr) and df_row.guess_badocr: # le PDF contient une couche d'OCR produite par un logiciel moins performant: refaire l'OCR # # PDF image pdf_type = \"image\" else: # PDF image pdf_type = \"image\" return pdf_type","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.determine_pdf_type.process_files","text":"D\u00e9terminer le type des fichiers PDF. Un fichier PDF peut \u00eatre natif (\"texte\") ou non (\"image\").","title":"process_files()"},{"location":"Code%20Source/preprocess/#src.preprocess.determine_pdf_type.process_files--parameters","text":"df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es.","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.determine_pdf_type.process_files--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. src\\preprocess\\determine_pdf_type.py def process_files( df_meta: pd.DataFrame, ) -> pd.DataFrame: \"\"\"D\u00e9terminer le type des fichiers PDF. Un fichier PDF peut \u00eatre natif (\"texte\") ou non (\"image\"). Parameters ---------- df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. \"\"\" processed_as = [] for df_row in df_meta.itertuples(): # fichier d'origine fp_pdf_in = Path(df_row.fullpath) # d\u00e9terminer le type de fichier: PDF natif (\"text\") ou non (\"image\") pdf_type = guess_pdf_type(df_row) processed_as.append(pdf_type) # remplir le fichier CSV de sortie df_mmod = df_meta.assign( processed_as=processed_as, ) # forcer les types des nouvelles colonnes df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_PDFTYPE) return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdfminer--extrait-le-texte-natif-des-fichiers-pdf-avec-pdfminersix","text":"N\u00e9cessite d'installer pdfminer.six. Non utilis\u00e9 pour le moment, faute d'avoir identifi\u00e9 les bonnes valeurs des param\u00e8tres utilis\u00e9s pour l'analyse du layout, mais pourrait \u00eatre utile pour r\u00e9-analyser les arr\u00eat\u00e9s de certaines communes avec une mise en page compliqu\u00e9e.","title":"Extrait le texte natif des fichiers PDF avec pdfminer.six"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdfminer.extract_native_text_pdfminer","text":"Extrait le texte natif d'un PDF avec pdfminer.six. Si le texte extrait par pdfminer.six n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\" \", \" \" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\").","title":"extract_native_text_pdfminer()"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdfminer.extract_native_text_pdfminer--parameters","text":"fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse).","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdfminer.extract_native_text_pdfminer--returns","text":"returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. src\\preprocess\\extract_native_text_pdfminer.py def extract_native_text_pdfminer( fp_pdf_in: Path, fp_txt_out: Path, page_beg: int, page_end: int ) -> int: \"\"\"Extrait le texte natif d'un PDF avec pdfminer.six. Si le texte extrait par pdfminer.six n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\"\\x0c\", \"\\f\" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\"). Parameters ---------- fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). Returns ------- returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. \"\"\" # pages \u00e0 extraire # les num\u00e9ros de page commencent \u00e0 1, mais pdftotext cr\u00e9e une liste de pages # qui commence \u00e0 l'index 0 page_beg_ix = page_beg - 1 # le num\u00e9ro de la derni\u00e8re page ne doit pas \u00eatre d\u00e9cal\u00e9 car la borne sup d'un slice est exclue # page_end_ix = page_end page_numbers = list(range(page_beg_ix, page_end)) # TODO v\u00e9rifier que le texte contient bien \"\\f\" en fin de page txt = extract_text(fp_pdf_in, page_numbers=page_numbers, laparams=LAPARAMS) # codec=\"utf-8\" ? \"latin-1\"? # with open(fp_pdf_in, \"rb\") as f, open(fp_txt_out, \"w\") as f_txt: # extract_text_to_fp(f, f_txt, page_numbers=page_numbers, output_type=\"text\") # normaliser le texte extrait en forme NFC norm_txt = unicodedata.normalize(\"NFC\", txt) # if norm_txt: # stocker le texte dans un fichier .txt with open(fp_txt_out, \"w\") as f_txt: f_txt.write(norm_txt) # code ok return 0 else: # code d'erreur return 1","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdftotext--extrait-le-texte-natif-des-fichiers-pdf-avec-pdftotext","text":"https://github.com/jalan/pdftotext D\u00e9pendances Windows ( https://github.com/jalan/pdftotext#os-dependencies ): * Microsoft Visual C++ Build Tools: https://visualstudio.microsoft.com/fr/visual-cpp-build-tools/ * poppler ( conda install -c conda-forge poppler )","title":"Extrait le texte natif des fichiers PDF avec pdftotext"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdftotext.extract_native_text_pdftotext","text":"Extrait le texte natif d'un PDF avec pdftotext. Si le texte extrait par pdftotext n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\" \", \" \" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\").","title":"extract_native_text_pdftotext()"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdftotext.extract_native_text_pdftotext--parameters","text":"fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse).","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_native_text_pdftotext.extract_native_text_pdftotext--returns","text":"returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. src\\preprocess\\extract_native_text_pdftotext.py def extract_native_text_pdftotext( fp_pdf_in: Path, fp_txt_out: Path, page_beg: int, page_end: int ) -> int: \"\"\"Extrait le texte natif d'un PDF avec pdftotext. Si le texte extrait par pdftotext n'est pas vide alors un fichier TXT est produit, sinon aucun fichier TXT n'est produit et un code d'erreur est renvoy\u00e9. Les pages sont s\u00e9par\u00e9es par un \"form feed\" (\"\\x0c\", \"\\f\" en python). Le texte est normalis\u00e9 en forme NFC (NEW 2023-03-10, NFC plut\u00f4t que NFKC car ce dernier transforme \"\u00ba\" en \"o\"). Parameters ---------- fp_pdf_in: Path Fichier PDF d'entr\u00e9e. fp_txt_out: Path Fichier TXT produit par extraction directe du texte. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int, defaults to None Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). Returns ------- returncode: int 0 si un fichier TXT a \u00e9t\u00e9 produit, 1 sinon. \"\"\" # les num\u00e9ros de page commencent \u00e0 1, mais pdftotext cr\u00e9e une liste de pages # qui commence \u00e0 l'index 0 page_beg_ix = page_beg - 1 # le num\u00e9ro de la derni\u00e8re page ne doit pas \u00eatre d\u00e9cal\u00e9 car la borne sup d'un slice est exclue # page_end_ix = page_end with open(fp_pdf_in, \"rb\") as f: try: pdf = pdftotext.PDF(f) except pdftotext.Error as e: logging.error(f\"erreur pdftotext: {e}\") return 1 # code d'erreur # pdftotext.PDF a getitem(), mais ne permet pas de r\u00e9cup\u00e9rer un slice # donc il faut cr\u00e9er un range et it\u00e9rer manuellement doc_txt = [pdf[i] for i in range(page_beg_ix, page_end)] # chaque page produite par pdftotext se termine par \"\\f\", # il faut enlever le dernier \"\\f\" pour avoir la m\u00eame # structure qu'en sortie d'ocrmypdf assert doc_txt[-1][-1] == \"\\f\" doc_txt[-1] = doc_txt[-1][:-1] # concat\u00e9ner le texte des pages txt = \"\".join(doc_txt) # .strip() ? # normaliser le texte extrait en forme NFC norm_txt = unicodedata.normalize(\"NFC\", txt) # stocker le texte dans un fichier .txt with open(fp_txt_out, \"w\") as f_txt: f_txt.write(norm_txt) return 0 # code ok","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr_ocrmypdf--extraire-le-texte-de-fichiers-pdf-par-ocr","text":"Utilise ocrmypdf.","title":"Extraire le texte de fichiers PDF par OCR."},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr_ocrmypdf.extract_text_from_pdf_image","text":"Extraire le texte d'un PDF image et convertir le fichier en PDF/A. Utilise ocrmypdf. On utilise \"-l fra\" pour am\u00e9liorer la reconnaissance de: \"\u00e0\", \"\u00e8\", \"\u00ea\", apostrophe, \"l'\", \"\u0153\", \"\u00f4\" etc.","title":"extract_text_from_pdf_image()"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr_ocrmypdf.extract_text_from_pdf_image--parameters","text":"fp_pdf_in: Path Fichier PDF image \u00e0 traiter. fp_txt_out: Path Fichier TXT produit, contenant le texte extrait par OCR. fp_pdf_out: Path Fichier PDF/A produit, incluant le texte oc\u00e9ris\u00e9. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). redo_ocr: boolean, defaults to False Si True, refait l'OCR m\u00eame si une couche d'OCR est d\u00e9tect\u00e9e sur certaines pages. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr_ocrmypdf.extract_text_from_pdf_image--returns","text":"returncode: int 0 si deux fichiers PDF/A et TXT ont \u00e9t\u00e9 produits, une autre valeur sinon https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy . src\\preprocess\\extract_text_ocr_ocrmypdf.py def extract_text_from_pdf_image( fp_pdf_in: Path, fp_txt_out: Path, fp_pdf_out: Path, page_beg: int, page_end: int, redo_ocr: bool = False, verbose: int = 0, ) -> int: \"\"\"Extraire le texte d'un PDF image et convertir le fichier en PDF/A. Utilise ocrmypdf. On utilise \"-l fra\" pour am\u00e9liorer la reconnaissance de: \"\u00e0\", \"\u00e8\", \"\u00ea\", apostrophe, \"l'\", \"\u0153\", \"\u00f4\" etc. Parameters ---------- fp_pdf_in: Path Fichier PDF image \u00e0 traiter. fp_txt_out: Path Fichier TXT produit, contenant le texte extrait par OCR. fp_pdf_out: Path Fichier PDF/A produit, incluant le texte oc\u00e9ris\u00e9. page_beg: int Num\u00e9ro de la premi\u00e8re page \u00e0 traiter, la premi\u00e8re page d'un PDF est suppos\u00e9e num\u00e9rot\u00e9e 1. page_end: int Num\u00e9ro de la derni\u00e8re page \u00e0 traiter (cette page \u00e9tant incluse). redo_ocr: boolean, defaults to False Si True, refait l'OCR m\u00eame si une couche d'OCR est d\u00e9tect\u00e9e sur certaines pages. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- returncode: int 0 si deux fichiers PDF/A et TXT ont \u00e9t\u00e9 produits, une autre valeur sinon <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> . \"\"\" # appeler ocrmypdf pour produire 2 fichiers: PDF/A-2b (inc. OCR) + sidecar (txt) cmd = ( [\"ocrmypdf\"] + [ # langue fran\u00e7aise \"-l\", \"fra\", # s\u00e9lection de pages \"--page\", f\"{page_beg}-{page_end}\", # TXT en sortie \"--sidecar\", fp_txt_out, # verbosit\u00e9 \"--verbose\", str(verbose), ] + ([\"--redo-ocr\"] if redo_ocr else []) + [ # PDF en entr\u00e9e fp_pdf_in, # PDF/A en sortie fp_pdf_out, ] ) try: compl_proc = subprocess.run( cmd, capture_output=True, check=False, text=True, ) finally: logging.info(compl_proc.stdout) logging.info(compl_proc.stderr) if compl_proc.returncode == ExitCode.pdfa_conversion_failed: # <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> # <https://ocrmypdf.readthedocs.io/en/v14.0.4/apiref.html#ocrmypdf.exceptions.ExitCode> logging.warning( f\"Un PDF a \u00e9t\u00e9 g\u00e9n\u00e9r\u00e9 mais la conversion en PDF/A a \u00e9chou\u00e9: {fp_pdf_out}\" ) # cela arrive quand les m\u00e9tadonn\u00e9es du PDF contiennent des caract\u00e8res que ghostscript consid\u00e8re incorrects # \"DEBUG ocrmypdf.subprocess.gs - GPL Ghostscript 9.54.0: Text string detected in DOCINFO cannot be represented in XMP for PDF/A1, discarding DOCINFO\" # ex: les m\u00e9tadonn\u00e9es PDF contiennent \"Microsoft\u00ae Word 2010\" # <https://stackoverflow.com/questions/57167784/ghostscript-wont-generate-pdf-a-with-utf16be-text-string-detected-in-docinfo> return compl_proc.returncode","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr--extraire-le-texte-des-fichiers-pdf-par-ocr","text":"Le texte des PDF non-natifs (\"PDF image\") est extrait avec ocrmypdf. ocrmypdf produit un fichier PDF/A incluant une couche de texte extrait par OCR, et un fichier \"sidecar\" contenant le texte extrait par l'OCR uniquement. Le fichier PDF/A est effac\u00e9, sauf mention contraire explicite par l'option \"keep_pdfa\".","title":"Extraire le texte des fichiers PDF par OCR."},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr.preprocess_pdf_file","text":"Extraire le texte par OCR et g\u00e9n\u00e9rer des fichiers PDF/A et txt. Le fichier PDF est OCRis\u00e9 avec ocrmypdf, qui cr\u00e9e un PDF/A et un fichier texte \"sidecar\". La version actuelle est: ocrmypdf 14.0.3 / Tesseract OCR-PDF 5.2.0 (+ pikepdf 5.6.1).","title":"preprocess_pdf_file()"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr.preprocess_pdf_file--parameters","text":"df_row: NamedTuple M\u00e9tadonn\u00e9es et informations sur le fichier PDF \u00e0 traiter. fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. fp_pdf_out: Path Chemin du fichier PDF converti en PDF/A (avec OCR le cas \u00e9ch\u00e9ant). fp_txt_out: Path Chemin du fichier txt contenant le texte extrait. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr.preprocess_pdf_file--returns","text":"retcode: int Code de retour d'ocrmypdf https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy . src\\preprocess\\extract_text_ocr.py def preprocess_pdf_file( df_row: NamedTuple, fp_pdf_in: Path, fp_pdf_out: Path, fp_txt_out: Path, verbose: int = 0, ) -> int: \"\"\"Extraire le texte par OCR et g\u00e9n\u00e9rer des fichiers PDF/A et txt. Le fichier PDF est OCRis\u00e9 avec ocrmypdf, qui cr\u00e9e un PDF/A et un fichier texte \"sidecar\". La version actuelle est: ocrmypdf 14.0.3 / Tesseract OCR-PDF 5.2.0 (+ pikepdf 5.6.1). Parameters ---------- df_row: NamedTuple M\u00e9tadonn\u00e9es et informations sur le fichier PDF \u00e0 traiter. fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. fp_pdf_out: Path Chemin du fichier PDF converti en PDF/A (avec OCR le cas \u00e9ch\u00e9ant). fp_txt_out: Path Chemin du fichier txt contenant le texte extrait. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- retcode: int Code de retour d'ocrmypdf <https://ocrmypdf.readthedocs.io/en/latest/advanced.html#return-code-policy> . \"\"\" logging.info(f\"Ouverture du fichier {fp_pdf_in}\") # d\u00e9finir les pages \u00e0 traiter page_beg = 1 # exclure la derni\u00e8re page de l'OCRisation, si c'est un accus\u00e9 de r\u00e9ception de transmission \u00e0 @ctes # TODO g\u00e9rer les cas o\u00f9 l'AR de transmission n'est pas en derni\u00e8re page car suivi d'annexes page_end = ( df_row.nb_pages - 1 if (pd.notna(df_row.guess_dernpage) and df_row.guess_dernpage) else df_row.nb_pages ) # Si un PDF est susceptible de contenir des couches d'OCR de mauvaise qualit\u00e9, indiquer \u00e0 # ocrmypdf de les ignorer et de refaire l'OCR, avec \"--redo-ocr\" (CLI) / \"redo_ocr\" (ici) # (si cela ne fonctionne pas ou pas toujours, modifier le code pour remplacer \"--redo-ocr\" # par \"--force-ocr\" qui forcera la rasterization des pages avant de leur appliquer l'OCR) # FIXME ? force ocr pour les PDF avec une mauvaise OCR, eg. \"Image Capture Plus\" ? redo_ocr = True # redo_ocr = df_row.processed_as == \"image\" # toujours vrai? assert df_row.processed_as == \"image\" logging.info(f\"PDF image: {fp_pdf_in}\") retcode = extract_text_from_pdf_image( fp_pdf_in, fp_txt_out, fp_pdf_out, page_beg=page_beg, page_end=page_end, redo_ocr=redo_ocr, verbose=verbose, ) return retcode","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr.process_files","text":"Traiter un ensemble de fichiers PDF: convertir les PDF en PDF/A et extraire le texte.","title":"process_files()"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr.process_files--parameters","text":"df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. out_txt_dir: Path Dossier de sortie pour les fichiers texte. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.extract_text_ocr.process_files--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. src\\preprocess\\extract_text_ocr.py def process_files( df_meta: pd.DataFrame, out_pdf_dir: Path, out_txt_dir: Path, redo: bool = False, keep_pdfa: bool = False, verbose: int = 0, ) -> pd.DataFrame: \"\"\"Traiter un ensemble de fichiers PDF: convertir les PDF en PDF/A et extraire le texte. Parameters ---------- df_meta: pd.DataFrame Liste de fichiers PDF \u00e0 traiter, avec leurs m\u00e9tadonn\u00e9es. out_pdf_dir: Path Dossier de sortie pour les PDF/A. out_txt_dir: Path Dossier de sortie pour les fichiers texte. redo: bool, defaults to False Si True, r\u00e9analyse les fichiers d\u00e9j\u00e0 trait\u00e9s. keep_pdfa: bool, defaults to False Si True, n'efface pas les fichiers PDF g\u00e9n\u00e9r\u00e9s par l'OCR. verbose: int, defaults to 0 Niveau de verbosit\u00e9 d'ocrmypdf (-1, 0, 1, 2): <https://ocrmypdf.readthedocs.io/en/latest/api.html#ocrmypdf.Verbosity> Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers d'entr\u00e9e et chemins vers les fichiers PDF/A et TXT. \"\"\" retcode_ocr = [] fullpath_pdfa = [] fullpath_txt = [] for i, df_row in enumerate(df_meta.itertuples()): if i != 0 and i % 10 == 0: print(f\"{i}/{len(df_meta)} pdf trait\u00e9s\") # fichier d'origine fp_pdf_in = Path(df_row.fullpath) # fichiers \u00e0 produire fp_pdf_out = out_pdf_dir / f\"{fp_pdf_in.name}\" fp_txt = out_txt_dir / (f\"{fp_pdf_in.stem}\" + \".txt\") # sauter les fichiers identifi\u00e9s comme PDF texte, et les fichiers exclus # et simplement reporter les chemins vers les fichiers txt et \u00e9ventuellement PDF/A if df_row.processed_as == \"text\" or df_row.exclude: retcode_ocr.append(None) # valeur de retour ocrmypdf fullpath_pdfa.append(df_row.fullpath_pdfa) fullpath_txt.append(df_row.fullpath_txt) continue # sinon processed_as est \"image\" (et pas <NA>) assert df_row.processed_as == \"image\" # et le fichier n'est pas exclus (et pas <NA>) assert not df_row.exclude # si le fichier txt \u00e0 produire existe d\u00e9j\u00e0 if fp_txt.is_file(): if redo: # r\u00e9-ex\u00e9cution explicitement demand\u00e9e: \u00e9mettre une info et traiter le fichier # TODO comparer les versions d'ocrmypdf/tesseract/pikepdf dans les m\u00e9tadonn\u00e9es du PDF de sortie et les versions actuelles des d\u00e9pendances, # et si pertinent \u00e9mettre un message proposant de r\u00e9-analyser le PDF ? logging.info( f\"Re-traitement de {fp_pdf_in}, le fichier de sortie {fp_txt} existant sera \u00e9cras\u00e9.\" ) else: # pas de r\u00e9-ex\u00e9cution demand\u00e9e: \u00e9mettre un warning et passer au fichier suivant logging.info( f\"{fp_pdf_in} est ignor\u00e9 car le fichier {fp_txt} existe d\u00e9j\u00e0.\" ) retcode_ocr.append( None ) # valeur de retour ocrmypdf, impossible \u00e0 r\u00e9cup\u00e9rer sans refaire tourner la conversion if fp_pdf_out.is_file(): fullpath_pdfa.append(fp_pdf_out) else: fullpath_pdfa.append(None) fullpath_txt.append(fp_txt) continue # traiter le fichier: extraire le texte par OCR si n\u00e9cessaire, corriger et convertir le PDF d'origine en PDF/A-2b retcode = preprocess_pdf_file( df_row, fp_pdf_in, fp_pdf_out, fp_txt, verbose=verbose ) # stocker les chemins: fichier TXT (OCR), \u00e9ventuellement PDF/A retcode_ocr.append(retcode) # valeur de retour ocrmypdf fullpath_txt.append(fp_txt) if not keep_pdfa: os.remove(fp_pdf_out) fullpath_pdfa.append(None) else: fullpath_pdfa.append(fp_pdf_out) df_mmod = df_meta.assign( retcode_ocr=retcode_ocr, fullpath_pdfa=fullpath_pdfa, fullpath_txt=fullpath_txt, ) # forcer les types des nouvelles colonnes df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_OCR) return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.filter_docs--filtrer-les-fichiers-pdf-hors-du-champ-de-la-base-de-donnees","text":"Annexes des arr\u00eat\u00e9s: plan de p\u00e9rim\u00e8tre de s\u00e9curit\u00e9, rapports d'expertise etc. TODO filtrer automatiquement \u00e0 partir du texte","title":"Filtrer les fichiers PDF hors du champ de la base de donn\u00e9es."},{"location":"Code%20Source/preprocess/#src.preprocess.filter_docs.process_files","text":"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes.","title":"process_files()"},{"location":"Code%20Source/preprocess/#src.preprocess.filter_docs.process_files--parameters","text":"df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter.","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.filter_docs.process_files--returns","text":"df_mmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers, filtr\u00e9s. df_tmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. src\\preprocess\\filter_docs.py def process_files( df_meta: pd.DataFrame, df_txts: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters ---------- df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns ------- df_mmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers, filtr\u00e9s. df_tmod: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. \"\"\" df_mmod = df_meta.assign(exclude=(lambda x: x.pdf.isin(SET_EXCLUDE))) df_mmod = df_mmod.astype(dtype=DTYPE_META_NTXT_FILT) df_tmod = df_txts.assign(exclude=(lambda x: x.pdf.isin(SET_EXCLUDE))) df_tmod = df_tmod.astype(dtype=DTYPE_NTXT_PAGES_FILT) return df_mmod, df_tmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.pdf_info--extraire-les-metadonnees-des-fichiers-pdf","text":"Ce module utilise pikepdf.","title":"Extraire les m\u00e9tadonn\u00e9es des fichiers PDF."},{"location":"Code%20Source/preprocess/#src.preprocess.pdf_info.get_pdf_info","text":"Extraire les informations (dont m\u00e9tadonn\u00e9es) d'un fichier PDF. Utilise actuellement pikepdf.","title":"get_pdf_info()"},{"location":"Code%20Source/preprocess/#src.preprocess.pdf_info.get_pdf_info--parameters","text":"fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Algorithme de hachage \u00e0 utiliser https://docs.python.org/3/library/hashlib.html#hash-algorithms . verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF.","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.pdf_info.get_pdf_info--returns","text":"pdf_info : dict Informations (dont m\u00e9tadonn\u00e9es) du fichier PDF d'entr\u00e9e src\\preprocess\\pdf_info.py def get_pdf_info( fp_pdf: Path, digest: str = \"blake2b\", verbose: bool = False ) -> Dict[str, str | int]: \"\"\"Extraire les informations (dont m\u00e9tadonn\u00e9es) d'un fichier PDF. Utilise actuellement pikepdf. Parameters ---------- fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Algorithme de hachage \u00e0 utiliser <https://docs.python.org/3/library/hashlib.html#hash-algorithms> . verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF. Returns ------- pdf_info : dict Informations (dont m\u00e9tadonn\u00e9es) du fichier PDF d'entr\u00e9e \"\"\" logging.info(f\"Ouverture du fichier {fp_pdf}\") pdf_info = { # m\u00e9tadonn\u00e9es du fichier lui-m\u00eame \"pdf\": fp_pdf.name, # nom du fichier \"fullpath\": fp_pdf.resolve(), # chemin complet \"filesize\": fp_pdf.stat().st_size, # taille du fichier digest: get_file_digest(fp_pdf, digest=digest), # hash du fichier } # lire les m\u00e9tadonn\u00e9es du PDF avec pikepdf meta_pike = get_pdf_info_pikepdf(fp_pdf, verbose=verbose) # ajouter les m\u00e9tadonn\u00e9es PDF \u00e0 celles du fichier pdf_info.update(meta_pike) return pdf_info","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.pdf_info.get_pdf_info_pikepdf","text":"Renvoie les infos du PDF en utilisant pikepdf. Les infos incluent un sous-ensemble des m\u00e9tadonn\u00e9es du PDF.","title":"get_pdf_info_pikepdf()"},{"location":"Code%20Source/preprocess/#src.preprocess.pdf_info.get_pdf_info_pikepdf--parameters","text":"fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF.","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.pdf_info.get_pdf_info_pikepdf--returns","text":"infos: dict Dictionnaire contenant les infos du PDF. src\\preprocess\\pdf_info.py def get_pdf_info_pikepdf(fp_pdf_in: Path, verbose: bool = False) -> dict: \"\"\"Renvoie les infos du PDF en utilisant pikepdf. Les infos incluent un sous-ensemble des m\u00e9tadonn\u00e9es du PDF. Parameters ---------- fp_pdf_in: Path Chemin du fichier PDF \u00e0 traiter. verbose: boolean, defaults to False Si True, des warnings sont \u00e9mis \u00e0 chaque anomalie constat\u00e9e dans les m\u00e9tadonn\u00e9es du PDF. Returns ------- infos: dict Dictionnaire contenant les infos du PDF. \"\"\" with pikepdf.open(fp_pdf_in) as f_pdf: with f_pdf.open_metadata(set_pikepdf_as_editor=False) as meta: # lire les m\u00e9tadonn\u00e9es stock\u00e9es en XMP (\"nouveau\" format) meta_base = {k: v for k, v in meta.items()} if verbose: try: logging.info( f\"{fp_pdf_in.name}: m\u00e9tadonn\u00e9es XMP brutes: {f_pdf.Root.Metadata.read_bytes().decode()}\" ) except AttributeError: logging.warning( f\"{fp_pdf_in.name}: absence de m\u00e9tadonn\u00e9es XMP brutes\" ) logging.info(f\"{fp_pdf_in.name}: m\u00e9tadonn\u00e9es XMP: {meta_base}\") # lire les m\u00e9tadonn\u00e9es stock\u00e9es dans docinfo (ancien format) meta.load_from_docinfo(f_pdf.docinfo) # NB: load_from_docinfo() peut lever un UserWarning, qui est alors inclus dans le log de ce script # <https://github.com/pikepdf/pikepdf/blob/94c50cd408b214f7569a717c3409e36b7a996769/src/pikepdf/models/metadata.py#L438> # ex: \"UserWarning: The metadata field /MetadataDate with value 'pikepdf.String(\"D:20230117110535+01'00'\")' has no XMP equivalent, so it was discarded\" meta_doci = {k: v for k, v in meta.items()} if verbose: logging.info(f\"{fp_pdf_in.name}: docinfo: {repr(f_pdf.docinfo)}\") logging.info(f\"{fp_pdf_in.name}: m\u00e9tadonn\u00e9es XMP+docinfo: {meta_doci}\") # comparaison des m\u00e9tadonn\u00e9es: XMP seul vs XMP mis \u00e0 jour avec docinfo base_keys = set(meta_base.keys()) doci_keys = set(meta_doci.keys()) # v\u00e9rifier que la lecture de docinfo n'a pas supprim\u00e9 de champ aux m\u00e9tadonn\u00e9es XMP assert (base_keys - doci_keys) == set() # v\u00e9rifier que les champs charg\u00e9s depuis docinfo n'ont modifi\u00e9 aucune valeur de champ XMP # (pas de modification / \u00e9crasement, condition plus forte que supra) for key, value in meta_base.items(): if key.endswith(\"Date\"): # traitement sp\u00e9cifique pour les dates: gestion de diff\u00e9rents formats + tol\u00e9rance de 2h pour les timezones base_v = datetime.fromisoformat( value.replace(\"Z\", \"+00:00\") ).astimezone(tz=TZ_FRA) doci_v = datetime.fromisoformat( meta_doci[key].replace(\"Z\", \"+00:00\") ).astimezone(tz=TZ_FRA) # base_eq_doci = abs(base_v - doci_v) <= timedelta(hours=1) # si besoin de permissivit\u00e9 base_eq_doci = doci_v == base_v else: # comparaison par d\u00e9faut: \u00e9galit\u00e9 stricte base_v = value doci_v = meta_doci[key] base_eq_doci = doci_v == base_v if not base_eq_doci: logging.warning( f\"{fp_pdf_in}: metadata: {key}={base_v} (xmp) vs {doci_v} (docinfo)\" ) if verbose: logging.info(f\"{fp_pdf_in}: pike:finalmetadata: {meta}\") # s\u00e9lection des champs et fixation de leur ordre infos = { \"nb_pages\": len(f_pdf.pages), # nombre de pages # m\u00e9tadonn\u00e9es PDF \"creatortool\": meta.get(\"xmp:CreatorTool\", \"\"), # string \"producer\": meta.get(\"pdf:Producer\", \"\"), # string \"createdate\": meta.get(\"xmp:CreateDate\", None), # date \"modifydate\": meta.get(\"xmp:ModifyDate\", None), # date } # analyse des dates if infos[\"createdate\"] is not None: infos[\"createdate\"] = datetime.fromisoformat(infos[\"createdate\"]).astimezone( tz=TZ_FRA ) if infos[\"modifydate\"] is not None: infos[\"modifydate\"] = datetime.fromisoformat(infos[\"modifydate\"]).astimezone( tz=TZ_FRA ) # WIP regarder si des champs sont toujours/souvent/jamais renseign\u00e9s if meta.get(\"dc:format\", None) is not None: assert meta.get(\"dc:format\", None) == \"application/pdf\" # return infos","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata--traiter-les-metadonnees-des-fichiers-pdf","text":"Les traitements permettent de: * d\u00e9tecter les fichiers doublons, * d\u00e9terminer si le PDF est du texte ou image, * d\u00e9terminer si le PDF contient des tampons de t\u00e9l\u00e9transmission en haut des pages, * d\u00e9terminer si le PDF contient une derni\u00e8re page qui est l'accus\u00e9 de r\u00e9ception de la t\u00e9l\u00e9transmission. La liste des tiers de transmission agr\u00e9\u00e9s pour @ctes est sur https://www.collectivites-locales.gouv.fr/sites/default/files/migration/2019_09_13_liste_operateurs_transmission_0.pdf .","title":"Traiter les m\u00e9tadonn\u00e9es des fichiers PDF."},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_badocr","text":"D\u00e9termine si le fichier contient une couche OCR de pi\u00e8tre qualit\u00e9. Arrive quand le champ \"creatortool\" vaut \"Image Capture Plus\".","title":"guess_badocr()"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_badocr--parameters","text":"df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_badocr--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_badocr\" src\\preprocess\\process_metadata.py def guess_badocr(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si le fichier contient une couche OCR de pi\u00e8tre qualit\u00e9. Arrive quand le champ \"creatortool\" vaut \"Image Capture Plus\". Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_badocr\" \"\"\" has_badocr = ( ( # \"Image Capture Plus\" df_meta[\"creatortool\"].str.strip() == \"Image Capture Plus\" ) | ( # \"Adobe PSL 1.2e for Canon\" (ou 1.1e, 1.3e) df_meta[\"producer\"] .str.strip() .str.startswith(\"Adobe PSL\") ) | ( # \"Canon\" (df_meta[\"producer\"].str.strip() == \"\") & (df_meta[\"creatortool\"].str.strip() == \"Canon\") ) ) df_mmod = df_meta.assign(guess_badocr=has_badocr) return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_dernpage_transmission","text":"D\u00e9termine si la derni\u00e8re page est un accus\u00e9 de r\u00e9ception de t\u00e9l\u00e9transmission. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission.","title":"guess_dernpage_transmission()"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_dernpage_transmission--parameters","text":"df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_dernpage_transmission--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_dernpage\" src\\preprocess\\process_metadata.py def guess_dernpage_transmission(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si la derni\u00e8re page est un accus\u00e9 de r\u00e9ception de t\u00e9l\u00e9transmission. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission. Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_dernpage\" \"\"\" has_dernpage = ( # @ctes (toute la p\u00e9riode?) df_meta[\"producer\"] == \"iText 2.1.7 by 1T3XT\" ) df_mmod = df_meta.assign(guess_dernpage=has_dernpage) return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_duplicates_meta","text":"D\u00e9termine si les fichiers PDF sont des doublons \u00e0 partir de leurs m\u00e9tadonn\u00e9es.","title":"guess_duplicates_meta()"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_duplicates_meta--parameters","text":"df_meta : pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF hash_fn : str Nom de la fonction de hachage, doit \u00eatre un nom de colonne du DataFrame de m\u00e9tadonn\u00e9es.","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_duplicates_meta--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF, avec des colonnes bool\u00e9ennes \"dup_*\" indiquant les fichiers doublons. src\\preprocess\\process_metadata.py def guess_duplicates_meta(df_meta: pd.DataFrame, hash_fn: str = \"blake2b\"): \"\"\"D\u00e9termine si les fichiers PDF sont des doublons \u00e0 partir de leurs m\u00e9tadonn\u00e9es. Parameters ---------- df_meta : pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF hash_fn : str Nom de la fonction de hachage, doit \u00eatre un nom de colonne du DataFrame de m\u00e9tadonn\u00e9es. Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF, avec des colonnes bool\u00e9ennes \"dup_*\" indiquant les fichiers doublons. \"\"\" # d\u00e9tection stricte: doublons sur toutes les infos (sauf \"pdf\" et \"fullpath\") # (trop de faux n\u00e9gatifs?) cols_dups_allinfo = [ # infos fichier \"filesize\", \"nb_pages\", # m\u00e9tadonn\u00e9es pdf \"creatortool\", \"producer\", \"createdate\", \"modifydate\", ] s_dups_allinfo = _guess_duplicates(df_meta, cols_dups_allinfo) df_mmod = df_meta.assign(dup_allinfo=s_dups_allinfo) # d\u00e9tection l\u00e2che: doublons sur la date de cr\u00e9ation # (trop de faux positifs) cols_dups_createdate = [\"createdate\"] s_dups_createdate = _guess_duplicates(df_mmod, cols_dups_createdate) df_mmod = df_mmod.assign(dup_createdate=s_dups_createdate) # d\u00e9tection bas\u00e9e sur le hachage des fichiers cols_dups_hash = [hash_fn] s_dups_hash = _guess_duplicates(df_mmod, cols_dups_hash) df_mmod = df_mmod.assign(dup_hash=s_dups_hash) # return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_pdftext","text":"D\u00e9termine si le fichier est un PDF texte (ou \"num\u00e9rique natif\").","title":"guess_pdftext()"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_pdftext--parameters","text":"df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_pdftext--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_pdftext\" src\\preprocess\\process_metadata.py def guess_pdftext(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si le fichier est un PDF texte (ou \"num\u00e9rique natif\"). Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_pdftext\" \"\"\" is_pdftext = ( # \"Microsoft\u00ae Word 2010\", \"Microsoft\u00ae Word 2013\", \"Microsoft\u00ae Word pour Microsoft 365\" df_meta[\"creatortool\"].str.startswith(\"Microsoft\u00ae Word\") # \"Writer\" (OpenOffice, LibreOffice) | (df_meta[\"creatortool\"] == \"Writer\") ) df_mmod = df_meta.assign(guess_pdftext=is_pdftext) return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_tampon_transmission","text":"D\u00e9termine si le haut des pages contient des tampons de transmission \u00e9lectronique. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission.","title":"guess_tampon_transmission()"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_tampon_transmission--parameters","text":"df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.process_metadata.guess_tampon_transmission--returns","text":"df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_tampon\" src\\preprocess\\process_metadata.py def guess_tampon_transmission(df_meta: pd.DataFrame) -> pd.DataFrame: \"\"\"D\u00e9termine si le haut des pages contient des tampons de transmission \u00e9lectronique. Permet de traiter certains arr\u00eat\u00e9s recueillis apr\u00e8s leur t\u00e9l\u00e9transmission. Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des fichiers PDF Returns ------- df_mmod: pd.DataFrame M\u00e9tadonn\u00e9es enrichies d'une nouvelle colonne \"guess_tampon\" \"\"\" has_stamp = ~( # faux positif: MS 365 + iText utilis\u00e9 pour ajouter la signature df_meta[\"producer\"] == \"Microsoft\u00ae Word pour Microsoft 365; modified using iText\u00ae 5.5.9 \u00a92000-2015 iText Group NV (AGPL-version)\" ) & ( df_meta[\"producer\"].str.endswith( # tampon en haut \u00e0 droite: tiers de t\u00e9l\u00e9transmission S2LOW (2019-06-14 - ..) et Berger Levrault (2021-02-08) \"; modified using iText\u00ae 7.1.5 \u00a92000-2019 iText Group NV (AGPL-version)\" ) | df_meta[\"producer\"].str.endswith( # tampon en haut \u00e0 droite: tiers de t\u00e9l\u00e9transmission S2LOW (.. - 2019-02-11) \"; modified using iText\u00ae 5.5.12 \u00a92000-2017 iText Group NV (AGPL-version)\" ) | df_meta[\"producer\"].str.endswith( # tampon en bas \u00e0 gauche (quel tiers?): \"; modified using iText\u00ae 5.5.9 \u00a92000-2015 iText Group NV (AGPL-version)\" (sans MS 365) \"; modified using iText\u00ae 5.5.9 \u00a92000-2015 iText Group NV (AGPL-version)\" ) ) # df_mmod = df_meta.assign(guess_tampon=has_stamp) return df_mmod","title":"Returns"},{"location":"Code%20Source/preprocess/#src.preprocess.separate_pages--charge-le-texte-des-documents-dans-un-dataframe","text":"Chaque ligne correspond \u00e0 une page d'un document.","title":"Charge le texte des documents dans un DataFrame."},{"location":"Code%20Source/preprocess/#src.preprocess.separate_pages.create_pages_dataframe","text":"Charger le texte des documents dans un DataFrame. Une entr\u00e9e par page de document.","title":"create_pages_dataframe()"},{"location":"Code%20Source/preprocess/#src.preprocess.separate_pages.create_pages_dataframe--parameters","text":"df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des documents.","title":"Parameters"},{"location":"Code%20Source/preprocess/#src.preprocess.separate_pages.create_pages_dataframe--returns","text":"df_txts: pd.DataFrame Tableau contenant le texte des documents, s\u00e9par\u00e9 par page. src\\preprocess\\separate_pages.py def create_pages_dataframe( df_meta: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Charger le texte des documents dans un DataFrame. Une entr\u00e9e par page de document. Parameters ---------- df_meta: pd.DataFrame M\u00e9tadonn\u00e9es des documents. Returns ------- df_txts: pd.DataFrame Tableau contenant le texte des documents, s\u00e9par\u00e9 par page. \"\"\" page_txts = [] for df_row in df_meta.itertuples(): if pd.isna(df_row.fullpath_txt): # cr\u00e9er une entr\u00e9e vide par page du PDF pages = [ { # m\u00e9tadonn\u00e9es de la page \"pagenum\": i, # texte de la page \"pagetxt\": None, } for i in range(1, df_row.nb_pages + 1) ] else: # cr\u00e9er une entr\u00e9e par page de texte doc_txt = load_pages_text(df_row.fullpath_txt) # v\u00e9rifier que le fichier TXT contient autant de pages que le PDF try: assert len(doc_txt) == df_row.nb_pages except AssertionError: print(repr(df_row)) print( f\"{len(doc_txt)} pages de texte != {df_row.nb_pages} pages dans le fichier PDF\" ) raise # pour chaque page, charger le texte pages = [ { # m\u00e9tadonn\u00e9es de la page \"pagenum\": i, # texte de la page \"pagetxt\": page_txt, } for i, page_txt in enumerate(doc_txt, start=1) ] # dupliquer les m\u00e9tadonn\u00e9es du fichier PDF et du TXT, dans chaque entr\u00e9e de page doc_rows = [ {x: getattr(df_row, x) for x in COLS_DOC} | page # python >= 3.9 (dict union) for page in pages ] # v\u00e9rifier que le nombre de pages de texte extrait est inf\u00e9rieur ou \u00e9gal au nombre de pages du PDF # (certaines pages peuvent \u00eatre blanches, ne contenir que des images ou photos...) # TODO v\u00e9rifier redondance avec l'assertion ci-dessus? assert len(doc_rows) <= df_row.nb_pages page_txts.extend(doc_rows) df_txts = pd.DataFrame.from_records(page_txts) df_txts = df_txts.astype(dtype=DTYPE_NTXT_PAGES) return df_txts","title":"Returns"},{"location":"Code%20Source/process/","text":"Process Fonctions d'extractions des donn\u00e9es des fichiers pr\u00e9trait\u00e9s. Agr\u00e8ge les pages, et leurs donn\u00e9es extraites, en documents. Chaque ligne correspond \u00e0 un document. Les \u00e9ventuelles incoh\u00e9rences entre valeurs extraites pour diff\u00e9rentes pages d'un m\u00eame document sont signal\u00e9es. aggregate_pages(df_grp, include_actes_page_ar=False) Fusionne les champs extraits des diff\u00e9rentes pages d'un document. Parameters df_grp: pd.core.groupby.DataFrame Pages d'un document include_actes_page_ar: boolean, defaults to False Inclut la page d'accus\u00e9 de r\u00e9ception d'@ctes. Returns rec_struct: dict Dictionnaire de valeurs de diff\u00e9rents types ou nulles, selon que les \u00e9l\u00e9ments ont \u00e9t\u00e9 d\u00e9tect\u00e9s. src\\process\\aggregate_pages.py def aggregate_pages(df_grp: pd.DataFrame, include_actes_page_ar: bool = False) -> Dict: \"\"\"Fusionne les champs extraits des diff\u00e9rentes pages d'un document. Parameters ---------- df_grp: pd.core.groupby.DataFrame Pages d'un document include_actes_page_ar: boolean, defaults to False Inclut la page d'accus\u00e9 de r\u00e9ception d'@ctes. Returns ------- rec_struct: dict Dictionnaire de valeurs de diff\u00e9rents types ou nulles, selon que les \u00e9l\u00e9ments ont \u00e9t\u00e9 d\u00e9tect\u00e9s. \"\"\" # conserver uniquement les pages avec du texte ; # actuellement: \"has_stamp is None\" implique que la page ne contient pas de texte # FIXME g\u00e9rer les pages sans texte en amont? grp = df_grp.dropna(subset=[\"has_stamp\"]) # si demand\u00e9, exclure l'\u00e9ventuelle page d'accus\u00e9 de r\u00e9ception d'actes if not include_actes_page_ar: grp = grp.query(\"not is_accusedereception_page\") # si le groupe est vide, renvoyer une ligne (pour le document) vide ; # utile lorsque le document ne contient pas de texte, notamment les PDF non-natifs non-oc\u00e9ris\u00e9s (ou pas encore) if grp.empty: rec_struct = {x: None for x in DTYPE_PARSE_AGG} return rec_struct # t0 = time.time() if False: grp[grp.has_stamp][\"pagenum\"].to_list() t0b = time.time() grp[grp[\"has_stamp\"]][\"pagenum\"].to_list() t0c = time.time() grp.query(\"has_stamp\")[\"pagenum\"].to_list() t0d = time.time() print(f\"{t0b - t0:.3f}\\t{t0c - t0b:.3f}\\t{t0d - t0c:.3f}\") # agr\u00e9ger les num\u00e9ros de pages ou les valeurs extraites rec_actes = { # - m\u00e9tadonn\u00e9es # * @ctes # table: contr\u00f4le ; expectation: liste de valeurs continue (ex: 1,2,3) ou vide (all NaN) # grp.query(\"has_stamp\")[\"pagenum\"].to_list(), \"actes_pages_tampon\": pagenums(grp, \"has_stamp\"), # table: contr\u00f4le ; expectation: liste vide (all NaN) ou valeur unique # grp.query(\"is_accusedereception_page\")[\"pagenum\"].to_list() \"actes_pages_ar\": pagenums(grp, \"is_accusedereception_page\"), } # t1 = time.time() rec_commu = { # - tous arr\u00eat\u00e9s # * champ \"commune\" # TODO table: ? ; TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"commune_maire\": first(grp, \"commune_maire\"), } # t2 = time.time() rec_pars = { # * champs structure de l'arr\u00eat\u00e9 \"pages_vu\": pagenums( grp, \"has_vu\" ), # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) \"pages_considerant\": pagenums( grp, \"has_considerant\" ), # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) \"pages_arrete\": pagenums( grp, \"has_arrete\" ), # table: contr\u00f4le ; expectation: valeur unique ou vide/NaN \"pages_article\": pagenums( grp, \"has_article\" ), # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) } # t3 = time.time() rec_regl = { # arr\u00eat\u00e9s sp\u00e9cifiques # - r\u00e9glementaires \"pages_cgct\": pagenums( grp, \"has_cgct\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) \"pages_cgct_art\": pagenums( grp, \"has_cgct_art\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch\": pagenums( grp, \"has_cch\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L111\": pagenums( grp, \"has_cch_L111\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L511\": pagenums( grp, \"has_cch_L511\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L521\": pagenums( grp, \"has_cch_L521\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L541\": pagenums( grp, \"has_cch_L541\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_R511\": pagenums( grp, \"has_cch_R511\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cc\": pagenums(grp, \"has_cc\"), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cc_art\": pagenums( grp, \"has_cc_art\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? } # t4 = time.time() rec_adre = { # - donn\u00e9es \"adresse_brute\": first( grp, \"adresse\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"adr_num\": first(grp, \"adr_num\"), \"adr_ind\": first(grp, \"adr_ind\"), \"adr_voie\": first(grp, \"adr_voie\"), \"adr_compl\": first(grp, \"adr_compl\"), \"adr_cpostal\": first(grp, \"adr_cpostal\"), \"adr_ville\": first(grp, \"adr_ville\"), } # t5 = time.time() rec_parce = { \"parcelle\": first( grp, \"parcelle\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t6 = time.time() rec_proprio = { \"proprio\": first( grp, \"proprio\" ), # TODO expectation: 1-n (TODO normalisation: casse, accents etc?) ; vide pour abrogation? } rec_syndi = { \"syndic\": first( grp, \"syndic\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } rec_gest = { \"gest\": first( grp, \"gest\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t7 = time.time() rec_date = { \"arr_date\": first( grp, \"date\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t8 = time.time() rec_num = { \"num_arr\": first( grp, \"num_arr\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t9 = time.time() if False: print( f\"actes: {t1 - t0:.3f}\\tcommune: {t2 - t1:.3f}\\tvu_etc: {t3 - t2:.3f}\" + f\"\\tregl: {t4 - t3:.3f}\\tadr: {t5 - t4:.3f}\\tparcelle: {t6 - t5:.3f}\" + f\"\\tsyndic: {t7 - t6:.3f}\\tdate: {t8 - t7:.3f}\\tnum: {t9 - t8:.3f}\" ) rec_nom = { \"nom_arr\": first( grp, \"nom_arr\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } rec_classi = { \"classe\": first( grp, \"classe\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"urgence\": first( grp, \"urgence\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"demo\": first( grp, \"demo\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"int_hab\": first( grp, \"int_hab\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"equ_com\": first( grp, \"equ_com\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } rec_struct = ( rec_actes | rec_commu | rec_pars | rec_regl | rec_adre | rec_parce | rec_proprio | rec_syndi | rec_gest | rec_date | rec_num | rec_nom | rec_classi ) return rec_struct create_docs_dataframe(df_pages) Rassembler les informations des documents dans un DataFrame. Fusionner les entr\u00e9es de chaque page en une entr\u00e9e par document. Parameters df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns df_docs: pd.DataFrame Tableau contenant les m\u00e9tadonn\u00e9es et donn\u00e9es extraites des documents. src\\process\\aggregate_pages.py def create_docs_dataframe( df_pages: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Rassembler les informations des documents dans un DataFrame. Fusionner les entr\u00e9es de chaque page en une entr\u00e9e par document. Parameters ---------- df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns ------- df_docs: pd.DataFrame Tableau contenant les m\u00e9tadonn\u00e9es et donn\u00e9es extraites des documents. \"\"\" doc_rows = [] for _, df_grp in df_pages.groupby(\"fullpath_txt\"): # reporter les m\u00e9tadonn\u00e9es du fichier PDF et du TXT, dans chaque entr\u00e9e de document meta_doc = { x: df_grp[x].to_list()[0] for x in DTYPE_META_NTXT_FILT } # FIXME prendre les m\u00e9tadonn\u00e9es du document dans le CSV 1 ligne par doc? # retraiter sp\u00e9cifiquement le champ \"exclude\": si toutes les pages sont \"exclude\", alors le fichier aussi, sinon non meta_doc[\"exclude\"] = df_grp[\"exclude\"].all() # rassembler les donn\u00e9es des pages ; # exclure l'\u00e9ventuelle page d'accus\u00e9 de r\u00e9ception d'actes data_doc = aggregate_pages(df_grp, include_actes_page_ar=False) doc_rows.append(meta_doc | data_doc) # python >= 3.9 (dict union) df_docs = pd.DataFrame.from_records(doc_rows) df_docs = df_docs.astype(dtype=DTYPE_META_NTXT_DOC) return df_docs first(df_grp, col_on) Renvoie la premi\u00e8re valeur non-vide de la colonne src\\process\\aggregate_pages.py def first(df_grp: pd.DataFrame, col_on: str): \"\"\"Renvoie la premi\u00e8re valeur non-vide de la colonne\"\"\" s_ok = df_grp[col_on].dropna() if s_ok.empty: return None else: return s_ok.to_list()[0] pagenums(df_grp, col_on) Renvoie la liste des num\u00e9ros de pages o\u00f9 une colonne est vraie src\\process\\aggregate_pages.py def pagenums(df_grp: pd.DataFrame, col_on: str): \"\"\"Renvoie la liste des num\u00e9ros de pages o\u00f9 une colonne est vraie\"\"\" return df_grp[df_grp[col_on]][\"pagenum\"].to_list() Enrichit les donn\u00e9es avec des donn\u00e9es suppl\u00e9mentaires. Ajoute le code INSEE de la commune. create_docs_dataframe(df_agg) Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document. Parameters df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. src\\process\\enrich_data.py def create_docs_dataframe( df_agg: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document. Parameters ---------- df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns ------- df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. \"\"\" # ajoute le code INSEE, \u00e0 partir de la commune et du code postal (pour Marseille) # df_agg[\"adr_codeinsee\"] = df_agg.apply( # lambda row: get_codeinsee(row[\"adr_ville\"], row[\"adr_cpostal\"]), axis=1 # ) # remplace la r\u00e9f\u00e9rence cadastrale par sa version normalis\u00e9e df_agg[\"par_ref_cad\"] = df_agg.apply( lambda row: generate_refcadastrale_norm( row[\"adr_codeinsee\"], row[\"par_ref_cad\"], row[\"arr_pdf\"], row[\"adr_cpostal\"], ), axis=1, ) df_docs = df_agg.astype(dtype=DTYPE_DATA) return df_docs Exporte les donn\u00e9es en fichiers CSV. 4 tables: * arr\u00eat\u00e9, * adresse, * parcelle, * notifi\u00e9 Extraire les donn\u00e9es des documents. Les donn\u00e9es sont extraites des empans de texte rep\u00e9r\u00e9s au pr\u00e9alable, et normalis\u00e9es. Lorsque plusieurs empans de texte sont susceptibles de renseigner sur la m\u00eame donn\u00e9e, les diff\u00e9rentes valeurs extraites sont accumul\u00e9es pour certains champs (ex: propri\u00e9taires) ou compar\u00e9es et s\u00e9lectionn\u00e9es pour d'autres champs (ex: commune). create_docs_dataframe(df_agg) Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document. Parameters df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. src\\process\\extract_data.py def create_docs_dataframe( df_agg: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document. Parameters ---------- df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns ------- df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. \"\"\" doc_rows = [] # filtrer les documents \u00e0 exclure compl\u00e8tement: documents hors p\u00e9rim\u00e8tre strict du jeu de donn\u00e9es cible df_filt = df_agg[~df_agg[\"exclude\"]] # it\u00e9rer sur tous les documents non-exclus for i, df_row in enumerate(df_filt.itertuples()): doc_idu = { \"idu\": f\"id_{i:04}\", # FIXME identifiant unique } doc_arr = { # arr\u00eat\u00e9 \"arr_date\": ( process_date_brute(getattr(df_row, \"arr_date\")) if pd.notna(getattr(df_row, \"arr_date\")) else None ), \"arr_num_arr\": ( normalize_string( getattr(df_row, \"num_arr\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"num_arr\")) else None ), \"arr_nom_arr\": ( normalize_string( getattr(df_row, \"nom_arr\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"nom_arr\")) else None ), \"arr_classe\": ( normalize_string( getattr(df_row, \"classe\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"classe\")) else None ), \"arr_urgence\": ( normalize_string( getattr(df_row, \"urgence\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"urgence\")) else None ), \"arr_demo\": ( normalize_string( getattr(df_row, \"demo\"), num=True, apos=True, hyph=True, spaces=True ) if pd.notna(getattr(df_row, \"demo\")) else None ), # TODO affiner \"arr_int_hab\": ( normalize_string( getattr(df_row, \"int_hab\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"int_hab\")) else None ), # TODO affiner \"arr_equ_com\": ( normalize_string( getattr(df_row, \"equ_com\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"equ_com\")) else None ), # TODO affiner # (m\u00e9tadonn\u00e9es du doc) \"arr_pdf\": getattr(df_row, \"pdf\"), \"arr_url\": getattr( df_row, \"fullpath\" ), # l'URL sera r\u00e9\u00e9crite avec une URL locale (r\u00e9seau) ou publique, au moment de l'export } # adresse # - nettoyer a minima de l'adresse brute adr_ad_brute = ( normalize_string( getattr(df_row, \"adresse_brute\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"adresse_brute\")) else None ) # WIP 2023-03-30: supprimer car sera fait dans parse_native_pages, parse_doc, parse_doc_direct # - extraire les \u00e9l\u00e9ments d'adresse en traitant l'adresse brute adr_num = getattr(df_row, \"adr_num\") # num\u00e9ro de la voie adr_ind = getattr(df_row, \"adr_ind\") # indice de r\u00e9p\u00e9tition adr_voie = getattr(df_row, \"adr_voie\") # nom de la voie adr_compl = getattr(df_row, \"adr_compl\") # compl\u00e9ment d'adresse adr_cpostal = getattr(df_row, \"adr_cpostal\") # code postal adr_ville = getattr(df_row, \"adr_ville\") # ville # end WIP 2023-03-30 # - nettoyer a minima la commune extraite des en-t\u00eate ou pied-de-page ou de la mention du maire signataire adr_commune_maire = ( normalize_string( getattr(df_row, \"commune_maire\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"commune_maire\")) else None ) # - d\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9 en reconciliant la commune de l'adresse et # celle de l'autorit\u00e9 adr_commune = determine_commune(adr_ville, adr_commune_maire) if pd.isna(adr_commune) or not adr_commune: logging.warning(f\"Pas de commune pour {doc_arr['arr_pdf']}\") # - d\u00e9terminer le code INSEE de la commune adr_codeinsee = get_codeinsee(adr_commune, adr_cpostal) # - si l'adresse ne contenait pas de code postal, essayer de d\u00e9terminer le code postal # \u00e0 partir du code INSEE de la commune (ne fonctionne pas pour Aix-en-Provence) if pd.isna(adr_cpostal) or not adr_cpostal: adr_cpostal = get_codepostal(adr_commune, adr_codeinsee) if not adr_cpostal: logging.warning( f\"{doc_arr['arr_pdf']}: Pas de code postal: cpostal(adr_brute)={adr_cpostal}, commune={adr_commune}, code_insee={adr_codeinsee}, get_codepostal={adr_cpostal}\" ) # - cr\u00e9er une adresse normalis\u00e9e ; la coh\u00e9rence des champs est v\u00e9rifi\u00e9e adr_interm = { \"num\": adr_num, # num\u00e9ro de la voie \"ind\": adr_ind, # indice de r\u00e9p\u00e9tition \"voie\": adr_voie, # nom de la voie \"compl\": adr_compl, # compl\u00e9ment d'adresse \"cpostal\": adr_cpostal, # code postal \"ville\": adr_commune, # ville } adr_norm = normalize_adresse(adr_interm) adr_adresse = create_adresse_normalisee( adr_norm[\"num\"], adr_norm[\"ind\"], adr_norm[\"voie\"], adr_norm[\"compl\"], adr_norm[\"cpostal\"], adr_norm[\"commune\"], ) # - rassembler les champs doc_adr = { # adresse \"adr_ad_brute\": adr_ad_brute, # adresse brute \"adr_num\": adr_norm[\"num\"], # num\u00e9ro de la voie \"adr_ind\": adr_norm[\"ind\"], # indice de r\u00e9p\u00e9tition \"adr_voie\": adr_norm[\"voie\"], # nom de la voie \"adr_compl\": adr_norm[\"compl\"], # compl\u00e9ment d'adresse \"adr_cpostal\": adr_norm[\"cpostal\"], # code postal \"adr_ville\": adr_norm[\"commune\"], # ville \"adr_adresse\": adr_adresse, # adresse normalis\u00e9e \"adr_codeinsee\": adr_codeinsee, # code insee (5 chars) # compl\u00e9t\u00e9 en aval par \"enrichi\" } # parcelle cadastrale ref_cad = ( normalize_string( getattr(df_row, \"parcelle\"), num=True, apos=True, hyph=True, spaces=True ) if pd.notna(getattr(df_row, \"parcelle\")) else None ) doc_par = { \"par_ref_cad\": ref_cad, # r\u00e9f\u00e9rence cadastrale } # notifi\u00e9s doc_not = { \"not_id_proprio\": ( normalize_string( getattr(df_row, \"proprio\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"proprio\")) else None ), # identification des propri\u00e9taires \"not_proprio\": \"\", # TODO liste des noms des propri\u00e9taires \"not_id_syndic\": ( normalize_string( getattr(df_row, \"syndic\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"syndic\")) else None ), # identification du syndic \"not_syndic\": \"\", # TODO nom du syndic \"not_id_gest\": ( normalize_string( getattr(df_row, \"gest\"), num=True, apos=True, hyph=True, spaces=True ) if pd.notna(getattr(df_row, \"gest\")) else None ), # identification du gestionnaire \"not_gest\": \"\", # TODO nom du gestionnaire } doc_data = doc_idu | doc_arr | doc_adr | doc_par | doc_not doc_rows.append(doc_data) df_docs = pd.DataFrame.from_records(doc_rows) df_docs = df_docs.astype(dtype=DTYPE_DATA) return df_docs determine_commune(adr_commune_brute, adr_commune_maire) D\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9. R\u00e9concilie la commune \u00e9ventuellement contenue dans l'adresse du ou des b\u00e2timents vis\u00e9s avec le nom de commune extrait du document (template, autorit\u00e9 ou lieu de signature). Parameters adr_commune_brute: str Commune extraite de l'adresse du b\u00e2timent vis\u00e9 par l'arr\u00eat\u00e9. adr_commune_maire: str Commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou du template du document. Returns adr_commune: str Commune de l'adresse vis\u00e9e. src\\process\\extract_data.py def determine_commune(adr_commune_brute: str, adr_commune_maire: str) -> str: \"\"\"D\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9. R\u00e9concilie la commune \u00e9ventuellement contenue dans l'adresse du ou des b\u00e2timents vis\u00e9s avec le nom de commune extrait du document (template, autorit\u00e9 ou lieu de signature). Parameters ---------- adr_commune_brute: str Commune extraite de l'adresse du b\u00e2timent vis\u00e9 par l'arr\u00eat\u00e9. adr_commune_maire: str Commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou du template du document. Returns ------- adr_commune: str Commune de l'adresse vis\u00e9e. \"\"\" # TODO normaliser vers la graphie de la table des codes INSEE? Quid des arrondissements de Marseille? # TODO comparer les graphies? if (pd.isna(adr_commune_brute)) and (pd.isna(adr_commune_maire)): # pas de commune adr_commune = None elif (pd.isna(adr_commune_maire)) or ( not P_COMMUNES_AMP_ALLFORMS.match(adr_commune_maire) ): adr_commune = adr_commune_brute # TODO normaliser? elif (pd.isna(adr_commune_brute)) or ( not P_COMMUNES_AMP_ALLFORMS.match(adr_commune_brute) ): adr_commune = adr_commune_maire else: # was: adr_commune = adr_commune_maire adr_commune = adr_commune_brute # .title() si on veut minimiser les diff\u00e9rences avec adr_commune_maire pour comparer return adr_commune Analyse un arr\u00eat\u00e9 et en extrait les donn\u00e9es. create_file_name_url(file_name, allowance=155) Creates a url-compliant filename by removing all bad characters and maintaining the windows path length limit (which by default is 255) 155 to take into account the path length Parameters file_name: str Nom du fichier allowance: int Longueur maximale du chemin complet (chemin + nom de fichier) src\\process\\parse_doc_direct.py def create_file_name_url(file_name: str, allowance: int = 155): \"\"\" Creates a url-compliant filename by removing all bad characters and maintaining the windows path length limit (which by default is 255) 155 to take into account the path length Parameters ---------- file_name: str Nom du fichier allowance: int Longueur maximale du chemin complet (chemin + nom de fichier) \"\"\" bad_characters = re.compile(r\"[\\\\/<>,:\\\"|?*^$&{}\\[\\]`\\x00-\\x1F\\x7F]+\") if allowance > 255: allowance = 255 # on most common filesystems, including NTFS a file_name can not exceed 255 characters # assign allowance for things that must be in the file name # make sure that user input doesn't contain bad characters file_name = bad_characters.sub(\"\", file_name) file_name = file_name.replace(\"'\", \"_\").replace(\" \", \"_\") ret = \"\" for string in [file_name]: length = len(string) if allowance - length < 0: string = string[:allowance] length = len(string) ret += string allowance -= length if allowance < 0: raise ValueError( \"\"\"It is not possible to give a reasonable file name, due to length limitations. Consider changing location to somewhere with a shorter path.\"\"\" ) return ret enrich_adresse(fn_pdf, adresse, commune_maire) Consolide et enrichit une adresse, avec ville et codes (INSEE et code postal). Harmonise et compl\u00e8te les informations extraites de l'adresse vis\u00e9e \u00e0 partir des informations extraites du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9. Ajoute une adresse normalis\u00e9e. Parameters fn_pdf: str Nom du fichier PDF (pour debug). adresse: dict Adresse vis\u00e9e par le document. commune_maire: str Ville extraite du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9. Returns adresse_enr: dict Adresse enrichie et augment\u00e9e. src\\process\\parse_doc_direct.py def enrich_adresse(fn_pdf: str, adresse: dict, commune_maire: str) -> Dict: \"\"\"Consolide et enrichit une adresse, avec ville et codes (INSEE et code postal). Harmonise et compl\u00e8te les informations extraites de l'adresse vis\u00e9e \u00e0 partir des informations extraites du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9. Ajoute une adresse normalis\u00e9e. Parameters ---------- fn_pdf: str Nom du fichier PDF (pour debug). adresse: dict Adresse vis\u00e9e par le document. commune_maire: str Ville extraite du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9. Returns ------- adresse_enr: dict Adresse enrichie et augment\u00e9e. \"\"\" adresse_enr = adresse.copy() # - d\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9 en reconciliant la commune mentionn\u00e9e # dans cette adresse avec celle extraite des mentions de l'autorit\u00e9 ou du template adresse_enr[\"ville\"] = determine_commune(adresse_enr[\"ville\"], commune_maire) if not adresse_enr[\"ville\"]: logging.warning( f\"{fn_pdf}: impossible de d\u00e9terminer la commune: {adresse_enr['ville'], commune_maire}\" ) # - d\u00e9terminer le code INSEE de la commune # FIXME communes hors M\u00e9tropole: le filtrage sera-t-il fait en amont, lors de l'extraction depuis actes? sinon AssertionError ici try: codeinsee = get_codeinsee(adresse_enr[\"ville\"], adresse_enr[\"cpostal\"]) except AssertionError: print( f\"{fn_pdf}: get_codeinsee(): adr_ville={adresse_enr['ville']}, adr_cpostal={adresse_enr['cpostal']}\" ) print(f\"{adresse}\") raise else: if not codeinsee: logging.warning( f\"{fn_pdf}: impossible de d\u00e9terminer le code INSEE: {adresse_enr['ville'], adresse_enr['cpostal']}\" ) # - si l'adresse ne contenait pas de code postal, essayer de d\u00e9terminer le code postal # \u00e0 partir du code INSEE de la commune (ne fonctionne pas pour Aix-en-Provence) if not adresse_enr[\"cpostal\"]: adresse_enr[\"cpostal\"] = get_codepostal(adresse_enr[\"ville\"], codeinsee) if not adresse_enr[\"cpostal\"]: logging.warning( f\"{fn_pdf}: Pas de code postal: adr_brute={adresse_enr['ad_brute']}, commune={adresse_enr['ville']}, code_insee={codeinsee}, get_codepostal={adresse_enr['cpostal']}\" ) # - cr\u00e9er une adresse normalis\u00e9e ; la coh\u00e9rence des champs est v\u00e9rifi\u00e9e adresse_enr = normalize_adresse(adresse_enr) if adresse_enr[\"ad_brute\"]: adresse_enr[\"adresse\"] = create_adresse_normalisee( adresse_enr[\"num\"], adresse_enr[\"ind\"], adresse_enr[\"voie\"], adresse_enr[\"compl\"], adresse_enr[\"cpostal\"], adresse_enr[\"ville\"], ) else: adresse_enr[\"adresse\"] = None # - positionner finalement le code INSEE adresse_enr[\"codeinsee\"] = codeinsee return adresse_enr extract_adresses_commune(fn_pdf, pg_txt_body, commune_maire) Extraire les adresses vis\u00e9es par l'arr\u00eat\u00e9, et la commune. Parameters fn_pdf: string Nom du fichier PDF de l'arr\u00eat\u00e9 (pour les messages de logs: warnings et erreurs) pg_txt_body: string Corps de texte de la page commune_maire: string Mention de la commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou des Returns adresses: list(dict) Adresses vis\u00e9es par l'arr\u00eat\u00e9 src\\process\\parse_doc_direct.py def extract_adresses_commune( fn_pdf: str, pg_txt_body: str, commune_maire: str ) -> List[Dict]: \"\"\"Extraire les adresses vis\u00e9es par l'arr\u00eat\u00e9, et la commune. Parameters ---------- fn_pdf: string Nom du fichier PDF de l'arr\u00eat\u00e9 (pour les messages de logs: warnings et erreurs) pg_txt_body: string Corps de texte de la page commune_maire: string Mention de la commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou des Returns ------- adresses: list(dict) Adresses vis\u00e9es par l'arr\u00eat\u00e9 \"\"\" try: adresses_visees = get_adr_doc(pg_txt_body) except AssertionError: logging.error(f\"{fn_pdf}: probl\u00e8me d'extraction d'adresse\") raise # if fn_pdf == \"p\u00e9rim\u00e8tre de s\u00e9curit\u00e9 82 Hoche 105 Kleber 13003.pdf\": # print(f\"{commune_maire}, {adresses_visees}\") # # raise ValueError(\"don't stop me now (too soon)\") if not adresses_visees: adr = { # adresse brute \"ad_brute\": None, # champs \"num\": None, \"ind\": None, \"voie\": None, \"compl\": None, \"cpostal\": None, \"ville\": None, # adresse propre \"adresse\": None, } adr_enr = enrich_adresse(fn_pdf, adr, commune_maire) return [adr_enr] # renommer les champs # TODO le faire dans get_adr_doc et adapter le code dans les autres modules adresses_visees = [ { \"ad_brute\": x[\"adresse_brute\"], \"adresses\": [ {k.replace(\"adr_\", \"\"): v for k, v in y.items()} for y in x[\"adresses\"] ], } for x in adresses_visees ] # prendre la 1re zone d'adresses reconnue dans le texte (heuristique) # TODO en rep\u00e9rer d'autres? incertain adr0 = adresses_visees[0] adresse_brute = adr0[\"ad_brute\"] # TODO am\u00e9liorer les r\u00e9sultats par une collecte plus exhaustive (qui n\u00e9cessiterait le d\u00e9doublonnage) ou une meilleure heuristique ? # extraire la ou les adresses de cette zone # (on supprime au passage les pr\u00e9fixes \"adr_\" des noms des champs, archa\u00efsme \u00e0 corriger plus tard \u00e9ventuellement) adresses = [({\"ad_brute\": adresse_brute} | x) for x in adr0[\"adresses\"]] if not adresses: logging.error( f\"{fn_pdf}: aucune adresse extraite de la zone d'adresse(s): {adresse_brute}\" ) if len(adresses_visees) > 1: # si la 1re adresse n'a pas de code postal, tenter de r\u00e9cup\u00e9rer le code postal des adresses suivantes # on construit 2 mappings: # - (num, voie) => cp numvoie2cp = dict() # - voie => cp # fallback, quand la mention d'adresse extraite ne contient pas de num\u00e9ro (mais une mention ult\u00e9rieure, oui) voie2cp = dict() # on it\u00e8re sur l'ensemble des adresses extraites du document pour cr\u00e9er une table d'association vers les codes postaux for x in adresses_visees: for y in x[\"adresses\"]: if y[\"cpostal\"]: norm_voie = normalize_string( remove_accents(y[\"voie\"]), num=True, apos=True, hyph=True, spaces=True, ).lower() # (num\u00e9ro, voie) -> cp numvoie2cp[(y[\"num\"], norm_voie)] = y[\"cpostal\"] # fallback: voie -> cp voie2cp[norm_voie] = y[\"cpostal\"] # WIP 2023-05-09 # print(numvoie2cp) for sel_adr in adresses: # pour chaque adresse consid\u00e9r\u00e9e comme \u00e9tant vis\u00e9e par l'arr\u00eat\u00e9 if sel_adr[\"voie\"] and not sel_adr[\"cpostal\"]: # si on a une voie mais pas de code postal, on essaie de renseigner # le code postal par propagation \u00e0 partir des autres adresses norm_voie = normalize_string( remove_accents(sel_adr[\"voie\"]), num=True, apos=True, hyph=True, spaces=True, ).lower() if sel_adr[\"num\"]: # si on a un num\u00e9ro de voie (c'est l'id\u00e9al, car le code postal est normalement unique) sel_short = (sel_adr[\"num\"], norm_voie) # print(f\">>>>>> sel_short: {sel_short}\") sel_adr[\"cpostal\"] = numvoie2cp.get(sel_short, None) else: # sans num\u00e9ro de voie, on recourt au tableau associatif sans num\u00e9ro sel_adr[\"cpostal\"] = voie2cp.get(norm_voie, None) # WIP 2023-05-09 # if fn_pdf == \"90 cours Sextius - ML.pdf\": # print(f\"{adresses_visees}\\n{numvoie2cp}\\n{adresses}\") # raise ValueError(\"don't stop me now\") # pass # si besoin d'une alternative: d\u00e9terminer commune, code INSEE et code postal pour les adresses[0] et propager les valeurs aux autres adresses adresses_enr = [enrich_adresse(fn_pdf, x, commune_maire) for x in adresses] return adresses_enr parse_arrete(fp_pdf_in, fp_txt_in) Analyse un arr\u00eat\u00e9 et extrait les donn\u00e9es qu'il contient. L'arr\u00eat\u00e9 est d\u00e9coup\u00e9 en paragraphes puis les donn\u00e9es sont extraites. Parameters fp_pdf_in : Path Fichier PDF source (temporairement?) fp_txt_in : Path Fichier texte \u00e0 analyser. Returns doc_data : dict Donn\u00e9es extraites du document. src\\process\\parse_doc_direct.py def parse_arrete(fp_pdf_in: Path, fp_txt_in: Path) -> dict: \"\"\"Analyse un arr\u00eat\u00e9 et extrait les donn\u00e9es qu'il contient. L'arr\u00eat\u00e9 est d\u00e9coup\u00e9 en paragraphes puis les donn\u00e9es sont extraites. Parameters ---------- fp_pdf_in : Path Fichier PDF source (temporairement?) fp_txt_in : Path Fichier texte \u00e0 analyser. Returns ------- doc_data : dict Donn\u00e9es extraites du document. \"\"\" fn_pdf = fp_pdf_in.name fn_pdf_out = create_file_name_url(fn_pdf) pages = load_pages_text(fp_txt_in) if not any(pages): logging.warning(f\"{fp_txt_in}: aucune page de texte\") arr_url = FS_URL_FALLBACK.format(pdf=fn_pdf_out) logging.warning(f\"URL temporaire (sans code commune ni ann\u00e9e): {arr_url}\") return { \"adresses\": [], \"arretes\": [ { \"pdf\": fn_pdf, \"url\": arr_url, } ], \"notifies\": [], \"parcelles\": [], } # filtrer les pages qui sont \u00e0 sortir du traitement: # - la ou les \u00e9ventuelles pages d'accus\u00e9 de r\u00e9ception d'actes pages_ar = [i for i, x in enumerate(pages, start=1) if P_ACCUSE.match(x)] if pages_ar: logging.warning( f\"{fp_txt_in}: {len(pages_ar)} page(s) d'accus\u00e9 de r\u00e9ception actes: {pages_ar} (sur {len(pages)})\" ) # - la ou les \u00e9ventuelles pages d'annexes ? (TODO) skip_pages = pages_ar # remplacer les pages filtr\u00e9es par une cha\u00eene vide filt_pages = [ (x if i not in skip_pages else \"\") for i, x in enumerate(pages, start=1) ] # analyser la structure des pages doc_content = parse_arrete_pages(fn_pdf, filt_pages) # extraire les donn\u00e9es adresses = [] arretes = {} # un seul notifies = { \"proprios\": OrderedDict(), # propri\u00e9taires \"syndics\": OrderedDict(), # syndic (normalement unique) \"gests\": OrderedDict(), # gestionnaire (normalement unique) } parcelles = OrderedDict() # r\u00e9f\u00e9rences de parcelles cadastrales # - au pr\u00e9alable, rassembler toutes les donn\u00e9es en ajoutant le num\u00e9ro de page (FIXME) pages_body = [pg_cont[\"body\"] for pg_cont in doc_content] # pages_cont = [pg_cont[\"content\"] for pg_cont in doc_content] # future pages_cont = [] for pg_num, pg_cont in enumerate(doc_content, start=1): # pg_template = page_cont[\"template\"] # pg_content = page_cont[\"content\"] # future # FIXME ajouter \"page_num\" en amont, dans parse_arrete_pages() pages_cont.extend([({\"page_num\": pg_num} | x) for x in pg_cont[\"content\"]]) # extraire les champs un par un: # - arr\u00eat\u00e9 arr_dates = [ process_date_brute(x[\"span_txt\"]) for x in pages_cont if x[\"span_typ\"] == \"arr_date\" ] if arr_dates: arretes[\"date\"] = normalize_string( arr_dates[0], num=True, apos=True, hyph=True, spaces=True ) arr_nums = [x[\"span_txt\"] for x in pages_cont if x[\"span_typ\"] == \"num_arr\"] if arr_nums: arretes[\"num_arr\"] = normalize_string( arr_nums[0], num=True, apos=True, hyph=True, spaces=True ) arr_noms = [x[\"span_txt\"] for x in pages_cont if x[\"span_typ\"] == \"nom_arr\"] if arr_noms: arretes[\"nom_arr\"] = normalize_string( arr_noms[0], num=True, apos=True, hyph=True, spaces=True ) # - commune extraite des mentions de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou du template du document adrs_commune_maire = [x for x in pages_cont if x[\"span_typ\"] == \"adr_ville\"] # - prendre arbitrairement la 1re mention et la nettoyer a minima # TODO regarder les erreurs et v\u00e9rifier si un autre choix donnerait de meilleurs r\u00e9sultats # TODO tester: si > 1, tester de matcher avec la liste des communes de la m\u00e9tropole # (et \u00e9ventuellement calculer la distance de Levenshtein pour v\u00e9rifier s'il est vraisemblable # que ce soient des variantes de graphie ou erreurs) if not adrs_commune_maire: adr_commune_maire = None else: adr_commune_maire = normalize_string( adrs_commune_maire[0][\"span_txt\"], num=True, apos=True, hyph=True, spaces=True, ) # remplacer par la forme canonique (communes AMP) adr_commune_maire = normalize_ville(adr_commune_maire) logging.warning(f\"adrs_commune_maire: {adrs_commune_maire}\") # DEBUG logging.warning(f\"adr_commune_maire: {adr_commune_maire}\") # DEBUG # # parcelles codeinsee = None # valeur par d\u00e9faut cpostal = None # valeur par d\u00e9faut for pg_txt_body in pages_body: if pg_txt_body: # extraire les informations sur l'arr\u00eat\u00e9 if \"classe\" not in arretes and (classe := get_classe(pg_txt_body)): arretes[\"classe\"] = classe if \"urgence\" not in arretes and (urgence := get_urgence(pg_txt_body)): arretes[\"urgence\"] = urgence if \"demo\" not in arretes and (demo := get_demo(pg_txt_body)): arretes[\"demo\"] = demo if \"int_hab\" not in arretes and (int_hab := get_int_hab(pg_txt_body)): arretes[\"int_hab\"] = int_hab if \"equ_com\" not in arretes and (equ_com := get_equ_com(pg_txt_body)): arretes[\"equ_com\"] = equ_com if \"pdf\" not in arretes: arretes[\"pdf\"] = fn_pdf # extraire la ou les adresse(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 d\u00e9tect\u00e9es sur cette page if not adresses: # pour le moment, on se contente de la premi\u00e8re page contenant au moins une zone d'adresse, # et sur cette page, de la premi\u00e8re zone d'adresse trouv\u00e9e ; # une zone peut contenir une ou plusieurs adresses obtenues par \"d\u00e9pliage\" (ex: 12 - 14 rue X) # TODO examiner les erreurs et d\u00e9terminer si une autre strat\u00e9gie donnerait de meilleurs r\u00e9sultats # si une adresse a d\u00e9j\u00e0 \u00e9t\u00e9 ajout\u00e9e mais qu'elle n'a \u00e9t\u00e9 remplie que gr\u00e2ce \u00e0 commune_maire pg_adresses = extract_adresses_commune( fn_pdf, pg_txt_body, adr_commune_maire ) if pg_adresses: adresses.extend(pg_adresses) # WIP on prend le code INSEE et code postal de la 1re adresse # print(adrs_doc) cpostal = adresses[0][\"cpostal\"] codeinsee = adresses[0][\"codeinsee\"] if (\"codeinsee\" not in arretes) and codeinsee: arretes[\"codeinsee\"] = codeinsee elif len(adresses) == 1 and not adresses[0][\"ad_brute\"]: # si une adresse a d\u00e9j\u00e0 \u00e9t\u00e9 ajout\u00e9e mais qu'elle n'a \u00e9t\u00e9 remplie que gr\u00e2ce \u00e0 commune_maire # (donc ne contient qu'une commune), on en cherche une plus pr\u00e9cise sur la page suivante, # \u00e0 tout hasard pg_adresses = extract_adresses_commune( fn_pdf, pg_txt_body, adr_commune_maire ) if pg_adresses and pg_adresses[0][\"ad_brute\"]: # on a bien extrait au moins une adresse du texte, on remplace l'adresse contenant # seulement une commune adresses = pg_adresses # WIP on prend le code INSEE et code postal de la 1re adresse # print(adrs_doc) cpostal = adresses[0][\"cpostal\"] codeinsee = adresses[0][\"codeinsee\"] if codeinsee: # on remplace le code commune INSEE pour tout le document arretes[\"codeinsee\"] = codeinsee # extraire les notifi\u00e9s if proprios := get_proprio(pg_txt_body): norm_proprios = normalize_string( proprios, num=True, apos=True, hyph=True, spaces=True ) notifies[\"proprios\"][ norm_proprios ] = proprios # WIP: proprios = [] + extend() if syndics := get_syndic(pg_txt_body): norm_syndics = normalize_string( syndics, num=True, apos=True, hyph=True, spaces=True ) notifies[\"syndics\"][ norm_syndics ] = syndics # WIP: syndics = [] + extend ? if gests := get_gest(pg_txt_body): norm_gests = normalize_string( gests, num=True, apos=True, hyph=True, spaces=True ) notifies[\"gests\"][norm_gests] = gests # WIP: gests = [] + extend ? # extraire la ou les parcelles vis\u00e9es par l'arr\u00eat\u00e9 if pg_parcelles_str_list := get_parcelles(pg_txt_body): # TODO supprimer les r\u00e9f\u00e9rences partielles (ex: Marseille mais sans code quartier) si la r\u00e9f\u00e9rence compl\u00e8te est aussi pr\u00e9sente dans le doc refcads_norm = [ generate_refcadastrale_norm( codeinsee, pg_parcelles_str, fn_pdf, cpostal ) for pg_parcelles_str in pg_parcelles_str_list ] parcelles = parcelles | OrderedDict( zip(refcads_norm, pg_parcelles_str_list) ) # WIP get_parcelles:list() if False: # WIP hypoth\u00e8ses sur les notifi\u00e9s try: assert len(notifies[\"proprios\"]) <= 1 assert len(notifies[\"syndics\"]) <= 1 assert len(notifies[\"gests\"]) <= 1 except AssertionError: print(f\"{notifies}\") raise # d\u00e9placer le PDF et d\u00e9terminer l'URL if \"codeinsee\" in arretes: if \"date\" in arretes: # r\u00e9-extraire l'ann\u00e9e de la date format\u00e9e # TODO stocker l'ann\u00e9e dans un champ d\u00e9di\u00e9, au moment de l'extraction et normalisation # de la date, et le r\u00e9cup\u00e9rer ici? # code correct: # arr_year = datetime.strptime(arretes[\"date\"], \"%d/%m/%Y\").date().year # mais ne fonctionne pas sur des dates mal reconnues (OCR) ex: \"00/02/2022\" # alors qu'on peut extraire l'ann\u00e9e arr_year = arretes[\"date\"].rsplit(\"/\", 1)[1] arr_comm = arretes[\"codeinsee\"] arretes[\"url\"] = FS_URL.format( commune=arretes[\"codeinsee\"], yyyy=arr_year, pdf=fn_pdf_out ) else: arretes[\"url\"] = FS_URL_NO_YEAR.format( commune=arretes[\"codeinsee\"], pdf=fn_pdf_out ) logging.warning(f\"URL temporaire (sans ann\u00e9e): {arretes['url']}\") else: # (\"codeinsee\" not in arretes) # dans le pire cas: (arretes == {}) arretes = {\"pdf\": fn_pdf, \"url\": FS_URL_FALLBACK.format(pdf=fn_pdf_out)} logging.warning( f\"URL temporaire (sans code commune ni ann\u00e9e): {arretes['url']}\" ) # notifies # formes brutes puis normalis\u00e9es # * propri\u00e9taires id_proprio = list(notifies[\"proprios\"])[0] if notifies[\"proprios\"] else None proprio = id_proprio # TODO appliquer une normalisation? # * syndic id_syndic = list(notifies[\"syndics\"])[0] if notifies[\"syndics\"] else None if ( id_syndic is not None and (P_NOMS_CABINETS.search(id_syndic) is None) and (P_CABINET.search(id_syndic) is None) and (P_MONSIEUR_MADAME.search(id_syndic) is not None) ): # si le champ \"id_syndic\" ne contient pas de mention de cabinet ou d'agence, # et contient une r\u00e9f\u00e9rence \u00e0 une personne physique, # alors la valeur normalis\u00e9e est \"syndic b\u00e9n\u00e9vole\" # TODO si on observe trop de faux positifs, mettre en place une condition # plus restrictive sur la cha\u00eene \"b\u00e9n\u00e9vole\" syndic = \"Syndic b\u00e9n\u00e9vole\" else: # sinon, valeur normalis\u00e9e (fallback: id_syndic) syndic = normalize_nom_cabinet(id_syndic) # * gestionnaire id_gest = list(notifies[\"gests\"])[0] if notifies[\"gests\"] else None # valeur normalis\u00e9e (fallback: id_syndic) gest = normalize_nom_cabinet(id_gest) # doc_data = { \"adresses\": adresses, \"arretes\": [arretes], # a priori un seul par fichier \"notifies\": [ { \"id_proprio\": id_proprio, \"proprio\": proprio, # forme normalis\u00e9e \"id_syndic\": id_syndic, \"syndic\": syndic, # forme normalis\u00e9e \"id_gest\": id_gest, \"gest\": gest, # forme normalis\u00e9e \"codeinsee\": codeinsee, } ], # a priori un seul par fichier (pour le moment) \"parcelles\": [{\"ref_cad\": x, \"codeinsee\": codeinsee} for x in parcelles], } return doc_data process_files(df_in, out_dir, date_exec) Analyse le texte des fichiers PDF extrait dans des fichiers TXT. Parameters df_in: pd.DataFrame Fichier meta_$RUN_otxt.csv contenant les m\u00e9tadonn\u00e9es enrichies et les fichiers PDF et TXT (natif ou OCR) \u00e0 traiter. out_dir : Path Dossier de sortie date_exec : date Date d'ex\u00e9cution du script, utilis\u00e9e pour (a) le nom des copies de fichiers CSV incluant la date de traitement, (b) l'identifiant unique des arr\u00eat\u00e9s dans les 4 tables, (c) le champ 'datemaj' initialement rempli avec la date d'ex\u00e9cution. Returns out_files : Dict[str, Path] Fichiers CSV produits, contenant les donn\u00e9es extraites. Dictionnaire index\u00e9 par les cl\u00e9s {\"adresse\", \"arrete\", \"notifie\", \"parcelle\"}. src\\process\\parse_doc_direct.py def process_files( df_in: pd.DataFrame, out_dir: Path, date_exec: date, ) -> Dict[str, Path]: \"\"\"Analyse le texte des fichiers PDF extrait dans des fichiers TXT. Parameters ---------- df_in: pd.DataFrame Fichier meta_$RUN_otxt.csv contenant les m\u00e9tadonn\u00e9es enrichies et les fichiers PDF et TXT (natif ou OCR) \u00e0 traiter. out_dir : Path Dossier de sortie date_exec : date Date d'ex\u00e9cution du script, utilis\u00e9e pour (a) le nom des copies de fichiers CSV incluant la date de traitement, (b) l'identifiant unique des arr\u00eat\u00e9s dans les 4 tables, (c) le champ 'datemaj' initialement rempli avec la date d'ex\u00e9cution. Returns ------- out_files : Dict[str, Path] Fichiers CSV produits, contenant les donn\u00e9es extraites. Dictionnaire index\u00e9 par les cl\u00e9s {\"adresse\", \"arrete\", \"notifie\", \"parcelle\"}. \"\"\" # - les fichiers CSV dat\u00e9s sont stock\u00e9s dans un sous-dossier \"csv_historique\" out_dir_csv = out_dir / \"csv_historique\" logging.info( f\"Sous-dossier de sortie: {out_dir_csv} {'existe d\u00e9j\u00e0' if out_dir_csv.is_dir() else 'va \u00eatre cr\u00e9\u00e9'}.\" ) out_dir_csv.mkdir(parents=True, exist_ok=True) # - les fichiers PDF \u00e0 reclasser sont stock\u00e9s dans un sous-dossier (temporaire) \"pdf_a_reclasser\" out_dir_pdf_areclass = out_dir / \"pdf_analyses/pdf_a_reclasser\" logging.info( f\"Sous-dossier de sortie: {out_dir_pdf_areclass} {'existe d\u00e9j\u00e0' if out_dir_pdf_areclass.is_dir() else 'va \u00eatre cr\u00e9\u00e9'}.\" ) out_dir_pdf_areclass.mkdir(parents=True, exist_ok=True) # - les fichiers TXT extraits nativement ou par OCR dans un sous-dossier \"txt\" out_dir_txt = out_dir / \"txt\" logging.info( f\"Sous-dossier de sortie: {out_dir_txt} {'existe d\u00e9j\u00e0' if out_dir_txt.is_dir() else 'va \u00eatre cr\u00e9\u00e9'}.\" ) out_dir_txt.mkdir(parents=True, exist_ok=True) # 0. charger la liste des PDF d\u00e9j\u00e0 trait\u00e9s, d\u00e9finis comme les PDF d\u00e9j\u00e0 # pr\u00e9sents dans un des fichiers \"paquet_arrete_*.csv\" fps_paquet_arrete = sorted( out_dir_csv.glob( f\"paquet_arrete_[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]_[0-9][0-9].csv\" ) ) pdfs_old = [] for fp_paquet_arrete in fps_paquet_arrete: df_arr_old = pd.read_csv(fp_paquet_arrete, dtype=DTYPE_ARRETE, sep=\";\") pdfs_old.extend(df_arr_old[\"pdf\"]) pdfs_old = set(pdfs_old) # 1. d\u00e9terminer le nom des fichiers de sortie # les noms des fichiers de sortie incluent: # - la date de traitement (ex: \"2023-05-30\") date_proc_dash = date_exec.strftime(\"%Y-%m-%d\") # - le num\u00e9ro d'ex\u00e9cution ce jour (ex: \"02\"), calcul\u00e9 en recensant les # \u00e9ventuels fichiers existants out_prevruns = sorted( itertools.chain.from_iterable( out_dir_csv.glob(f\"paquet_{x}_{date_proc_dash}_[0-9][0-9].csv\") for x in OUT_BASENAMES ) ) # - num\u00e9ro d'ex\u00e9cution du script ce jour i_run = 0 # init for fp_prevrun in out_prevruns: # le num\u00e9ro se trouve \u00e0 la fin du stem, apr\u00e8s le dernier s\u00e9parateur \"_\" fp_out_idx = int(fp_prevrun.stem.split(\"_\")[-1]) i_run = max(i_run, fp_out_idx) i_run += 1 # on prend le num\u00e9ro d'ex\u00e9cution suivant # r\u00e9sultat: fichiers g\u00e9n\u00e9r\u00e9s par cette ex\u00e9cution out_files = { x: out_dir_csv / f\"paquet_{x}_{date_proc_dash}_{i_run:>02}.csv\" for x in OUT_BASENAMES } # 2. d\u00e9terminer le premier identifiant unique (idu) des prochaines entr\u00e9es: # il suit le dernier idu g\u00e9n\u00e9r\u00e9 par les ex\u00e9cutions pr\u00e9c\u00e9dentes le m\u00eame jour i_idu = 0 # init for fp_prevrun in out_prevruns: # ouvrir le fichier, lire les idus, prendre le dernier, extraire l'index s_idus = pd.read_csv( fp_prevrun, usecols=[\"idu\"], dtype={\"idu\": \"string\"}, sep=\";\" )[\"idu\"] max_idx = s_idus.str.rsplit(\"-\", n=1, expand=True)[1].astype(\"int32\").max() i_idu = max(i_idu, max_idx) i_idu += 1 # on prend le num\u00e9ro d'arr\u00eat\u00e9 suivant # 3. filtrer les arr\u00eat\u00e9s # - filtrer les documents hors p\u00e9rim\u00e8tre th\u00e9matique ou g\u00e9ographique ? # TODO v\u00e9rifier si ok sans liste d'exclusion ici ; sinon corriger avant d\u00e9ploiement? # df_in[\"pdf\"].str.split(\"-\", 1)[0] not in set(EXCLUDE_FILES + EXCLUDE_FIXME_FILES) # + EXCLUDE_HORS_AMP) # # - filtrer les fichiers d\u00e9j\u00e0 trait\u00e9s: ne garder que les PDF qui ne # sont pas d\u00e9j\u00e0 pr\u00e9sents dans un \"paquet_arrete_*.csv\" pdfs_in_old = set(df_in[\"pdf\"].tolist()).intersection(pdfs_old) # v\u00e9rifier que le fichier PDF existe bien dans les dossiers destination, # pdf_a_reclasser ou un dossier de commune, # sinon il faut le traiter comme s'il \u00e9tait compl\u00e8tement nouveau already_proc = [] # sous-dossiers par code commune (INSEE), sur 5 chiffres out_dir_pdf_communes = out_dir / \"[0-9][0-9][0-9][0-9][0-9]\" for fn in pdfs_in_old: areclass = sorted(out_dir_pdf_areclass.rglob(fn)) bienclas = sorted(out_dir_pdf_communes.rglob(fn)) if areclass or bienclas: logging.warning( f\"Fichier \u00e0 ignorer car d\u00e9j\u00e0 trait\u00e9: {areclass[0] if areclass else bienclas[0]}\" ) already_proc.append(fn) already_proc = set(already_proc) # s_dups = df_in[\"pdf\"].isin(already_proc) if any(s_dups): logging.info( f\"{s_dups.sum()} fichiers seront d\u00e9plac\u00e9s dans 'doublons/'\" + \" et ne seront pas retrait\u00e9s, car ils sont d\u00e9j\u00e0 pr\u00e9sents\" + \" dans un fichier 'paquet_arrete_*.csv' de 'csv_historique/'\" + \" et dans un dossier de commune\" + \" ou 'pdf_a_reclasser' .\" ) # d\u00e9placer les fichiers d\u00e9j\u00e0 trait\u00e9s dans doublons/ # (plus prudent que de les supprimer d'embl\u00e9e) out_dups = out_dir / \"doublons\" out_dups.mkdir(exist_ok=True) # df_dups = df_in[s_dups] for df_row in df_dups.itertuples(): fp = Path(df_row.fullpath) fp_dst = out_dups / fp.name shutil.move(fp, fp_dst) # si le move a r\u00e9ussi, on peut supprimer le fichier dans le dossier d'entr\u00e9e if fp_dst.is_file(): fp_orig = Path(df_row.origpath) fp_orig.unlink() # df_in = df_in[~s_dups] # si apr\u00e8s filtrage, df_in est vide, aucun fichier CSV ne sera produit # et on peut sortir imm\u00e9diatement if df_in.empty: return {} # 4 tables de sortie rows_adresse = [] rows_arrete = [] rows_notifie = [] rows_parcelle = [] # date de traitement, en 2 formats date_proc = date_exec.strftime(\"%Y%m%d\") # pour \"idu\" (id uniques des arr\u00eat\u00e9s) datemaj = date_exec.strftime(\"%d/%m/%Y\") # pour \"datemaj\" des 4 tables # identifiant des entr\u00e9es dans les fichiers de sortie: <type arr\u00eat\u00e9>-<date du traitement>-<index> # it\u00e9rer sur les fichiers PDF et TXT for i, df_row in enumerate(df_in.itertuples(), start=i_idu): # fichier PDF fp_pdf = Path(df_row.fullpath) if not fp_pdf.is_file(): raise ValueError(f\"{fp_pdf}: fichier PDF introuvable ({fp_txt})\") # fichier TXT (OCR sinon natif) fp_txt = Path(df_row.fullpath_txt) if not fp_txt.is_file(): raise ValueError(f\"{fp_pdf}: fichier TXT introuvable ({fp_txt})\") # type d'arr\u00eat\u00e9 ; \u00e0 date, seulement des arr\u00eat\u00e9s de p\u00e9ril \"AP\" ; # \u00e0 l'avenir, pourrait \u00eatre pr\u00e9dit \u00e0 partir du texte, avec un classifieur type_arr = \"AP\" # identifiant unique du document dans les tables de sortie (paquet_*.csv): # TODO d\u00e9tecter le ou les \u00e9ventuels fichiers d\u00e9j\u00e0 produits ce jour, \u00e9carter les doublons (blake2b?) # et initialiser le compteur \u00e0 la prochaine valeur # format: {type d'arr\u00eat\u00e9}-{date}-{id relatif, sur 4 chiffres} idu = f\"{type_arr}-{date_proc}-{i:04}\" # analyser le texte doc_data = parse_arrete(fp_pdf, fp_txt) # ajouter des entr\u00e9es dans les 4 tables rows_adresse.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"adresses\"] ) rows_arrete.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"arretes\"] ) rows_notifie.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"notifies\"] ) rows_parcelle.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"parcelles\"] ) # cr\u00e9er les 4 DataFrames et les exporter en CSV for key, rows, dtype in [ (\"adresse\", rows_adresse, DTYPE_ADRESSE), (\"arrete\", rows_arrete, DTYPE_ARRETE), (\"notifie\", rows_notifie, DTYPE_NOTIFIE), (\"parcelle\", rows_parcelle, DTYPE_PARCELLE), ]: out_file = out_files[key] df = pd.DataFrame.from_records(rows) for dtype_key, _ in dtype.items(): if dtype_key not in df.columns: df[dtype_key] = np.nan df = df.astype(dtype=dtype) df.to_csv(out_file, index=False, sep=\";\") # d\u00e9placer les fichiers PDF trait\u00e9s ; # le code est redondant avec celui utilis\u00e9 pour remplir le champ d'URL # mais on fait les d\u00e9placements de fichiers apr\u00e8s l'\u00e9criture du dataframe # pour \u00e9viter de d\u00e9placer le fichier si les CSV ne sont finalement pas # produits (eg. \u00e0 cause d'un \u00e9chec sur un autre document) df_arr = pd.read_csv(out_files[\"arrete\"], dtype=DTYPE_ARRETE, sep=\";\") for df_row in df_arr.itertuples(): # nom du PDF (incluant hash) fn = df_row.pdf # d\u00e9terminer le dossier destination if pd.notna(df_row.codeinsee): commune = df_row.codeinsee if pd.notna(df_row.date): # code correct # year = str(datetime.strptime(df_row.date, \"%d/%m/%Y\").date().year) # mais ne fonctionne pas sur des dates mal reconnues (OCR) ex: \"00/02/2022\" # alors qu'on peut extraire l'ann\u00e9e year = df_row.date.rsplit(\"/\", 1)[1] dest_dir = out_dir / \"pdf_analyses\" / commune / year else: dest_dir = out_dir / \"pdf_analyses/pdf_a_reclasser\" / commune else: dest_dir = out_dir / \"pdf_analyses/pdf_a_reclasser\" # cr\u00e9er le dossier destination si besoin dest_dir.mkdir(parents=True, exist_ok=True) # retrouver l'entr\u00e9e correspondance dans df_in, pour avoir le # chemin complet de sa copie (\u00e0 d\u00e9placer) et du fichier original # (\u00e0 supprimer) # .head(1) car normalement il y a *exactement une* entr\u00e9e correspondante # et .itertuples() pour avoir facilement un namedtuple for df_row_in in df_in.loc[df_in[\"pdf\"] == fn].head(1).itertuples(): # chemin du fichier trait\u00e9 (copi\u00e9 depuis dir_in vers le dossier de travail) fp = Path(df_row_in.fullpath) # chemin du fichier d'origine (pour suppression apr\u00e8s move) fp_orig = Path(df_row_in.origpath) # chemin destination du fichier trait\u00e9 print(fp.name) fp_dst = dest_dir / create_file_name_url(fp.name) print(fp_dst) print() shutil.move(fp, fp_dst) # si le move a r\u00e9ussi, on peut supprimer le fichier dans le dossier d'entr\u00e9e if fp_dst.is_file(): fp_orig.unlink() # chemin du fichier TXT (OCR sinon natif) fp_txt = Path(df_row_in.fullpath_txt) shutil.copy2(fp_txt, out_dir_txt / fp_txt.name) # faire une copie des 4 fichiers g\u00e9n\u00e9r\u00e9s avec les noms de base (\u00e9craser chaque fichier # pr\u00e9-existant ayant le nom de base) for fp_out in out_files.values(): # retirer la date et le num\u00e9ro d'ex\u00e9cution pour retrouver le nom de base fp_copy = ( out_dir / fp_out.with_stem(f\"{fp_out.stem.rsplit('_', maxsplit=2)[0]}\").name ) shutil.copy2(fp_out, fp_copy) return out_files Analyse le document dans son ensemble. Extrait des empans de texte correspondant aux en-t\u00eates, pieds-de-page, autorit\u00e9, vus, correspondants, articles, signature... has_one(spans, span_typ) D\u00e9tecte si la liste contient au moins un empan d'un type donn\u00e9. Si la liste est vide, renvoie None. Parameters spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9 Returns has_span: boolean True si au moins un empan de la liste est du type recherch\u00e9. src\\process\\parse_doc.py def has_one(spans: list[dict], span_typ: str) -> str: \"\"\"D\u00e9tecte si la liste contient au moins un empan d'un type donn\u00e9. Si la liste est vide, renvoie None. Parameters ---------- spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9 Returns ------- has_span: boolean True si au moins un empan de la liste est du type recherch\u00e9. \"\"\" if not spans: return None return any(x for x in spans if x[\"span_typ\"] == span_typ) parse_arrete_pages(fn_pdf, pages) Analyse les pages de texte d'un arr\u00eat\u00e9. Parameters fn_pdf: str Nom du fichier PDF. pages: list[str] Liste de pages de texte \u00e0 analyser. Returns doc_content: list[dict] Contenu du document, par page d\u00e9coup\u00e9e en zones de texte. src\\process\\parse_doc.py def parse_arrete_pages(fn_pdf: str, pages: list[str]) -> list: \"\"\"Analyse les pages de texte d'un arr\u00eat\u00e9. Parameters ---------- fn_pdf: str Nom du fichier PDF. pages: list[str] Liste de pages de texte \u00e0 analyser. Returns ------- doc_content: list[dict] Contenu du document, par page d\u00e9coup\u00e9e en zones de texte. \"\"\" doc_content = [] # valeur de retour # FIXME on ne traite pas une poign\u00e9e de documents qui posent diff\u00e9rents probl\u00e8mes if fn_pdf in EXCLUDE_SET: return doc_content # end FIXME # m\u00e9tadonn\u00e9es du document mdata_doc = { \"pdf\": fn_pdf, } # print(fn_pdf) # DEBUG # traiter les pages # TODO \u00e9tats alternatifs? [\"preambule\", \"vu\", \"considerant\", \"arrete\", \"article\", \"postambule\" ou \"signature\", \"apres_signature\" ou \"annexes\"] ? cur_state = \"avant_vucons\" # init ; \"avant_vucons\" < \"avant_articles\" < \"avant_signature\" # TODO ajouter \"avant_considerant\" ? latest_span = None # init for i, page in enumerate(pages, start=1): # m\u00e9tadonn\u00e9es de la page mdata_page = mdata_doc | {\"page_num\": i} if pd.isna(page): # * la page n'a pas de texte page_content = mdata_page | { \"template\": None, # empans de template \"body\": None, # texte (sans le texte du template) \"content\": None, # empans de contenu (paragraphes et donn\u00e9es): vide } doc_content.append(page_content) continue # NEW normalisation du texte # spaces=False sinon on perd les retours \u00e0 la ligne ! page = normalize_string(page, num=True, apos=True, hyph=True, spaces=False) # end NEW # rep\u00e9rer et effacer les \u00e9l\u00e9ments de template, pour ne garder que le contenu de chaque page pg_template, pg_txt_body = parse_page_template(page) pg_content = [] # initialisation de la liste des \u00e9l\u00e9ments de contenu # d\u00e9tecter et traiter sp\u00e9cifiquement les pages vides, de bordereau ou d'annexes if pg_txt_body.strip() == \"\": # * la page est vide de texte (hors template), donc aucun empan de contenu ne pourra \u00eatre reconnu page_content = mdata_page | { \"template\": pg_template, # empans de template \"body\": pg_txt_body, # texte (sans le texte du template) \"content\": pg_content, # empans de contenu (paragraphes et donn\u00e9es): vide } doc_content.append(page_content) continue elif P_BORDEREAU.search(pg_txt_body): # * page de bordereau de formalit\u00e9s (Aix-en-Provence) # TODO extraire le contenu (date de l'acte, num\u00e9ro, titre) pour v\u00e9rifier la correction des donn\u00e9es extraites ailleurs? page_content = mdata_page | { \"template\": pg_template, # empans de template \"body\": pg_txt_body, # texte (sans le texte du template) \"content\": pg_content, # empans de contenu (paragraphes et donn\u00e9es): vide } doc_content.append(page_content) continue # TODO pages d'annexe # TODO si la signature est d\u00e9j\u00e0 pass\u00e9e, on peut consid\u00e9rer que le document est termin\u00e9 et stopper tout le traitement? => ajouter un \u00e9tat cur_state == \"apres_signature\" ? # NB: certains fichiers PDF contiennent un arr\u00eat\u00e9 modificatif puis l'arr\u00eat\u00e9 d'origine (ex: \"modif 39 rue Tapis Vert 13001.pdf\"), on ignore le 2e ? # la page n'est pas vide de texte main_end = len(pg_txt_body) # 1. pr\u00e9ambule du document: avant le 1er \"Vu\", contient la commune, l'autorit\u00e9 prenant l'arr\u00eat\u00e9, parfois le num\u00e9ro de l'arr\u00eat\u00e9 if cur_state == \"avant_vucons\": fst_vucons = [] if fst_vu := P_VU.search(pg_txt_body): fst_vucons.append(fst_vu) if fst_cons := P_CONSIDERANT.search(pg_txt_body): fst_vucons.append(fst_cons) if fst_vucons: fst_vu_or_cons = sorted(fst_vucons, key=lambda x: x.start())[0] pream_beg = 0 pream_end = fst_vu_or_cons.start() pream_content = parse_doc_preamble( fn_pdf, pg_txt_body, pream_beg, pream_end ) pg_content.extend(pream_content) if pream_content: latest_span = None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible cur_state = \"avant_articles\" else: # la 1re page ne contient ni \"vu\" ni \"consid\u00e9rant\", ce doit \u00eatre une page de courrier # ex: \"21, rue Martinot Aubagne.pdf\" logging.warning( f\"{fn_pdf}: page {i}: ni 'vu' ni 'consid\u00e9rant' donc page ignor\u00e9e\" ) continue main_beg = pream_end else: # p. 2 et suivantes: la zone \u00e0 analyser commence en haut de la page (les \u00e9l\u00e9ments de # template ayant \u00e9t\u00e9 effac\u00e9s au pr\u00e9alable) main_beg = 0 # TODO si tout le texte a d\u00e9j\u00e0 \u00e9t\u00e9 reconnu, ajouter le contenu de la page au doc et passer \u00e0 la page suivante # 2. les \"vu\" et \"consid\u00e9rant\" if cur_state == \"avant_articles\": vucons_beg = main_beg # la page contient-elle un \"Article\" ? (le 1er) if m_article := P_ARTICLE.search(pg_txt_body, main_beg): # si oui, les \"Vu\" et \"Consid\u00e9rant\" de cette page, puis \"Arr\u00eate\", # sont \u00e0 chercher avant le 1er \"Article\" # print(f\"m_article={m_article}\") # DEBUG vucons_end = m_article.start() else: # si non, les \"Vu\" et \"Consid\u00e9rant\" sont sur toute la page vucons_end = main_end # rep\u00e9rer les \"Vu\" et \"Consid\u00e9rant\", et \"Arr\u00eate\" si pr\u00e9sent # print(f\"avant parse_page_content/Vucons: pg_content={pg_content}\") # DEBUG vucons_content = parse_page_content( pg_txt_body, vucons_beg, vucons_end, cur_state, latest_span ) # FIXME sp\u00e9cialiser la fonction pour restreindre aux \"Vu\" et \"Consid\u00e9rant\" et/ou passer cur_state? ; NB: ces deux types de paragraphes admettent des continuations pg_content.extend(vucons_content) # print(f\"apr\u00e8s parse_page_content/Vucons: pg_content={pg_content}\") # DEBUG if vucons_content: latest_span = ( None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible ) # si \"Arr\u00eate\" \u00e9tait bien sur la page, il faut ajouter l'empan reconnu, d\u00e9placer le curseur et changer d'\u00e9tat if pg_content: spans_arrete = [x for x in pg_content if x[\"span_typ\"] == \"par_arrete\"] if spans_arrete: assert len(spans_arrete) == 1 span_arrete = spans_arrete[0] # main_beg = span_arrete[\"span_end\"] latest_span = None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible cur_state = \"avant_signature\" # WIP 2023-05-09 else: logging.warning(f\"{fn_pdf} / {i}: parse_doc: pas de 'par_arrete'\") # end WIP 2023-05-09 # TODO si tout le texte a d\u00e9j\u00e0 \u00e9t\u00e9 reconnu, ajouter le contenu de la page au doc et passer \u00e0 la page suivante # TODO d\u00e9tecter la signature m\u00eame si \"Arr\u00eate\" n'a pas \u00e9t\u00e9 d\u00e9tect\u00e9 (simplification code + am\u00e9lioration robustesse?) # 3. les \"article\" et le postambule if cur_state == \"avant_signature\": # le corps du document s'arr\u00eate \u00e0 la signature ou la date de prise de l'arr\u00eat\u00e9 # FIXME attraper le 1er qui appara\u00eet: date de signature ou signataire artic_beg = main_beg if m_sign := P_DATE_SIGNAT.search(pg_txt_body, main_beg): # si la page contient la signature de fin de l'acte, l'analyse du contenu # principal doit s'arr\u00eater \u00e0 la signature (ici avec date) artic_end = m_sign.start() elif m_sign := P_LIEU_SIGNAT.search(pg_txt_body, main_beg): # si la page contient la signature de fin de l'acte, l'analyse du contenu # principal doit s'arr\u00eater \u00e0 la signature (ici avec lieu seul, date absente # ou non-reconnue) artic_end = m_sign.start() else: artic_end = main_end # rep\u00e9rer les articles # print(f\"avant parse_page_content/Articles: pg_content={pg_content}\") # DEBUG try: artic_content = parse_page_content( pg_txt_body, artic_beg, artic_end, cur_state, latest_span ) # FIXME sp\u00e9cialiser la fonction pour restreindre aux \"Vu\" et \"Consid\u00e9rant\" et/ou passer cur_state? ; NB: ces deux types de paragraphes admettent des continuations except TypeError: print(f\"Fichier fautif: {fn_pdf}, p. {i}\") raise pg_content.extend(artic_content) if artic_content: latest_span = ( None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible ) if m_sign: # analyser le postambule et changer l'\u00e9tat posta_beg = m_sign.start() posta_end = main_end posta_content = parse_doc_postamble(pg_txt_body, posta_beg, posta_end) pg_content.extend(posta_content) if posta_content: latest_span = None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible cur_state = \"apres_signature\" logging.warning(f\"{fn_pdf}: parse_doc: apr\u00e8s m_sign\") # DEBUG # TODO si tout le texte a d\u00e9j\u00e0 \u00e9t\u00e9 reconnu, ajouter le contenu de la page au doc et passer \u00e0 la page suivante if cur_state == \"apres_signature\": pass # FIXME faire quelque chose? v\u00e9rifier s'il reste du texte? # r\u00e9cup\u00e9rer le dernier paragraphe de la page, car il peut \u00eatre continu\u00e9 # en d\u00e9but de page suivante if pg_content: try: latest_span = [ x for x in pg_content if x[\"span_typ\"].startswith(\"par_\") ][-1] except IndexError: print( f\"{fn_pdf} / p.{i} : pas de paragraphe sur l'empan {main_beg}:{main_end}\\ncontenu:{pg_content}\\ntexte:\\n{pg_txt_body}\" ) raise # accumulation au niveau du document page_content = mdata_page | { \"template\": pg_template, # empans de template \"body\": pg_txt_body, # texte (sans le texte du template) \"content\": pg_content, # empans de contenu (paragraphes et donn\u00e9es) } doc_content.append(page_content) if False: # DEBUG print(\"<<<<<<<<<<<<<<<<\") print(pg_content) # DEBUG print(\"----------------\") print(pg_txt_body) # DEBUG print(\"~~~~~~~~~~~~~~~~\") print(pg_txt_body[main_beg:main_end]) # DEBUG print(\"================\") # TODO arr\u00eater le traitement \u00e0 la fin du postambule et tronquer le texte / le PDF si possible? (utile pour l'OCR) # v\u00e9rifier que le r\u00e9sultat est bien form\u00e9 examine_doc_content(fn_pdf, doc_content) # return doc_content parse_doc_postamble(txt_body, pream_beg, pream_end) Analyse le postambule d'un document, sur la derni\u00e8re page (hors annexes). Le postambule correspond \u00e0 la zone de signature: date, lieu \u00e9ventuel et signataire. Parameters txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\". Returns content: list Liste d'empans de contenu src\\process\\parse_doc.py def parse_doc_postamble(txt_body: str, pream_beg: int, pream_end: int) -> list[dict]: \"\"\"Analyse le postambule d'un document, sur la derni\u00e8re page (hors annexes). Le postambule correspond \u00e0 la zone de signature: date, lieu \u00e9ventuel et signataire. Parameters ---------- txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\". Returns ------- content: list Liste d'empans de contenu \"\"\" content = [] # a. extraire la date de signature if m_signature := P_DATE_SIGNAT.search(txt_body, pream_beg, pream_end): logging.warning(f\"parse_doc_postamble: signature: {m_signature}\") # stocker la zone reconnue content.append( { \"span_beg\": m_signature.start(), \"span_end\": m_signature.end(), \"span_txt\": m_signature.group(0), \"span_typ\": \"par_sign_date\", } ) # stocker la donn\u00e9e content.append( { \"span_beg\": m_signature.start(\"arr_date\"), \"span_end\": m_signature.end(\"arr_date\"), \"span_txt\": m_signature.group(\"arr_date\"), \"span_typ\": \"arr_date\", } ) # b. extraire la ville de signature if m_signature.group(\"arr_ville_signat\"): # stocker la donn\u00e9e content.append( { \"span_beg\": m_signature.start(\"arr_ville_signat\"), \"span_end\": m_signature.end(\"arr_ville_signat\"), \"span_txt\": m_signature.group(\"arr_ville_signat\"), \"span_typ\": \"adr_ville\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) elif m_signature := P_LIEU_SIGNAT.search(txt_body, pream_beg, pream_end): logging.warning(f\"parse_doc_postamble: signature (lieu): {m_signature}\") # stocker la zone reconnue content.append( { \"span_beg\": m_signature.start(), \"span_end\": m_signature.end(), \"span_txt\": m_signature.group(0), \"span_typ\": \"par_sign_lieu\", } ) # b. extraire la ville de signature # TODO ne capture pas toutes les villes (probl\u00e8me de named group avec contexte trop diff\u00e9rent) if m_signature.group(\"arr_ville_signat\"): # stocker la donn\u00e9e content.append( { \"span_beg\": m_signature.start(\"arr_ville_signat\"), \"span_end\": m_signature.end(\"arr_ville_signat\"), \"span_txt\": m_signature.group(\"arr_ville_signat\"), \"span_typ\": \"adr_ville\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) # TODO c. extraire l'identit\u00e9 et la qualit\u00e9 du signataire? (eg. d\u00e9l\u00e9gation de signature) # else: logging.warning( f\"parse_doc_postamble: aucune signature ? {txt_body[pream_beg:pream_end]}\" ) return content parse_doc_preamble(fn_pdf, txt_body, pream_beg, pream_end) Analyse le pr\u00e9ambule d'un document, sur la 1re page, avant le 1er \"Vu\". Parameters fn_pdf: string Nom du fichier PDF txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\". Returns content: list Liste d'empans de contenu src\\process\\parse_doc.py def parse_doc_preamble( fn_pdf: str, txt_body: str, pream_beg: int, pream_end: int ) -> list[dict]: \"\"\"Analyse le pr\u00e9ambule d'un document, sur la 1re page, avant le 1er \"Vu\". Parameters ---------- fn_pdf: string Nom du fichier PDF txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\". Returns ------- content: list Liste d'empans de contenu \"\"\" content = [] rem_txt = \"\" # s'il reste du texte apr\u00e8s l'autorit\u00e9 (WIP) # cr\u00e9er une copie du texte du pr\u00e9ambule, de m\u00eame longueur que le texte complet pour que les empans soient bien positionn\u00e9s # le texte sera effac\u00e9 au fur et \u00e0 mesure qu'il sera \"consomm\u00e9\" txt_copy = txt_body[:] txt_copy = ( \" \" * (pream_beg - 0) + txt_copy[pream_beg:pream_end] + \" \" * (len(txt_copy) - pream_end) ) assert len(txt_copy) == len(txt_body) # a. ce pr\u00e9ambule contient (vers la fin) l'intitul\u00e9 de l'autorit\u00e9 prenant l'arr\u00eat\u00e9 # TODO est-ce obligatoire? exceptions: La Ciotat if matches := list(P_MAIRE_COMMUNE.finditer(txt_copy, pream_beg, pream_end)): # on garde la premi\u00e8re occurrence, normalement la seule match = matches[0] # * toute la zone reconnue span_beg, span_end = match.span() content.append( { \"span_beg\": span_beg, \"span_end\": span_end, \"span_txt\": match.group(0), \"span_typ\": \"par_autorite\", } ) if match.group(\"commune\"): # * stocker la donn\u00e9e de la commune content.append( { \"span_beg\": match.start(\"commune\"), \"span_end\": match.end(\"commune\"), \"span_txt\": match.group(\"commune\"), \"span_typ\": \"adr_ville\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) # * effacer l'empan reconnu txt_copy = ( txt_copy[:span_beg] + \" \" * (span_end - span_beg) + txt_copy[span_end:] ) # la ou les \u00e9ventuelles autres occurrences sont des doublons if len(matches) > 1: logging.warning( f\"{fn_pdf}: > 1 mention d'autorit\u00e9 trouv\u00e9e dans le pr\u00e9ambule: {matches}\" ) for match_dup in matches[1:]: # toute la zone reconnue span_dup_beg, span_dup_end = match_dup.span() content.append( { \"span_beg\": span_dup_beg, \"span_end\": span_dup_end, \"span_txt\": match_dup.group(0), \"span_typ\": \"par_autorite_dup\", } ) if match.group(\"commune\"): # stocker la donn\u00e9e de la commune content.append( { \"span_beg\": match_dup.start(\"commune\"), \"span_end\": match_dup.end(\"commune\"), \"span_txt\": match_dup.group(\"commune\"), \"span_typ\": \"adr_ville_dup\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) # effacer l'empan reconnu txt_copy = ( txt_copy[:span_dup_beg] + \" \" * (span_dup_end - span_dup_beg) + txt_copy[span_dup_end:] ) # v\u00e9rifier que la zone de l'autorit\u00e9 est bien en fin de pr\u00e9ambule try: rem_txt = txt_copy[span_end:pream_end].strip() assert rem_txt == \"\" except AssertionError: logging.warning( f\"{fn_pdf}: Texte apr\u00e8s l'autorit\u00e9, en fin de pr\u00e9ambule: {rem_txt}\" ) if len(rem_txt) < 2: # s'il ne reste qu'un caract\u00e8re, c'est probablement une typo => avertir et effacer logging.warning( f\"{fn_pdf}: Ignorer le fragment de texte en fin de pr\u00e9ambule, probablement une typo: {rem_txt}\" ) txt_copy = ( txt_copy[:span_end] + \" \" * (pream_end - span_end) + txt_copy[pream_end:] ) else: # pas d'autorit\u00e9 d\u00e9tect\u00e9e: anormal logging.warning(f\"{fn_pdf}: pas d'autorit\u00e9 d\u00e9tect\u00e9e dans le pr\u00e9ambule\") # b. ce pr\u00e9ambule peut contenir le num\u00e9ro de l'arr\u00eat\u00e9 (si pr\u00e9sent, absent dans certaines communes) # NB: ce num\u00e9ro d'arr\u00eat\u00e9 peut se trouver avant ou apr\u00e8s l'autorit\u00e9 (ex: Gardanne) match = P_NUM_ARR.search(txt_copy, pream_beg, pream_end) if match is None: # si la capture pr\u00e9cise \u00e9choue, utiliser une capture plus permissive (mais risque d'attrape-tout) match = P_NUM_ARR_FALLBACK.search(txt_copy, pream_beg, pream_end) if match is not None: # marquer toute la zone reconnue (contexte + num\u00e9ro de l'arr\u00eat\u00e9) span_beg, span_end = match.span() content.append( { \"span_beg\": span_beg, \"span_end\": span_end, \"span_txt\": match.group(0), \"span_typ\": \"par_num_arr\", # paragraphe contenant le num\u00e9ro de l'arr\u00eat\u00e9 } ) # stocker le num\u00e9ro de l'arr\u00eat\u00e9 content.append( { \"span_beg\": match.start(\"num_arr\"), \"span_end\": match.end(\"num_arr\"), \"span_txt\": match.group(\"num_arr\"), \"span_typ\": \"num_arr\", } ) # effacer le texte reconnu txt_copy = ( txt_copy[:span_beg] + \" \" * (span_end - span_beg) + txt_copy[span_end:] ) # print(f\"num arr: {content[-1]['span_txt']}\") # DEBUG else: # pas de num\u00e9ro d'arr\u00eat\u00e9 (ex: Aubagne) logging.warning( f\"{fn_pdf}: Pas de num\u00e9ro d'arr\u00eat\u00e9 trouv\u00e9: \" + '\"' + txt_copy[pream_beg:pream_end].replace(\"\\n\", \" \").strip() + '\"' ) pass # c. entre les deux doit se trouver le titre ou objet de l'arr\u00eat\u00e9 (obligatoire) if match := P_NOM_ARR.search(txt_copy, pream_beg, pream_end): span_beg, span_end = match.span() # stocker la zone reconnue content.append( { \"span_beg\": span_beg, \"span_end\": span_end, \"span_txt\": match.group(0), \"span_typ\": \"par_nom_arr\", } ) # stocker la donn\u00e9e content.append( { \"span_beg\": match.start(\"nom_arr\"), \"span_end\": match.end(\"nom_arr\"), \"span_txt\": match.group(\"nom_arr\"), \"span_typ\": \"nom_arr\", } ) # effacer l'empan reconnu txt_copy = ( txt_copy[:span_beg] + \" \" * (span_end - span_beg) + txt_copy[span_end:] ) else: # hypoth\u00e8se: sans marquage explicite comme \"Objet:\", le titre est tout le texte restant # dans cette zone (entre le num\u00e9ro et l'autorit\u00e9) if (not P_LINE.fullmatch(txt_copy, pream_beg, pream_end)) and ( match := P_STRIP.fullmatch(txt_copy, pream_beg, pream_end) ): # stocker la zone reconnue content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": \"par_nom_arr\", } ) # stocker la donn\u00e9e content.append( { \"span_beg\": match.start(\"outstrip\"), \"span_end\": match.end(\"outstrip\"), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": \"nom_arr\", } ) else: logging.warning( f\"{fn_pdf}: Pas de texte restant pour le nom de l'arr\u00eat\u00e9: \" + '\"' + txt_copy[pream_beg:pream_end].replace(\"\\n\", \" \").strip() + '\"' ) # WIP if rem_txt and content[-1][\"span_typ\"] == \"nom_arr\": arr_nom = content[-1][\"span_txt\"].replace(\"\\n\", \" \") logging.warning(f\"{fn_pdf}: texte restant et nom: {arr_nom}\") # end WIP # print(content) # WIP # TODO remplacer les zones reconnues par des espaces, et afficher le texte non-captur\u00e9? return content parse_page_content(txt_body, main_beg, main_end, cur_state, latest_span) Analyse une page pour rep\u00e9rer les zones de contenus. Parameters txt_body: string Corps de texte de la page \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser. cur_state: str \u00c9tat actuel: \"avant_articles\", \"avant_signature\", \"apres_signature\" latest_span: dict, optional Dernier empan de contenu rep\u00e9r\u00e9 sur la page pr\u00e9c\u00e9dente. Vaut None pour la premi\u00e8re page. Returns content: list Liste d'empans de contenu src\\process\\parse_doc.py def parse_page_content( txt_body: str, main_beg: int, main_end: int, cur_state: str, latest_span: Optional[dict], ) -> list: \"\"\"Analyse une page pour rep\u00e9rer les zones de contenus. Parameters ---------- txt_body: string Corps de texte de la page \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser. cur_state: str \u00c9tat actuel: \"avant_articles\", \"avant_signature\", \"apres_signature\" latest_span: dict, optional Dernier empan de contenu rep\u00e9r\u00e9 sur la page pr\u00e9c\u00e9dente. Vaut `None` pour la premi\u00e8re page. Returns ------- content: list Liste d'empans de contenu \"\"\" # print(f\"parse_page_content: {(main_beg, main_end, cur_state, latest_span)}\") # DEBUG if cur_state not in (\"avant_articles\", \"avant_signature\"): raise ValueError( f\"\u00c9tat inattendu {cur_state}\\n{main_beg}:{main_end}\\n{txt_body[main_beg:main_end]}\" ) if txt_body[main_beg:main_end].strip() == \"\": # la zone de texte \u00e0 analyser est vide return [] content = [] # rep\u00e9rer les d\u00e9buts de paragraphes: \"Vu\", \"Consid\u00e9rant\", \"Arr\u00eate\", \"Article\" if cur_state == \"avant_articles\": # \"Vu\" et \"Consid\u00e9rant\" par_begs = sorted( [(m.start(), \"par_vu\") for m in P_VU.finditer(txt_body, main_beg, main_end)] + [ (m.start(), \"par_considerant\") for m in P_CONSIDERANT.finditer(txt_body, main_beg, main_end) ] ) # \u00e9ventuellement, \"Arr\u00eatons|Arr\u00eate|Arr\u00eat\u00e9\" entre les \"Vu\" \"Consid\u00e9rant\" d'une part, # les \"Article\" d'autre part # si la page en cours contient le dernier \"Vu\" ou \"Consid\u00e9rant\" du document, ou # la fin de ce dernier \"Vu\" ou \"Consid\u00e9rant\", \"Arr\u00eatons\" doit \u00eatre l\u00e0 if par_begs: # il y a au moins un \"Vu\" ou \"Consid\u00e9rant\" sur la page: # le dernier de la page est-il aussi le dernier du document? searchzone_beg = par_begs[-1][0] else: # le dernier \"Vu\" ou \"Consid\u00e9rant\" peut avoir commenc\u00e9 sur la page pr\u00e9c\u00e9dente, # auquel cas on cherche \"Arr\u00eatons|Arr\u00eate|Arr\u00eat\u00e9\" sur toute la zone en cours # d'analyse searchzone_beg = main_beg # print(f\"Cherche ARRETE dans:\\n{txt_body[searchzone_beg:main_end]}\") # DEBUG if m_arretons := P_ARRETONS.search(txt_body, searchzone_beg, main_end): par_begs.append((m_arretons.start(), \"par_arrete\")) elif cur_state == \"avant_signature\": par_begs = [ (m.start(), \"par_article\") for m in P_ARTICLE.finditer(txt_body, main_beg, main_end) ] else: raise ValueError(f\"cur_state: {cur_state}?\") # 1. traiter le texte avant le 1er d\u00e9but de paragraphe if not par_begs: # aucun d\u00e9but de paragraphe d\u00e9tect\u00e9 sur la page ; cela peut arriver lorsqu'un empan # court sur plusieurs pages, eg. un \"Consid\u00e9rant\" tr\u00e8s long incluant la liste des # copropri\u00e9taires # # ce n'est possible que s'il y a bien un latest_span, et d'un type admettant une continuation if (latest_span is None) or ( latest_span[\"span_typ\"] not in ( \"par_vu\", \"par_considerant\", \"par_article\", # une continuation peut \u00eatre elle-m\u00eame continu\u00e9e \"par_vu_suite\", \"par_considerant_suite\", \"par_article_suite\", ) ): raise ValueError( f\"Aucun paragraphe continuable sur cette page sans nouveau paragraphe?\\ncur_state={cur_state}, latest_span={latest_span}, (main_beg, main_end)=({main_beg}, {main_end})\\n{txt_body[main_beg:main_end]}\" ) # analyser jusqu'en bas de la page, sans visibilit\u00e9 sur le type du prochain empan (absent de la page) nxt_beg = main_end nxt_typ = None else: # s'il y a du texte avant le 1er d\u00e9but de paragraphe, c'est la continuation du # dernier paragraphe de la page pr\u00e9c\u00e9dente ; # le mettre dans un empan de type span_typ + \"_suite\" nxt_beg, nxt_typ = par_begs[0] # r\u00e9cup\u00e9rer ce texte et le mettre dans un empan sp\u00e9cial _suite if (not P_LINE.fullmatch(txt_body, main_beg, nxt_beg)) and ( match := P_STRIP.fullmatch(txt_body, main_beg, nxt_beg) ): txt_dang = match.group(\"outstrip\") if txt_dang: # print(f\"txt_dang: {txt_dang}\") # DEBUG try: lst_typ = latest_span[\"span_typ\"] except TypeError: print( f\"cur_state={cur_state}\\npar_begs={par_begs}\\n(main_beg, nxt_beg)=({main_beg}, {nxt_beg})\\n{txt_body[main_beg:nxt_beg]}\" ) raise # un empan peut courir sur plus d'une page compl\u00e8te (ex: \"Consid\u00e9rant\" tr\u00e8s long, incluant la liste des copropri\u00e9taires) cur_typ = lst_typ if lst_typ.endswith(\"_suite\") else lst_typ + \"_suite\" # stocker la zone reconnue content.append( { \"span_beg\": match.start(\"outstrip\"), \"span_end\": match.end(\"outstrip\"), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": cur_typ, } ) if nxt_typ is not None: # v\u00e9rifier que la transition autoris\u00e9e est correcte # TODO d\u00e9placer cette v\u00e9rification en amont ou en aval? try: assert (cur_typ, nxt_typ) in ( # les \"Vu\" sont g\u00e9n\u00e9ralement avant les \"Consid\u00e9rant\" mais certains arr\u00eat\u00e9s m\u00ealent les deux types de paragraphes (\"par_vu_suite\", \"par_vu\"), (\"par_vu_suite\", \"par_considerant\"), (\"par_vu_suite\", \"par_arrete\"), # NEW 2023-03-23 (\"par_considerant_suite\", \"par_vu\"), # NEW 2023-03-23 (\"par_considerant_suite\", \"par_considerant\"), (\"par_considerant_suite\", \"par_arrete\"), # (\"par_arrete_suite\", \"par_article\"), # \"Arr\u00eate\" ne peut pas \u00eatre coup\u00e9 par un saut de page car il est toujours sur une seule ligne # les articles forment un bloc homog\u00e8ne, sans retour vers des \"Vu\" ou \"Consid\u00e9rant\" (s'il y en a, ce sont des citations de passage dans un article...) (\"par_article_suite\", \"par_article\"), ) except AssertionError: print( f\"Transition inattendue: ({cur_typ}, {nxt_typ})\\n{latest_span}\\n{txt_body}\" ) raise # 2. pour chaque d\u00e9but de paragraphe, cr\u00e9er un empan allant jusqu'au prochain d\u00e9but for (cur_beg, cur_typ), (nxt_beg, nxt_typ) in zip(par_begs[:-1], par_begs[1:]): # extraire le texte hors espaces de d\u00e9but et fin if (not P_LINE.fullmatch(txt_body, cur_beg, nxt_beg)) and ( match := P_STRIP.fullmatch(txt_body, cur_beg, nxt_beg) ): # stocker la zone reconnue content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": cur_typ, } ) # v\u00e9rifier que la transition autoris\u00e9e est correcte # TODO d\u00e9placer cette v\u00e9rification en amont ou en aval? try: assert (cur_typ, nxt_typ) in ( (\"par_vu\", \"par_vu\"), (\"par_vu\", \"par_considerant\"), # (consid\u00e9rant, vu): transition rare mais qui arrive (\"par_considerant\", \"par_vu\"), # (vu, arr\u00eate): transition rare mais qui arrive (ex: abrogation d'arr\u00eat\u00e9 dont la raison est donn\u00e9e dans un autre arr\u00eat\u00e9...) (\"par_vu\", \"par_arrete\"), (\"par_considerant\", \"par_considerant\"), (\"par_considerant\", \"par_arrete\"), (\"par_arrete\", \"par_article\"), (\"par_article\", \"par_article\"), ) except AssertionError: print(f\"Transition impr\u00e9vue: ({cur_typ, nxt_typ})\\n{content}\") raise # 3. pour le dernier d\u00e9but de paragraphe, cr\u00e9er un empan allant jusqu'\u00e0 la fin du texte if par_begs: cur_beg, cur_typ = par_begs[-1] nxt_beg = main_end nxt_typ = None # extraire le texte hors espaces de d\u00e9but et fin if (not P_LINE.fullmatch(txt_body, cur_beg, nxt_beg)) and ( match := P_STRIP.fullmatch(txt_body, cur_beg, nxt_beg) ): # stocker la zone reconnue content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": cur_typ, } ) # on ne peut pas v\u00e9rifier si la transition autoris\u00e9e est correcte puisque # le prochain empan n'est pas connu (page suivante) ; cette v\u00e9rification # sera faite de toute fa\u00e7on lors du traitement du haut de la prochaine page # rep\u00e9rer, dans chaque paragraphe, les r\u00e9f\u00e9rences au cadre r\u00e9glementaire # TODO seulement pour les \"vu\"? ou utile pour les autres? # TODO certaines r\u00e9f\u00e9rences peuvent-elles \u00eatre coup\u00e9es par des sauts de page ? => concat\u00e9ner latest_span[\"span_txt\"] et content[0][\"span_txt\"] ? content_reg = [] for par in content: par_reg = parse_refs_reglement(txt_body, par[\"span_beg\"], par[\"span_end\"]) content_reg.extend(par_reg) content.extend(content_reg) return content parse_page_template(txt) Analyse une page pour rep\u00e9rer le template. Rep\u00e8re les en-t\u00eates, pieds-de-page, tampons, et renvoie les empans correspondants, ainsi que le texte d\u00e9barrass\u00e9 de ces \u00e9l\u00e9ments de template. Parameters txt: str Texte d'origine de la page. Returns content: list Liste d'empans rep\u00e9r\u00e9s sur la page. txt_body: string Corps de texte, d\u00e9fini comme le texte en entr\u00e9e dans lequel les empans d'en-t\u00eates, pieds-de-page et tampons de content ont \u00e9t\u00e9 effac\u00e9s (remplac\u00e9s par des espaces de m\u00eame longueur). src\\process\\parse_doc.py def parse_page_template(txt: str) -> tuple[list, str]: \"\"\"Analyse une page pour rep\u00e9rer le template. Rep\u00e8re les en-t\u00eates, pieds-de-page, tampons, et renvoie les empans correspondants, ainsi que le texte d\u00e9barrass\u00e9 de ces \u00e9l\u00e9ments de template. Parameters ---------- txt: str Texte d'origine de la page. Returns ------- content: list Liste d'empans rep\u00e9r\u00e9s sur la page. txt_body: string Corps de texte, d\u00e9fini comme le texte en entr\u00e9e dans lequel les empans d'en-t\u00eates, pieds-de-page et tampons de `content` ont \u00e9t\u00e9 effac\u00e9s (remplac\u00e9s par des espaces de m\u00eame longueur). \"\"\" content = [] # en-t\u00eate # TODO expectation: n=0..2 par page if m_headers := P_HEADER.finditer(txt): for match in m_headers: content.append( { \"span_beg\": match.span()[0], \"span_end\": match.span()[1], \"span_txt\": match.group(0), \"span_typ\": \"header\", } ) # print(f\"<<<<< template:headers={content}\") # DEBUG # pied-de-page # TODO expectation: n=0..2 par page if m_footers := P_FOOTER.finditer(txt): for match in m_footers: content.append( { \"span_beg\": match.span()[0], \"span_end\": match.span()[1], \"span_txt\": match.group(0), \"span_typ\": \"footer\", } ) # tampon de transmission \u00e0 actes if m_stamps := P_STAMP.finditer(txt): for match in m_stamps: m_beg, m_end = match.span() content.append( { \"span_beg\": m_beg, \"span_end\": m_end, \"span_txt\": match.group(0), \"span_typ\": \"stamp\", } ) # corps du texte # d\u00e9fini comme le texte d'origine, dans lequel on a effac\u00e9 les empans rep\u00e9r\u00e9s # (en-t\u00eates, pieds-de-page, tampons) ; # remplacer les empans par des espaces permet de conserver les indices d'origine # et \u00e9viter les d\u00e9calages spans = list((x[\"span_beg\"], x[\"span_end\"]) for x in content) txt_body = txt[:] for sp_beg, sp_end in spans: txt_body = txt_body[:sp_beg] + \" \" * (sp_end - sp_beg) + txt_body[sp_end:] return content, txt_body process_files(df_meta, df_txts) Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. src\\process\\parse_doc.py def process_files( df_meta: pd.DataFrame, df_txts: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters ---------- df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns ------- df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. \"\"\" indics_struct = [] for _, df_doc_pages in df_txts.groupby(\"fullpath\"): # RESUME HERE ~exclude # m\u00e9ta \u00e0 passer en fin df_doc_meta = df_doc_pages[[\"pdf\", \"fullpath\", \"pagenum\"]] # fn_pdf = df_doc_pages[\"pdf\"].iat[0] pages = df_doc_pages[\"pagetxt\"].values exclude = df_doc_pages[\"exclude\"].values # actes try: has_stamp_pages = [ (P_STAMP.search(x) is not None) if pd.notna(x) else None for x in pages ] except TypeError: print(repr(pages)) raise # rep\u00e9rer les pages d'accus\u00e9 de r\u00e9ception d'actes, elles seront marqu\u00e9es et non pass\u00e9es au parser # TODO v\u00e9rifier si la page d'AR actes appara\u00eet seulement en derni\u00e8re page, sinon on peut couper le doc et passer moins de pages au parser # TODO timer et r\u00e9\u00e9crire les deux instructions en pandas[pyarrow] pour am\u00e9liorer la vitesse? is_ar_pages = [ (P_ACCUSE.match(x) is not None) if pd.notna(x) else None for x in pages ] # filtrer les pages filt_pages = [] for x, excl, is_ar in zip(pages, exclude, is_ar_pages): if excl: # flag d'exclusion de la page filt_p = None elif is_ar: # page d'accus\u00e9 de r\u00e9ception de t\u00e9l\u00e9transmission actes filt_p = \"\" else: filt_p = x filt_pages.append(filt_p) # analyser les pages doc_content = parse_arrete_pages(fn_pdf, filt_pages) # filtrer les empans de donn\u00e9es, et laisser de c\u00f4t\u00e9 les empans de structure for page_cont, has_st, is_ar, page_meta in zip( doc_content, has_stamp_pages, is_ar_pages, df_doc_meta.itertuples() ): pg_content = page_cont[\"content\"] pg_txt_body = page_cont[\"body\"] # donn\u00e9es if pg_txt_body: # adresse(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 if pg_adrs_doc := get_adr_doc(pg_txt_body): # on s\u00e9lectionne arbitrairement la 1re zone d'adresse(s) (FIXME?) pg_adr_doc = pg_adrs_doc[0][\"adresse_brute\"] # temporairement: on prend la 1re adresse pr\u00e9cise extraite de cette zone adr_fields = pg_adrs_doc[0][\"adresses\"][0] # end WIP else: pg_adr_doc = None adr_fields = { \"adr_num\": None, # num\u00e9ro de la voie \"adr_ind\": None, # indice de r\u00e9p\u00e9tition \"adr_voie\": None, # nom de la voie \"adr_compl\": None, # compl\u00e9ment d'adresse \"adr_cpostal\": None, # code postal \"adr_ville\": None, # ville } # parcelle(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 if pg_parcelle := get_parcelles(pg_txt_body): pg_parcelle = pg_parcelle[0] # get_parcelles:list[str] else: pg_parcelle = None else: pg_adr_doc = None adr_fields = { \"adr_num\": None, # num\u00e9ro de la voie \"adr_ind\": None, # indice de r\u00e9p\u00e9tition \"adr_voie\": None, # nom de la voie \"adr_compl\": None, # compl\u00e9ment d'adresse \"adr_cpostal\": None, # code postal \"adr_ville\": None, # ville } pg_parcelle = None # rassembler les donn\u00e9es dans un dict rec_struct = { # @ctes \"has_stamp\": has_st, \"is_accusedereception_page\": is_ar, # tous arr\u00eat\u00e9s \"commune_maire\": unique_txt(pg_content, \"adr_ville\"), \"has_vu\": has_one(pg_content, \"par_vu\"), \"has_considerant\": has_one(pg_content, \"par_considerant\"), \"has_arrete\": has_one(pg_content, \"par_arrete\"), \"has_article\": has_one(pg_content, \"par_article\"), # arr\u00eat\u00e9s sp\u00e9cifiques # - r\u00e9glementaires \"has_cgct\": contains_cgct(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cgct_art\": contains_cgct_art(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch\": contains_cch(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L111\": contains_cch_L111(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L511\": contains_cch_L511(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L521\": contains_cch_L521(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L541\": contains_cch_L541(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_R511\": contains_cch_R511(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cc\": contains_cc(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cc_art\": contains_cc_art(pg_txt_body) if pg_txt_body is not None else None, # TODO # - donn\u00e9es \"adresse\": pg_adr_doc, # TODO urgent # refactor 2023-03-31: remonter l'extraction de l'adresse pr\u00e9cise \"adr_num\": adr_fields[\"adr_num\"], # num\u00e9ro de la voie \"adr_ind\": adr_fields[\"adr_ind\"], # indice de r\u00e9p\u00e9tition \"adr_voie\": adr_fields[\"adr_voie\"], # nom de la voie \"adr_compl\": adr_fields[\"adr_compl\"], # compl\u00e9ment d'adresse \"adr_cpostal\": adr_fields[\"adr_cpostal\"], # code postal \"adr_ville\": adr_fields[\"adr_ville\"], # ville # end refactor 2023-03-31 \"parcelle\": pg_parcelle, # TODO urgent \"proprio\": get_proprio(pg_txt_body) if pg_txt_body is not None else None, # WIP \"syndic\": get_syndic(pg_txt_body) if pg_txt_body is not None else None, # TODO urgent- \"gest\": get_gest(pg_txt_body) if pg_txt_body is not None else None, # TODO urgent- \"date\": unique_txt(pg_content, \"arr_date\"), # * arr\u00eat\u00e9 \"num_arr\": unique_txt(pg_content, \"num_arr\"), \"nom_arr\": unique_txt(pg_content, \"nom_arr\"), \"classe\": get_classe(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"urgence\": get_urgence(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"demo\": get_demo(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"int_hab\": get_int_hab(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"equ_com\": get_equ_com(pg_txt_body) if pg_txt_body is not None else None, # TODO improve } indics_struct.append( { \"pdf\": page_meta.pdf, \"fullpath\": page_meta.fullpath, \"pagenum\": page_meta.pagenum, } | rec_struct # python >= 3.9 (dict union) ) df_indics = pd.DataFrame.from_records(indics_struct) df_proc = pd.merge(df_meta, df_indics, on=[\"pdf\", \"fullpath\"]) df_proc = df_proc.astype(dtype=DTYPE_META_NTXT_PROC) return df_proc unique_txt(spans, span_typ) Cherche l'unique empan d'un type donn\u00e9 et renvoie son texte. Si plusieurs empans de la liste en entr\u00e9e sont du type recherch\u00e9, une exception est lev\u00e9e. Si aucun empan de la liste n'est du type recherch\u00e9, renvoie None. Parameters spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9 Returns span_txt: str Texte de l'unique empan du type recherch\u00e9. src\\process\\parse_doc.py def unique_txt(spans: list[dict], span_typ: str) -> str: \"\"\"Cherche l'unique empan d'un type donn\u00e9 et renvoie son texte. Si plusieurs empans de la liste en entr\u00e9e sont du type recherch\u00e9, une exception est lev\u00e9e. Si aucun empan de la liste n'est du type recherch\u00e9, renvoie None. Parameters ---------- spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9 Returns ------- span_txt: str Texte de l'unique empan du type recherch\u00e9. \"\"\" if not spans: return None cands = [x for x in spans if x[\"span_typ\"] == span_typ] if not cands: return None elif len(cands) > 1: raise ValueError(f\"Plusieurs empans de type {span_typ}: {cands}\") else: return cands[0][\"span_txt\"] Extrait la structure des documents. D\u00e9coupe chaque arr\u00eat\u00e9 en zones: * pr\u00e9ambule (?), * VUs, * CONSIDERANTs, * ARTICLES, * postambule (?) process_files(df_meta, df_txts) Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. src\\process\\parse_native_pages.py def process_files( df_meta: pd.DataFrame, df_txts: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters ---------- df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns ------- df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. \"\"\" indics_struct = [] for df_row in df_txts.itertuples(): # pour chaque page de document, rep\u00e9rer des indications de structure rec_struct = spot_text_structure(df_row) indics_struct.append( { \"pdf\": df_row.pdf, \"fullpath\": df_row.fullpath, \"pagenum\": df_row.pagenum, } | rec_struct # python >= 3.9 (dict union) ) df_indics = pd.DataFrame.from_records(indics_struct) df_proc = pd.merge(df_meta, df_indics, on=[\"pdf\", \"fullpath\"]) df_proc = df_proc.astype(dtype=DTYPE_META_NTXT_PROC) return df_proc spot_text_structure(df_row) D\u00e9tecte la pr\u00e9sence d'\u00e9l\u00e9ments de structure dans une page d'arr\u00eat\u00e9. D\u00e9tecte la pr\u00e9sence de tampons, pages d'accus\u00e9 de r\u00e9ception, VU, CONSIDERANT, ARTICLE. Parameters df_row: NamedTuple Page de document Returns rec_struct: dict Dictionnaire de valeurs bool\u00e9ennes ou nulles, selon que les \u00e9l\u00e9ments de structure ont \u00e9t\u00e9 d\u00e9tect\u00e9s. Les cl\u00e9s et les types de valeurs sont sp\u00e9cifi\u00e9s dans DTYPE_PARSE . Si df_row ne contient pas de texte, toutes les valeurs de sortie sont None. src\\process\\parse_native_pages.py def spot_text_structure( df_row: NamedTuple, ) -> pd.DataFrame: \"\"\"D\u00e9tecte la pr\u00e9sence d'\u00e9l\u00e9ments de structure dans une page d'arr\u00eat\u00e9. D\u00e9tecte la pr\u00e9sence de tampons, pages d'accus\u00e9 de r\u00e9ception, VU, CONSIDERANT, ARTICLE. Parameters ---------- df_row: NamedTuple Page de document Returns ------- rec_struct: dict Dictionnaire de valeurs bool\u00e9ennes ou nulles, selon que les \u00e9l\u00e9ments de structure ont \u00e9t\u00e9 d\u00e9tect\u00e9s. Les cl\u00e9s et les types de valeurs sont sp\u00e9cifi\u00e9s dans `DTYPE_PARSE`. Si df_row ne contient pas de texte, toutes les valeurs de sortie sont None. \"\"\" if pd.notna(df_row.pagetxt) and ( not df_row.exclude ): # WIP \" and (not df_row.exclude)\" logging.warning(f\"{df_row.pdf} / {df_row.pagenum}\") # WIP # adresse(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 if pg_adrs_doc := get_adr_doc(df_row.pagetxt): # on s\u00e9lectionne arbitrairement la 1re zone d'adresse(s) (FIXME?) pg_adr_doc = pg_adrs_doc[0][\"adresse_brute\"] # temporairement: on prend la 1re adresse pr\u00e9cise extraite de cette zone adr_fields = pg_adrs_doc[0][\"adresses\"][0] # end WIP else: pg_adr_doc = None adr_fields = { \"adr_num\": None, # num\u00e9ro de la voie \"adr_ind\": None, # indice de r\u00e9p\u00e9tition \"adr_voie\": None, # nom de la voie \"adr_compl\": None, # compl\u00e9ment d'adresse \"adr_cpostal\": None, # code postal \"adr_ville\": None, # ville } # parcelle(s) vis\u00e9es par l'arr\u00eat\u00e9 parcelles = get_parcelles(df_row.pagetxt) # rec_struct = { # @ctes \"has_stamp\": is_stamped_page(df_row.pagetxt), \"is_accusedereception_page\": is_accusedereception_page(df_row.pagetxt), # tous arr\u00eat\u00e9s \"commune_maire\": get_commune_maire(df_row.pagetxt), \"has_vu\": contains_vu(df_row.pagetxt), \"has_considerant\": contains_considerant(df_row.pagetxt), \"has_arrete\": contains_arrete(df_row.pagetxt), \"has_article\": contains_article(df_row.pagetxt), # arr\u00eat\u00e9s sp\u00e9cifiques # - r\u00e9glementaires \"has_cgct\": contains_cgct(df_row.pagetxt), \"has_cgct_art\": contains_cgct_art(df_row.pagetxt), \"has_cch\": contains_cch(df_row.pagetxt), \"has_cch_L111\": contains_cch_L111(df_row.pagetxt), \"has_cch_L511\": contains_cch_L511(df_row.pagetxt), \"has_cch_L521\": contains_cch_L521(df_row.pagetxt), \"has_cch_L541\": contains_cch_L541(df_row.pagetxt), \"has_cch_R511\": contains_cch_R511(df_row.pagetxt), \"has_cc\": contains_cc(df_row.pagetxt), \"has_cc_art\": contains_cc_art(df_row.pagetxt), # - donn\u00e9es \"adresse\": pg_adr_doc, # refactor 2023-03-31: remonter l'extraction de l'adresse pr\u00e9cise \"adr_num\": adr_fields[\"adr_num\"], # num\u00e9ro de la voie \"adr_ind\": adr_fields[\"adr_ind\"], # indice de r\u00e9p\u00e9tition \"adr_voie\": adr_fields[\"adr_voie\"], # nom de la voie \"adr_compl\": adr_fields[\"adr_compl\"], # compl\u00e9ment d'adresse \"adr_cpostal\": adr_fields[\"adr_cpostal\"], # code postal \"adr_ville\": adr_fields[\"adr_ville\"], # ville # end refactor 2023-03-31 \"parcelle\": parcelles[0] if parcelles else None, # TODO si la page contient plusieurs empans d\u00e9signant une ou plusieurs parcelles \"proprio\": get_proprio(df_row.pagetxt), # WIP \"syndic\": get_syndic(df_row.pagetxt), \"gest\": get_gest(df_row.pagetxt), \"date\": get_date(df_row.pagetxt), # * arr\u00eat\u00e9 \"num_arr\": get_num(df_row.pagetxt), \"nom_arr\": get_nom(df_row.pagetxt), \"classe\": get_classe(df_row.pagetxt), \"urgence\": get_urgence(df_row.pagetxt), \"demo\": get_demo(df_row.pagetxt), \"int_hab\": get_int_hab(df_row.pagetxt), \"equ_com\": get_equ_com(df_row.pagetxt), } else: # tous les champs sont vides (\"None\") rec_struct = {x: None for x in DTYPE_PARSE} return rec_struct","title":"Process"},{"location":"Code%20Source/process/#process","text":"Fonctions d'extractions des donn\u00e9es des fichiers pr\u00e9trait\u00e9s.","title":"Process"},{"location":"Code%20Source/process/#src.process.aggregate_pages--agrege-les-pages-et-leurs-donnees-extraites-en-documents","text":"Chaque ligne correspond \u00e0 un document. Les \u00e9ventuelles incoh\u00e9rences entre valeurs extraites pour diff\u00e9rentes pages d'un m\u00eame document sont signal\u00e9es.","title":"Agr\u00e8ge les pages, et leurs donn\u00e9es extraites, en documents."},{"location":"Code%20Source/process/#src.process.aggregate_pages.aggregate_pages","text":"Fusionne les champs extraits des diff\u00e9rentes pages d'un document.","title":"aggregate_pages()"},{"location":"Code%20Source/process/#src.process.aggregate_pages.aggregate_pages--parameters","text":"df_grp: pd.core.groupby.DataFrame Pages d'un document include_actes_page_ar: boolean, defaults to False Inclut la page d'accus\u00e9 de r\u00e9ception d'@ctes.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.aggregate_pages.aggregate_pages--returns","text":"rec_struct: dict Dictionnaire de valeurs de diff\u00e9rents types ou nulles, selon que les \u00e9l\u00e9ments ont \u00e9t\u00e9 d\u00e9tect\u00e9s. src\\process\\aggregate_pages.py def aggregate_pages(df_grp: pd.DataFrame, include_actes_page_ar: bool = False) -> Dict: \"\"\"Fusionne les champs extraits des diff\u00e9rentes pages d'un document. Parameters ---------- df_grp: pd.core.groupby.DataFrame Pages d'un document include_actes_page_ar: boolean, defaults to False Inclut la page d'accus\u00e9 de r\u00e9ception d'@ctes. Returns ------- rec_struct: dict Dictionnaire de valeurs de diff\u00e9rents types ou nulles, selon que les \u00e9l\u00e9ments ont \u00e9t\u00e9 d\u00e9tect\u00e9s. \"\"\" # conserver uniquement les pages avec du texte ; # actuellement: \"has_stamp is None\" implique que la page ne contient pas de texte # FIXME g\u00e9rer les pages sans texte en amont? grp = df_grp.dropna(subset=[\"has_stamp\"]) # si demand\u00e9, exclure l'\u00e9ventuelle page d'accus\u00e9 de r\u00e9ception d'actes if not include_actes_page_ar: grp = grp.query(\"not is_accusedereception_page\") # si le groupe est vide, renvoyer une ligne (pour le document) vide ; # utile lorsque le document ne contient pas de texte, notamment les PDF non-natifs non-oc\u00e9ris\u00e9s (ou pas encore) if grp.empty: rec_struct = {x: None for x in DTYPE_PARSE_AGG} return rec_struct # t0 = time.time() if False: grp[grp.has_stamp][\"pagenum\"].to_list() t0b = time.time() grp[grp[\"has_stamp\"]][\"pagenum\"].to_list() t0c = time.time() grp.query(\"has_stamp\")[\"pagenum\"].to_list() t0d = time.time() print(f\"{t0b - t0:.3f}\\t{t0c - t0b:.3f}\\t{t0d - t0c:.3f}\") # agr\u00e9ger les num\u00e9ros de pages ou les valeurs extraites rec_actes = { # - m\u00e9tadonn\u00e9es # * @ctes # table: contr\u00f4le ; expectation: liste de valeurs continue (ex: 1,2,3) ou vide (all NaN) # grp.query(\"has_stamp\")[\"pagenum\"].to_list(), \"actes_pages_tampon\": pagenums(grp, \"has_stamp\"), # table: contr\u00f4le ; expectation: liste vide (all NaN) ou valeur unique # grp.query(\"is_accusedereception_page\")[\"pagenum\"].to_list() \"actes_pages_ar\": pagenums(grp, \"is_accusedereception_page\"), } # t1 = time.time() rec_commu = { # - tous arr\u00eat\u00e9s # * champ \"commune\" # TODO table: ? ; TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"commune_maire\": first(grp, \"commune_maire\"), } # t2 = time.time() rec_pars = { # * champs structure de l'arr\u00eat\u00e9 \"pages_vu\": pagenums( grp, \"has_vu\" ), # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) \"pages_considerant\": pagenums( grp, \"has_considerant\" ), # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) \"pages_arrete\": pagenums( grp, \"has_arrete\" ), # table: contr\u00f4le ; expectation: valeur unique ou vide/NaN \"pages_article\": pagenums( grp, \"has_article\" ), # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) } # t3 = time.time() rec_regl = { # arr\u00eat\u00e9s sp\u00e9cifiques # - r\u00e9glementaires \"pages_cgct\": pagenums( grp, \"has_cgct\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? # table: contr\u00f4le ; expectation: liste de valeurs continue ou vide (all NaN) \"pages_cgct_art\": pagenums( grp, \"has_cgct_art\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch\": pagenums( grp, \"has_cch\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L111\": pagenums( grp, \"has_cch_L111\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L511\": pagenums( grp, \"has_cch_L511\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L521\": pagenums( grp, \"has_cch_L521\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_L541\": pagenums( grp, \"has_cch_L541\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cch_R511\": pagenums( grp, \"has_cch_R511\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cc\": pagenums(grp, \"has_cc\"), # TODO retraiter pour classer les arr\u00eat\u00e9s? \"pages_cc_art\": pagenums( grp, \"has_cc_art\" ), # TODO retraiter pour classer les arr\u00eat\u00e9s? } # t4 = time.time() rec_adre = { # - donn\u00e9es \"adresse_brute\": first( grp, \"adresse\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"adr_num\": first(grp, \"adr_num\"), \"adr_ind\": first(grp, \"adr_ind\"), \"adr_voie\": first(grp, \"adr_voie\"), \"adr_compl\": first(grp, \"adr_compl\"), \"adr_cpostal\": first(grp, \"adr_cpostal\"), \"adr_ville\": first(grp, \"adr_ville\"), } # t5 = time.time() rec_parce = { \"parcelle\": first( grp, \"parcelle\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t6 = time.time() rec_proprio = { \"proprio\": first( grp, \"proprio\" ), # TODO expectation: 1-n (TODO normalisation: casse, accents etc?) ; vide pour abrogation? } rec_syndi = { \"syndic\": first( grp, \"syndic\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } rec_gest = { \"gest\": first( grp, \"gest\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t7 = time.time() rec_date = { \"arr_date\": first( grp, \"date\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t8 = time.time() rec_num = { \"num_arr\": first( grp, \"num_arr\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } # t9 = time.time() if False: print( f\"actes: {t1 - t0:.3f}\\tcommune: {t2 - t1:.3f}\\tvu_etc: {t3 - t2:.3f}\" + f\"\\tregl: {t4 - t3:.3f}\\tadr: {t5 - t4:.3f}\\tparcelle: {t6 - t5:.3f}\" + f\"\\tsyndic: {t7 - t6:.3f}\\tdate: {t8 - t7:.3f}\\tnum: {t9 - t8:.3f}\" ) rec_nom = { \"nom_arr\": first( grp, \"nom_arr\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } rec_classi = { \"classe\": first( grp, \"classe\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"urgence\": first( grp, \"urgence\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"demo\": first( grp, \"demo\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"int_hab\": first( grp, \"int_hab\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN \"equ_com\": first( grp, \"equ_com\" ), # TODO expectation: valeur unique (modulo normalisation: casse, accents etc?) ou vide/NaN } rec_struct = ( rec_actes | rec_commu | rec_pars | rec_regl | rec_adre | rec_parce | rec_proprio | rec_syndi | rec_gest | rec_date | rec_num | rec_nom | rec_classi ) return rec_struct","title":"Returns"},{"location":"Code%20Source/process/#src.process.aggregate_pages.create_docs_dataframe","text":"Rassembler les informations des documents dans un DataFrame. Fusionner les entr\u00e9es de chaque page en une entr\u00e9e par document.","title":"create_docs_dataframe()"},{"location":"Code%20Source/process/#src.process.aggregate_pages.create_docs_dataframe--parameters","text":"df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.aggregate_pages.create_docs_dataframe--returns","text":"df_docs: pd.DataFrame Tableau contenant les m\u00e9tadonn\u00e9es et donn\u00e9es extraites des documents. src\\process\\aggregate_pages.py def create_docs_dataframe( df_pages: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Rassembler les informations des documents dans un DataFrame. Fusionner les entr\u00e9es de chaque page en une entr\u00e9e par document. Parameters ---------- df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns ------- df_docs: pd.DataFrame Tableau contenant les m\u00e9tadonn\u00e9es et donn\u00e9es extraites des documents. \"\"\" doc_rows = [] for _, df_grp in df_pages.groupby(\"fullpath_txt\"): # reporter les m\u00e9tadonn\u00e9es du fichier PDF et du TXT, dans chaque entr\u00e9e de document meta_doc = { x: df_grp[x].to_list()[0] for x in DTYPE_META_NTXT_FILT } # FIXME prendre les m\u00e9tadonn\u00e9es du document dans le CSV 1 ligne par doc? # retraiter sp\u00e9cifiquement le champ \"exclude\": si toutes les pages sont \"exclude\", alors le fichier aussi, sinon non meta_doc[\"exclude\"] = df_grp[\"exclude\"].all() # rassembler les donn\u00e9es des pages ; # exclure l'\u00e9ventuelle page d'accus\u00e9 de r\u00e9ception d'actes data_doc = aggregate_pages(df_grp, include_actes_page_ar=False) doc_rows.append(meta_doc | data_doc) # python >= 3.9 (dict union) df_docs = pd.DataFrame.from_records(doc_rows) df_docs = df_docs.astype(dtype=DTYPE_META_NTXT_DOC) return df_docs","title":"Returns"},{"location":"Code%20Source/process/#src.process.aggregate_pages.first","text":"Renvoie la premi\u00e8re valeur non-vide de la colonne src\\process\\aggregate_pages.py def first(df_grp: pd.DataFrame, col_on: str): \"\"\"Renvoie la premi\u00e8re valeur non-vide de la colonne\"\"\" s_ok = df_grp[col_on].dropna() if s_ok.empty: return None else: return s_ok.to_list()[0]","title":"first()"},{"location":"Code%20Source/process/#src.process.aggregate_pages.pagenums","text":"Renvoie la liste des num\u00e9ros de pages o\u00f9 une colonne est vraie src\\process\\aggregate_pages.py def pagenums(df_grp: pd.DataFrame, col_on: str): \"\"\"Renvoie la liste des num\u00e9ros de pages o\u00f9 une colonne est vraie\"\"\" return df_grp[df_grp[col_on]][\"pagenum\"].to_list()","title":"pagenums()"},{"location":"Code%20Source/process/#src.process.enrich_data--enrichit-les-donnees-avec-des-donnees-supplementaires","text":"Ajoute le code INSEE de la commune.","title":"Enrichit les donn\u00e9es avec des donn\u00e9es suppl\u00e9mentaires."},{"location":"Code%20Source/process/#src.process.enrich_data.create_docs_dataframe","text":"Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document.","title":"create_docs_dataframe()"},{"location":"Code%20Source/process/#src.process.enrich_data.create_docs_dataframe--parameters","text":"df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.enrich_data.create_docs_dataframe--returns","text":"df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. src\\process\\enrich_data.py def create_docs_dataframe( df_agg: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document. Parameters ---------- df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns ------- df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. \"\"\" # ajoute le code INSEE, \u00e0 partir de la commune et du code postal (pour Marseille) # df_agg[\"adr_codeinsee\"] = df_agg.apply( # lambda row: get_codeinsee(row[\"adr_ville\"], row[\"adr_cpostal\"]), axis=1 # ) # remplace la r\u00e9f\u00e9rence cadastrale par sa version normalis\u00e9e df_agg[\"par_ref_cad\"] = df_agg.apply( lambda row: generate_refcadastrale_norm( row[\"adr_codeinsee\"], row[\"par_ref_cad\"], row[\"arr_pdf\"], row[\"adr_cpostal\"], ), axis=1, ) df_docs = df_agg.astype(dtype=DTYPE_DATA) return df_docs","title":"Returns"},{"location":"Code%20Source/process/#src.process.export_data--exporte-les-donnees-en-fichiers-csv","text":"4 tables: * arr\u00eat\u00e9, * adresse, * parcelle, * notifi\u00e9","title":"Exporte les donn\u00e9es en fichiers CSV."},{"location":"Code%20Source/process/#src.process.extract_data--extraire-les-donnees-des-documents","text":"Les donn\u00e9es sont extraites des empans de texte rep\u00e9r\u00e9s au pr\u00e9alable, et normalis\u00e9es. Lorsque plusieurs empans de texte sont susceptibles de renseigner sur la m\u00eame donn\u00e9e, les diff\u00e9rentes valeurs extraites sont accumul\u00e9es pour certains champs (ex: propri\u00e9taires) ou compar\u00e9es et s\u00e9lectionn\u00e9es pour d'autres champs (ex: commune).","title":"Extraire les donn\u00e9es des documents."},{"location":"Code%20Source/process/#src.process.extract_data.create_docs_dataframe","text":"Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document.","title":"create_docs_dataframe()"},{"location":"Code%20Source/process/#src.process.extract_data.create_docs_dataframe--parameters","text":"df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.extract_data.create_docs_dataframe--returns","text":"df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. src\\process\\extract_data.py def create_docs_dataframe( df_agg: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Extraire les informations des documents dans un DataFrame. Normaliser et extraire les donn\u00e9es de chaque document en une entr\u00e9e par document. Parameters ---------- df_pages: pd.DataFrame M\u00e9tadonn\u00e9es et donn\u00e9es extraites des pages. Returns ------- df_docs: pd.DataFrame Tableau contenant les donn\u00e9es normalis\u00e9es extraites des documents. \"\"\" doc_rows = [] # filtrer les documents \u00e0 exclure compl\u00e8tement: documents hors p\u00e9rim\u00e8tre strict du jeu de donn\u00e9es cible df_filt = df_agg[~df_agg[\"exclude\"]] # it\u00e9rer sur tous les documents non-exclus for i, df_row in enumerate(df_filt.itertuples()): doc_idu = { \"idu\": f\"id_{i:04}\", # FIXME identifiant unique } doc_arr = { # arr\u00eat\u00e9 \"arr_date\": ( process_date_brute(getattr(df_row, \"arr_date\")) if pd.notna(getattr(df_row, \"arr_date\")) else None ), \"arr_num_arr\": ( normalize_string( getattr(df_row, \"num_arr\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"num_arr\")) else None ), \"arr_nom_arr\": ( normalize_string( getattr(df_row, \"nom_arr\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"nom_arr\")) else None ), \"arr_classe\": ( normalize_string( getattr(df_row, \"classe\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"classe\")) else None ), \"arr_urgence\": ( normalize_string( getattr(df_row, \"urgence\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"urgence\")) else None ), \"arr_demo\": ( normalize_string( getattr(df_row, \"demo\"), num=True, apos=True, hyph=True, spaces=True ) if pd.notna(getattr(df_row, \"demo\")) else None ), # TODO affiner \"arr_int_hab\": ( normalize_string( getattr(df_row, \"int_hab\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"int_hab\")) else None ), # TODO affiner \"arr_equ_com\": ( normalize_string( getattr(df_row, \"equ_com\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"equ_com\")) else None ), # TODO affiner # (m\u00e9tadonn\u00e9es du doc) \"arr_pdf\": getattr(df_row, \"pdf\"), \"arr_url\": getattr( df_row, \"fullpath\" ), # l'URL sera r\u00e9\u00e9crite avec une URL locale (r\u00e9seau) ou publique, au moment de l'export } # adresse # - nettoyer a minima de l'adresse brute adr_ad_brute = ( normalize_string( getattr(df_row, \"adresse_brute\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"adresse_brute\")) else None ) # WIP 2023-03-30: supprimer car sera fait dans parse_native_pages, parse_doc, parse_doc_direct # - extraire les \u00e9l\u00e9ments d'adresse en traitant l'adresse brute adr_num = getattr(df_row, \"adr_num\") # num\u00e9ro de la voie adr_ind = getattr(df_row, \"adr_ind\") # indice de r\u00e9p\u00e9tition adr_voie = getattr(df_row, \"adr_voie\") # nom de la voie adr_compl = getattr(df_row, \"adr_compl\") # compl\u00e9ment d'adresse adr_cpostal = getattr(df_row, \"adr_cpostal\") # code postal adr_ville = getattr(df_row, \"adr_ville\") # ville # end WIP 2023-03-30 # - nettoyer a minima la commune extraite des en-t\u00eate ou pied-de-page ou de la mention du maire signataire adr_commune_maire = ( normalize_string( getattr(df_row, \"commune_maire\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"commune_maire\")) else None ) # - d\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9 en reconciliant la commune de l'adresse et # celle de l'autorit\u00e9 adr_commune = determine_commune(adr_ville, adr_commune_maire) if pd.isna(adr_commune) or not adr_commune: logging.warning(f\"Pas de commune pour {doc_arr['arr_pdf']}\") # - d\u00e9terminer le code INSEE de la commune adr_codeinsee = get_codeinsee(adr_commune, adr_cpostal) # - si l'adresse ne contenait pas de code postal, essayer de d\u00e9terminer le code postal # \u00e0 partir du code INSEE de la commune (ne fonctionne pas pour Aix-en-Provence) if pd.isna(adr_cpostal) or not adr_cpostal: adr_cpostal = get_codepostal(adr_commune, adr_codeinsee) if not adr_cpostal: logging.warning( f\"{doc_arr['arr_pdf']}: Pas de code postal: cpostal(adr_brute)={adr_cpostal}, commune={adr_commune}, code_insee={adr_codeinsee}, get_codepostal={adr_cpostal}\" ) # - cr\u00e9er une adresse normalis\u00e9e ; la coh\u00e9rence des champs est v\u00e9rifi\u00e9e adr_interm = { \"num\": adr_num, # num\u00e9ro de la voie \"ind\": adr_ind, # indice de r\u00e9p\u00e9tition \"voie\": adr_voie, # nom de la voie \"compl\": adr_compl, # compl\u00e9ment d'adresse \"cpostal\": adr_cpostal, # code postal \"ville\": adr_commune, # ville } adr_norm = normalize_adresse(adr_interm) adr_adresse = create_adresse_normalisee( adr_norm[\"num\"], adr_norm[\"ind\"], adr_norm[\"voie\"], adr_norm[\"compl\"], adr_norm[\"cpostal\"], adr_norm[\"commune\"], ) # - rassembler les champs doc_adr = { # adresse \"adr_ad_brute\": adr_ad_brute, # adresse brute \"adr_num\": adr_norm[\"num\"], # num\u00e9ro de la voie \"adr_ind\": adr_norm[\"ind\"], # indice de r\u00e9p\u00e9tition \"adr_voie\": adr_norm[\"voie\"], # nom de la voie \"adr_compl\": adr_norm[\"compl\"], # compl\u00e9ment d'adresse \"adr_cpostal\": adr_norm[\"cpostal\"], # code postal \"adr_ville\": adr_norm[\"commune\"], # ville \"adr_adresse\": adr_adresse, # adresse normalis\u00e9e \"adr_codeinsee\": adr_codeinsee, # code insee (5 chars) # compl\u00e9t\u00e9 en aval par \"enrichi\" } # parcelle cadastrale ref_cad = ( normalize_string( getattr(df_row, \"parcelle\"), num=True, apos=True, hyph=True, spaces=True ) if pd.notna(getattr(df_row, \"parcelle\")) else None ) doc_par = { \"par_ref_cad\": ref_cad, # r\u00e9f\u00e9rence cadastrale } # notifi\u00e9s doc_not = { \"not_id_proprio\": ( normalize_string( getattr(df_row, \"proprio\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"proprio\")) else None ), # identification des propri\u00e9taires \"not_proprio\": \"\", # TODO liste des noms des propri\u00e9taires \"not_id_syndic\": ( normalize_string( getattr(df_row, \"syndic\"), num=True, apos=True, hyph=True, spaces=True, ) if pd.notna(getattr(df_row, \"syndic\")) else None ), # identification du syndic \"not_syndic\": \"\", # TODO nom du syndic \"not_id_gest\": ( normalize_string( getattr(df_row, \"gest\"), num=True, apos=True, hyph=True, spaces=True ) if pd.notna(getattr(df_row, \"gest\")) else None ), # identification du gestionnaire \"not_gest\": \"\", # TODO nom du gestionnaire } doc_data = doc_idu | doc_arr | doc_adr | doc_par | doc_not doc_rows.append(doc_data) df_docs = pd.DataFrame.from_records(doc_rows) df_docs = df_docs.astype(dtype=DTYPE_DATA) return df_docs","title":"Returns"},{"location":"Code%20Source/process/#src.process.extract_data.determine_commune","text":"D\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9. R\u00e9concilie la commune \u00e9ventuellement contenue dans l'adresse du ou des b\u00e2timents vis\u00e9s avec le nom de commune extrait du document (template, autorit\u00e9 ou lieu de signature).","title":"determine_commune()"},{"location":"Code%20Source/process/#src.process.extract_data.determine_commune--parameters","text":"adr_commune_brute: str Commune extraite de l'adresse du b\u00e2timent vis\u00e9 par l'arr\u00eat\u00e9. adr_commune_maire: str Commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou du template du document.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.extract_data.determine_commune--returns","text":"adr_commune: str Commune de l'adresse vis\u00e9e. src\\process\\extract_data.py def determine_commune(adr_commune_brute: str, adr_commune_maire: str) -> str: \"\"\"D\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9. R\u00e9concilie la commune \u00e9ventuellement contenue dans l'adresse du ou des b\u00e2timents vis\u00e9s avec le nom de commune extrait du document (template, autorit\u00e9 ou lieu de signature). Parameters ---------- adr_commune_brute: str Commune extraite de l'adresse du b\u00e2timent vis\u00e9 par l'arr\u00eat\u00e9. adr_commune_maire: str Commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou du template du document. Returns ------- adr_commune: str Commune de l'adresse vis\u00e9e. \"\"\" # TODO normaliser vers la graphie de la table des codes INSEE? Quid des arrondissements de Marseille? # TODO comparer les graphies? if (pd.isna(adr_commune_brute)) and (pd.isna(adr_commune_maire)): # pas de commune adr_commune = None elif (pd.isna(adr_commune_maire)) or ( not P_COMMUNES_AMP_ALLFORMS.match(adr_commune_maire) ): adr_commune = adr_commune_brute # TODO normaliser? elif (pd.isna(adr_commune_brute)) or ( not P_COMMUNES_AMP_ALLFORMS.match(adr_commune_brute) ): adr_commune = adr_commune_maire else: # was: adr_commune = adr_commune_maire adr_commune = adr_commune_brute # .title() si on veut minimiser les diff\u00e9rences avec adr_commune_maire pour comparer return adr_commune","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc_direct--analyse-un-arrete-et-en-extrait-les-donnees","text":"","title":"Analyse un arr\u00eat\u00e9 et en extrait les donn\u00e9es."},{"location":"Code%20Source/process/#src.process.parse_doc_direct.create_file_name_url","text":"Creates a url-compliant filename by removing all bad characters and maintaining the windows path length limit (which by default is 255) 155 to take into account the path length","title":"create_file_name_url()"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.create_file_name_url--parameters","text":"file_name: str Nom du fichier allowance: int Longueur maximale du chemin complet (chemin + nom de fichier) src\\process\\parse_doc_direct.py def create_file_name_url(file_name: str, allowance: int = 155): \"\"\" Creates a url-compliant filename by removing all bad characters and maintaining the windows path length limit (which by default is 255) 155 to take into account the path length Parameters ---------- file_name: str Nom du fichier allowance: int Longueur maximale du chemin complet (chemin + nom de fichier) \"\"\" bad_characters = re.compile(r\"[\\\\/<>,:\\\"|?*^$&{}\\[\\]`\\x00-\\x1F\\x7F]+\") if allowance > 255: allowance = 255 # on most common filesystems, including NTFS a file_name can not exceed 255 characters # assign allowance for things that must be in the file name # make sure that user input doesn't contain bad characters file_name = bad_characters.sub(\"\", file_name) file_name = file_name.replace(\"'\", \"_\").replace(\" \", \"_\") ret = \"\" for string in [file_name]: length = len(string) if allowance - length < 0: string = string[:allowance] length = len(string) ret += string allowance -= length if allowance < 0: raise ValueError( \"\"\"It is not possible to give a reasonable file name, due to length limitations. Consider changing location to somewhere with a shorter path.\"\"\" ) return ret","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.enrich_adresse","text":"Consolide et enrichit une adresse, avec ville et codes (INSEE et code postal). Harmonise et compl\u00e8te les informations extraites de l'adresse vis\u00e9e \u00e0 partir des informations extraites du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9. Ajoute une adresse normalis\u00e9e.","title":"enrich_adresse()"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.enrich_adresse--parameters","text":"fn_pdf: str Nom du fichier PDF (pour debug). adresse: dict Adresse vis\u00e9e par le document. commune_maire: str Ville extraite du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.enrich_adresse--returns","text":"adresse_enr: dict Adresse enrichie et augment\u00e9e. src\\process\\parse_doc_direct.py def enrich_adresse(fn_pdf: str, adresse: dict, commune_maire: str) -> Dict: \"\"\"Consolide et enrichit une adresse, avec ville et codes (INSEE et code postal). Harmonise et compl\u00e8te les informations extraites de l'adresse vis\u00e9e \u00e0 partir des informations extraites du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9. Ajoute une adresse normalis\u00e9e. Parameters ---------- fn_pdf: str Nom du fichier PDF (pour debug). adresse: dict Adresse vis\u00e9e par le document. commune_maire: str Ville extraite du template ou de l'autorit\u00e9 prenant l'arr\u00eat\u00e9. Returns ------- adresse_enr: dict Adresse enrichie et augment\u00e9e. \"\"\" adresse_enr = adresse.copy() # - d\u00e9terminer la commune de l'adresse vis\u00e9e par l'arr\u00eat\u00e9 en reconciliant la commune mentionn\u00e9e # dans cette adresse avec celle extraite des mentions de l'autorit\u00e9 ou du template adresse_enr[\"ville\"] = determine_commune(adresse_enr[\"ville\"], commune_maire) if not adresse_enr[\"ville\"]: logging.warning( f\"{fn_pdf}: impossible de d\u00e9terminer la commune: {adresse_enr['ville'], commune_maire}\" ) # - d\u00e9terminer le code INSEE de la commune # FIXME communes hors M\u00e9tropole: le filtrage sera-t-il fait en amont, lors de l'extraction depuis actes? sinon AssertionError ici try: codeinsee = get_codeinsee(adresse_enr[\"ville\"], adresse_enr[\"cpostal\"]) except AssertionError: print( f\"{fn_pdf}: get_codeinsee(): adr_ville={adresse_enr['ville']}, adr_cpostal={adresse_enr['cpostal']}\" ) print(f\"{adresse}\") raise else: if not codeinsee: logging.warning( f\"{fn_pdf}: impossible de d\u00e9terminer le code INSEE: {adresse_enr['ville'], adresse_enr['cpostal']}\" ) # - si l'adresse ne contenait pas de code postal, essayer de d\u00e9terminer le code postal # \u00e0 partir du code INSEE de la commune (ne fonctionne pas pour Aix-en-Provence) if not adresse_enr[\"cpostal\"]: adresse_enr[\"cpostal\"] = get_codepostal(adresse_enr[\"ville\"], codeinsee) if not adresse_enr[\"cpostal\"]: logging.warning( f\"{fn_pdf}: Pas de code postal: adr_brute={adresse_enr['ad_brute']}, commune={adresse_enr['ville']}, code_insee={codeinsee}, get_codepostal={adresse_enr['cpostal']}\" ) # - cr\u00e9er une adresse normalis\u00e9e ; la coh\u00e9rence des champs est v\u00e9rifi\u00e9e adresse_enr = normalize_adresse(adresse_enr) if adresse_enr[\"ad_brute\"]: adresse_enr[\"adresse\"] = create_adresse_normalisee( adresse_enr[\"num\"], adresse_enr[\"ind\"], adresse_enr[\"voie\"], adresse_enr[\"compl\"], adresse_enr[\"cpostal\"], adresse_enr[\"ville\"], ) else: adresse_enr[\"adresse\"] = None # - positionner finalement le code INSEE adresse_enr[\"codeinsee\"] = codeinsee return adresse_enr","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.extract_adresses_commune","text":"Extraire les adresses vis\u00e9es par l'arr\u00eat\u00e9, et la commune.","title":"extract_adresses_commune()"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.extract_adresses_commune--parameters","text":"fn_pdf: string Nom du fichier PDF de l'arr\u00eat\u00e9 (pour les messages de logs: warnings et erreurs) pg_txt_body: string Corps de texte de la page commune_maire: string Mention de la commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou des","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.extract_adresses_commune--returns","text":"adresses: list(dict) Adresses vis\u00e9es par l'arr\u00eat\u00e9 src\\process\\parse_doc_direct.py def extract_adresses_commune( fn_pdf: str, pg_txt_body: str, commune_maire: str ) -> List[Dict]: \"\"\"Extraire les adresses vis\u00e9es par l'arr\u00eat\u00e9, et la commune. Parameters ---------- fn_pdf: string Nom du fichier PDF de l'arr\u00eat\u00e9 (pour les messages de logs: warnings et erreurs) pg_txt_body: string Corps de texte de la page commune_maire: string Mention de la commune extraite de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou des Returns ------- adresses: list(dict) Adresses vis\u00e9es par l'arr\u00eat\u00e9 \"\"\" try: adresses_visees = get_adr_doc(pg_txt_body) except AssertionError: logging.error(f\"{fn_pdf}: probl\u00e8me d'extraction d'adresse\") raise # if fn_pdf == \"p\u00e9rim\u00e8tre de s\u00e9curit\u00e9 82 Hoche 105 Kleber 13003.pdf\": # print(f\"{commune_maire}, {adresses_visees}\") # # raise ValueError(\"don't stop me now (too soon)\") if not adresses_visees: adr = { # adresse brute \"ad_brute\": None, # champs \"num\": None, \"ind\": None, \"voie\": None, \"compl\": None, \"cpostal\": None, \"ville\": None, # adresse propre \"adresse\": None, } adr_enr = enrich_adresse(fn_pdf, adr, commune_maire) return [adr_enr] # renommer les champs # TODO le faire dans get_adr_doc et adapter le code dans les autres modules adresses_visees = [ { \"ad_brute\": x[\"adresse_brute\"], \"adresses\": [ {k.replace(\"adr_\", \"\"): v for k, v in y.items()} for y in x[\"adresses\"] ], } for x in adresses_visees ] # prendre la 1re zone d'adresses reconnue dans le texte (heuristique) # TODO en rep\u00e9rer d'autres? incertain adr0 = adresses_visees[0] adresse_brute = adr0[\"ad_brute\"] # TODO am\u00e9liorer les r\u00e9sultats par une collecte plus exhaustive (qui n\u00e9cessiterait le d\u00e9doublonnage) ou une meilleure heuristique ? # extraire la ou les adresses de cette zone # (on supprime au passage les pr\u00e9fixes \"adr_\" des noms des champs, archa\u00efsme \u00e0 corriger plus tard \u00e9ventuellement) adresses = [({\"ad_brute\": adresse_brute} | x) for x in adr0[\"adresses\"]] if not adresses: logging.error( f\"{fn_pdf}: aucune adresse extraite de la zone d'adresse(s): {adresse_brute}\" ) if len(adresses_visees) > 1: # si la 1re adresse n'a pas de code postal, tenter de r\u00e9cup\u00e9rer le code postal des adresses suivantes # on construit 2 mappings: # - (num, voie) => cp numvoie2cp = dict() # - voie => cp # fallback, quand la mention d'adresse extraite ne contient pas de num\u00e9ro (mais une mention ult\u00e9rieure, oui) voie2cp = dict() # on it\u00e8re sur l'ensemble des adresses extraites du document pour cr\u00e9er une table d'association vers les codes postaux for x in adresses_visees: for y in x[\"adresses\"]: if y[\"cpostal\"]: norm_voie = normalize_string( remove_accents(y[\"voie\"]), num=True, apos=True, hyph=True, spaces=True, ).lower() # (num\u00e9ro, voie) -> cp numvoie2cp[(y[\"num\"], norm_voie)] = y[\"cpostal\"] # fallback: voie -> cp voie2cp[norm_voie] = y[\"cpostal\"] # WIP 2023-05-09 # print(numvoie2cp) for sel_adr in adresses: # pour chaque adresse consid\u00e9r\u00e9e comme \u00e9tant vis\u00e9e par l'arr\u00eat\u00e9 if sel_adr[\"voie\"] and not sel_adr[\"cpostal\"]: # si on a une voie mais pas de code postal, on essaie de renseigner # le code postal par propagation \u00e0 partir des autres adresses norm_voie = normalize_string( remove_accents(sel_adr[\"voie\"]), num=True, apos=True, hyph=True, spaces=True, ).lower() if sel_adr[\"num\"]: # si on a un num\u00e9ro de voie (c'est l'id\u00e9al, car le code postal est normalement unique) sel_short = (sel_adr[\"num\"], norm_voie) # print(f\">>>>>> sel_short: {sel_short}\") sel_adr[\"cpostal\"] = numvoie2cp.get(sel_short, None) else: # sans num\u00e9ro de voie, on recourt au tableau associatif sans num\u00e9ro sel_adr[\"cpostal\"] = voie2cp.get(norm_voie, None) # WIP 2023-05-09 # if fn_pdf == \"90 cours Sextius - ML.pdf\": # print(f\"{adresses_visees}\\n{numvoie2cp}\\n{adresses}\") # raise ValueError(\"don't stop me now\") # pass # si besoin d'une alternative: d\u00e9terminer commune, code INSEE et code postal pour les adresses[0] et propager les valeurs aux autres adresses adresses_enr = [enrich_adresse(fn_pdf, x, commune_maire) for x in adresses] return adresses_enr","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.parse_arrete","text":"Analyse un arr\u00eat\u00e9 et extrait les donn\u00e9es qu'il contient. L'arr\u00eat\u00e9 est d\u00e9coup\u00e9 en paragraphes puis les donn\u00e9es sont extraites.","title":"parse_arrete()"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.parse_arrete--parameters","text":"fp_pdf_in : Path Fichier PDF source (temporairement?) fp_txt_in : Path Fichier texte \u00e0 analyser.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.parse_arrete--returns","text":"doc_data : dict Donn\u00e9es extraites du document. src\\process\\parse_doc_direct.py def parse_arrete(fp_pdf_in: Path, fp_txt_in: Path) -> dict: \"\"\"Analyse un arr\u00eat\u00e9 et extrait les donn\u00e9es qu'il contient. L'arr\u00eat\u00e9 est d\u00e9coup\u00e9 en paragraphes puis les donn\u00e9es sont extraites. Parameters ---------- fp_pdf_in : Path Fichier PDF source (temporairement?) fp_txt_in : Path Fichier texte \u00e0 analyser. Returns ------- doc_data : dict Donn\u00e9es extraites du document. \"\"\" fn_pdf = fp_pdf_in.name fn_pdf_out = create_file_name_url(fn_pdf) pages = load_pages_text(fp_txt_in) if not any(pages): logging.warning(f\"{fp_txt_in}: aucune page de texte\") arr_url = FS_URL_FALLBACK.format(pdf=fn_pdf_out) logging.warning(f\"URL temporaire (sans code commune ni ann\u00e9e): {arr_url}\") return { \"adresses\": [], \"arretes\": [ { \"pdf\": fn_pdf, \"url\": arr_url, } ], \"notifies\": [], \"parcelles\": [], } # filtrer les pages qui sont \u00e0 sortir du traitement: # - la ou les \u00e9ventuelles pages d'accus\u00e9 de r\u00e9ception d'actes pages_ar = [i for i, x in enumerate(pages, start=1) if P_ACCUSE.match(x)] if pages_ar: logging.warning( f\"{fp_txt_in}: {len(pages_ar)} page(s) d'accus\u00e9 de r\u00e9ception actes: {pages_ar} (sur {len(pages)})\" ) # - la ou les \u00e9ventuelles pages d'annexes ? (TODO) skip_pages = pages_ar # remplacer les pages filtr\u00e9es par une cha\u00eene vide filt_pages = [ (x if i not in skip_pages else \"\") for i, x in enumerate(pages, start=1) ] # analyser la structure des pages doc_content = parse_arrete_pages(fn_pdf, filt_pages) # extraire les donn\u00e9es adresses = [] arretes = {} # un seul notifies = { \"proprios\": OrderedDict(), # propri\u00e9taires \"syndics\": OrderedDict(), # syndic (normalement unique) \"gests\": OrderedDict(), # gestionnaire (normalement unique) } parcelles = OrderedDict() # r\u00e9f\u00e9rences de parcelles cadastrales # - au pr\u00e9alable, rassembler toutes les donn\u00e9es en ajoutant le num\u00e9ro de page (FIXME) pages_body = [pg_cont[\"body\"] for pg_cont in doc_content] # pages_cont = [pg_cont[\"content\"] for pg_cont in doc_content] # future pages_cont = [] for pg_num, pg_cont in enumerate(doc_content, start=1): # pg_template = page_cont[\"template\"] # pg_content = page_cont[\"content\"] # future # FIXME ajouter \"page_num\" en amont, dans parse_arrete_pages() pages_cont.extend([({\"page_num\": pg_num} | x) for x in pg_cont[\"content\"]]) # extraire les champs un par un: # - arr\u00eat\u00e9 arr_dates = [ process_date_brute(x[\"span_txt\"]) for x in pages_cont if x[\"span_typ\"] == \"arr_date\" ] if arr_dates: arretes[\"date\"] = normalize_string( arr_dates[0], num=True, apos=True, hyph=True, spaces=True ) arr_nums = [x[\"span_txt\"] for x in pages_cont if x[\"span_typ\"] == \"num_arr\"] if arr_nums: arretes[\"num_arr\"] = normalize_string( arr_nums[0], num=True, apos=True, hyph=True, spaces=True ) arr_noms = [x[\"span_txt\"] for x in pages_cont if x[\"span_typ\"] == \"nom_arr\"] if arr_noms: arretes[\"nom_arr\"] = normalize_string( arr_noms[0], num=True, apos=True, hyph=True, spaces=True ) # - commune extraite des mentions de l'autorit\u00e9 prenant l'arr\u00eat\u00e9, ou du template du document adrs_commune_maire = [x for x in pages_cont if x[\"span_typ\"] == \"adr_ville\"] # - prendre arbitrairement la 1re mention et la nettoyer a minima # TODO regarder les erreurs et v\u00e9rifier si un autre choix donnerait de meilleurs r\u00e9sultats # TODO tester: si > 1, tester de matcher avec la liste des communes de la m\u00e9tropole # (et \u00e9ventuellement calculer la distance de Levenshtein pour v\u00e9rifier s'il est vraisemblable # que ce soient des variantes de graphie ou erreurs) if not adrs_commune_maire: adr_commune_maire = None else: adr_commune_maire = normalize_string( adrs_commune_maire[0][\"span_txt\"], num=True, apos=True, hyph=True, spaces=True, ) # remplacer par la forme canonique (communes AMP) adr_commune_maire = normalize_ville(adr_commune_maire) logging.warning(f\"adrs_commune_maire: {adrs_commune_maire}\") # DEBUG logging.warning(f\"adr_commune_maire: {adr_commune_maire}\") # DEBUG # # parcelles codeinsee = None # valeur par d\u00e9faut cpostal = None # valeur par d\u00e9faut for pg_txt_body in pages_body: if pg_txt_body: # extraire les informations sur l'arr\u00eat\u00e9 if \"classe\" not in arretes and (classe := get_classe(pg_txt_body)): arretes[\"classe\"] = classe if \"urgence\" not in arretes and (urgence := get_urgence(pg_txt_body)): arretes[\"urgence\"] = urgence if \"demo\" not in arretes and (demo := get_demo(pg_txt_body)): arretes[\"demo\"] = demo if \"int_hab\" not in arretes and (int_hab := get_int_hab(pg_txt_body)): arretes[\"int_hab\"] = int_hab if \"equ_com\" not in arretes and (equ_com := get_equ_com(pg_txt_body)): arretes[\"equ_com\"] = equ_com if \"pdf\" not in arretes: arretes[\"pdf\"] = fn_pdf # extraire la ou les adresse(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 d\u00e9tect\u00e9es sur cette page if not adresses: # pour le moment, on se contente de la premi\u00e8re page contenant au moins une zone d'adresse, # et sur cette page, de la premi\u00e8re zone d'adresse trouv\u00e9e ; # une zone peut contenir une ou plusieurs adresses obtenues par \"d\u00e9pliage\" (ex: 12 - 14 rue X) # TODO examiner les erreurs et d\u00e9terminer si une autre strat\u00e9gie donnerait de meilleurs r\u00e9sultats # si une adresse a d\u00e9j\u00e0 \u00e9t\u00e9 ajout\u00e9e mais qu'elle n'a \u00e9t\u00e9 remplie que gr\u00e2ce \u00e0 commune_maire pg_adresses = extract_adresses_commune( fn_pdf, pg_txt_body, adr_commune_maire ) if pg_adresses: adresses.extend(pg_adresses) # WIP on prend le code INSEE et code postal de la 1re adresse # print(adrs_doc) cpostal = adresses[0][\"cpostal\"] codeinsee = adresses[0][\"codeinsee\"] if (\"codeinsee\" not in arretes) and codeinsee: arretes[\"codeinsee\"] = codeinsee elif len(adresses) == 1 and not adresses[0][\"ad_brute\"]: # si une adresse a d\u00e9j\u00e0 \u00e9t\u00e9 ajout\u00e9e mais qu'elle n'a \u00e9t\u00e9 remplie que gr\u00e2ce \u00e0 commune_maire # (donc ne contient qu'une commune), on en cherche une plus pr\u00e9cise sur la page suivante, # \u00e0 tout hasard pg_adresses = extract_adresses_commune( fn_pdf, pg_txt_body, adr_commune_maire ) if pg_adresses and pg_adresses[0][\"ad_brute\"]: # on a bien extrait au moins une adresse du texte, on remplace l'adresse contenant # seulement une commune adresses = pg_adresses # WIP on prend le code INSEE et code postal de la 1re adresse # print(adrs_doc) cpostal = adresses[0][\"cpostal\"] codeinsee = adresses[0][\"codeinsee\"] if codeinsee: # on remplace le code commune INSEE pour tout le document arretes[\"codeinsee\"] = codeinsee # extraire les notifi\u00e9s if proprios := get_proprio(pg_txt_body): norm_proprios = normalize_string( proprios, num=True, apos=True, hyph=True, spaces=True ) notifies[\"proprios\"][ norm_proprios ] = proprios # WIP: proprios = [] + extend() if syndics := get_syndic(pg_txt_body): norm_syndics = normalize_string( syndics, num=True, apos=True, hyph=True, spaces=True ) notifies[\"syndics\"][ norm_syndics ] = syndics # WIP: syndics = [] + extend ? if gests := get_gest(pg_txt_body): norm_gests = normalize_string( gests, num=True, apos=True, hyph=True, spaces=True ) notifies[\"gests\"][norm_gests] = gests # WIP: gests = [] + extend ? # extraire la ou les parcelles vis\u00e9es par l'arr\u00eat\u00e9 if pg_parcelles_str_list := get_parcelles(pg_txt_body): # TODO supprimer les r\u00e9f\u00e9rences partielles (ex: Marseille mais sans code quartier) si la r\u00e9f\u00e9rence compl\u00e8te est aussi pr\u00e9sente dans le doc refcads_norm = [ generate_refcadastrale_norm( codeinsee, pg_parcelles_str, fn_pdf, cpostal ) for pg_parcelles_str in pg_parcelles_str_list ] parcelles = parcelles | OrderedDict( zip(refcads_norm, pg_parcelles_str_list) ) # WIP get_parcelles:list() if False: # WIP hypoth\u00e8ses sur les notifi\u00e9s try: assert len(notifies[\"proprios\"]) <= 1 assert len(notifies[\"syndics\"]) <= 1 assert len(notifies[\"gests\"]) <= 1 except AssertionError: print(f\"{notifies}\") raise # d\u00e9placer le PDF et d\u00e9terminer l'URL if \"codeinsee\" in arretes: if \"date\" in arretes: # r\u00e9-extraire l'ann\u00e9e de la date format\u00e9e # TODO stocker l'ann\u00e9e dans un champ d\u00e9di\u00e9, au moment de l'extraction et normalisation # de la date, et le r\u00e9cup\u00e9rer ici? # code correct: # arr_year = datetime.strptime(arretes[\"date\"], \"%d/%m/%Y\").date().year # mais ne fonctionne pas sur des dates mal reconnues (OCR) ex: \"00/02/2022\" # alors qu'on peut extraire l'ann\u00e9e arr_year = arretes[\"date\"].rsplit(\"/\", 1)[1] arr_comm = arretes[\"codeinsee\"] arretes[\"url\"] = FS_URL.format( commune=arretes[\"codeinsee\"], yyyy=arr_year, pdf=fn_pdf_out ) else: arretes[\"url\"] = FS_URL_NO_YEAR.format( commune=arretes[\"codeinsee\"], pdf=fn_pdf_out ) logging.warning(f\"URL temporaire (sans ann\u00e9e): {arretes['url']}\") else: # (\"codeinsee\" not in arretes) # dans le pire cas: (arretes == {}) arretes = {\"pdf\": fn_pdf, \"url\": FS_URL_FALLBACK.format(pdf=fn_pdf_out)} logging.warning( f\"URL temporaire (sans code commune ni ann\u00e9e): {arretes['url']}\" ) # notifies # formes brutes puis normalis\u00e9es # * propri\u00e9taires id_proprio = list(notifies[\"proprios\"])[0] if notifies[\"proprios\"] else None proprio = id_proprio # TODO appliquer une normalisation? # * syndic id_syndic = list(notifies[\"syndics\"])[0] if notifies[\"syndics\"] else None if ( id_syndic is not None and (P_NOMS_CABINETS.search(id_syndic) is None) and (P_CABINET.search(id_syndic) is None) and (P_MONSIEUR_MADAME.search(id_syndic) is not None) ): # si le champ \"id_syndic\" ne contient pas de mention de cabinet ou d'agence, # et contient une r\u00e9f\u00e9rence \u00e0 une personne physique, # alors la valeur normalis\u00e9e est \"syndic b\u00e9n\u00e9vole\" # TODO si on observe trop de faux positifs, mettre en place une condition # plus restrictive sur la cha\u00eene \"b\u00e9n\u00e9vole\" syndic = \"Syndic b\u00e9n\u00e9vole\" else: # sinon, valeur normalis\u00e9e (fallback: id_syndic) syndic = normalize_nom_cabinet(id_syndic) # * gestionnaire id_gest = list(notifies[\"gests\"])[0] if notifies[\"gests\"] else None # valeur normalis\u00e9e (fallback: id_syndic) gest = normalize_nom_cabinet(id_gest) # doc_data = { \"adresses\": adresses, \"arretes\": [arretes], # a priori un seul par fichier \"notifies\": [ { \"id_proprio\": id_proprio, \"proprio\": proprio, # forme normalis\u00e9e \"id_syndic\": id_syndic, \"syndic\": syndic, # forme normalis\u00e9e \"id_gest\": id_gest, \"gest\": gest, # forme normalis\u00e9e \"codeinsee\": codeinsee, } ], # a priori un seul par fichier (pour le moment) \"parcelles\": [{\"ref_cad\": x, \"codeinsee\": codeinsee} for x in parcelles], } return doc_data","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.process_files","text":"Analyse le texte des fichiers PDF extrait dans des fichiers TXT.","title":"process_files()"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.process_files--parameters","text":"df_in: pd.DataFrame Fichier meta_$RUN_otxt.csv contenant les m\u00e9tadonn\u00e9es enrichies et les fichiers PDF et TXT (natif ou OCR) \u00e0 traiter. out_dir : Path Dossier de sortie date_exec : date Date d'ex\u00e9cution du script, utilis\u00e9e pour (a) le nom des copies de fichiers CSV incluant la date de traitement, (b) l'identifiant unique des arr\u00eat\u00e9s dans les 4 tables, (c) le champ 'datemaj' initialement rempli avec la date d'ex\u00e9cution.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc_direct.process_files--returns","text":"out_files : Dict[str, Path] Fichiers CSV produits, contenant les donn\u00e9es extraites. Dictionnaire index\u00e9 par les cl\u00e9s {\"adresse\", \"arrete\", \"notifie\", \"parcelle\"}. src\\process\\parse_doc_direct.py def process_files( df_in: pd.DataFrame, out_dir: Path, date_exec: date, ) -> Dict[str, Path]: \"\"\"Analyse le texte des fichiers PDF extrait dans des fichiers TXT. Parameters ---------- df_in: pd.DataFrame Fichier meta_$RUN_otxt.csv contenant les m\u00e9tadonn\u00e9es enrichies et les fichiers PDF et TXT (natif ou OCR) \u00e0 traiter. out_dir : Path Dossier de sortie date_exec : date Date d'ex\u00e9cution du script, utilis\u00e9e pour (a) le nom des copies de fichiers CSV incluant la date de traitement, (b) l'identifiant unique des arr\u00eat\u00e9s dans les 4 tables, (c) le champ 'datemaj' initialement rempli avec la date d'ex\u00e9cution. Returns ------- out_files : Dict[str, Path] Fichiers CSV produits, contenant les donn\u00e9es extraites. Dictionnaire index\u00e9 par les cl\u00e9s {\"adresse\", \"arrete\", \"notifie\", \"parcelle\"}. \"\"\" # - les fichiers CSV dat\u00e9s sont stock\u00e9s dans un sous-dossier \"csv_historique\" out_dir_csv = out_dir / \"csv_historique\" logging.info( f\"Sous-dossier de sortie: {out_dir_csv} {'existe d\u00e9j\u00e0' if out_dir_csv.is_dir() else 'va \u00eatre cr\u00e9\u00e9'}.\" ) out_dir_csv.mkdir(parents=True, exist_ok=True) # - les fichiers PDF \u00e0 reclasser sont stock\u00e9s dans un sous-dossier (temporaire) \"pdf_a_reclasser\" out_dir_pdf_areclass = out_dir / \"pdf_analyses/pdf_a_reclasser\" logging.info( f\"Sous-dossier de sortie: {out_dir_pdf_areclass} {'existe d\u00e9j\u00e0' if out_dir_pdf_areclass.is_dir() else 'va \u00eatre cr\u00e9\u00e9'}.\" ) out_dir_pdf_areclass.mkdir(parents=True, exist_ok=True) # - les fichiers TXT extraits nativement ou par OCR dans un sous-dossier \"txt\" out_dir_txt = out_dir / \"txt\" logging.info( f\"Sous-dossier de sortie: {out_dir_txt} {'existe d\u00e9j\u00e0' if out_dir_txt.is_dir() else 'va \u00eatre cr\u00e9\u00e9'}.\" ) out_dir_txt.mkdir(parents=True, exist_ok=True) # 0. charger la liste des PDF d\u00e9j\u00e0 trait\u00e9s, d\u00e9finis comme les PDF d\u00e9j\u00e0 # pr\u00e9sents dans un des fichiers \"paquet_arrete_*.csv\" fps_paquet_arrete = sorted( out_dir_csv.glob( f\"paquet_arrete_[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]_[0-9][0-9].csv\" ) ) pdfs_old = [] for fp_paquet_arrete in fps_paquet_arrete: df_arr_old = pd.read_csv(fp_paquet_arrete, dtype=DTYPE_ARRETE, sep=\";\") pdfs_old.extend(df_arr_old[\"pdf\"]) pdfs_old = set(pdfs_old) # 1. d\u00e9terminer le nom des fichiers de sortie # les noms des fichiers de sortie incluent: # - la date de traitement (ex: \"2023-05-30\") date_proc_dash = date_exec.strftime(\"%Y-%m-%d\") # - le num\u00e9ro d'ex\u00e9cution ce jour (ex: \"02\"), calcul\u00e9 en recensant les # \u00e9ventuels fichiers existants out_prevruns = sorted( itertools.chain.from_iterable( out_dir_csv.glob(f\"paquet_{x}_{date_proc_dash}_[0-9][0-9].csv\") for x in OUT_BASENAMES ) ) # - num\u00e9ro d'ex\u00e9cution du script ce jour i_run = 0 # init for fp_prevrun in out_prevruns: # le num\u00e9ro se trouve \u00e0 la fin du stem, apr\u00e8s le dernier s\u00e9parateur \"_\" fp_out_idx = int(fp_prevrun.stem.split(\"_\")[-1]) i_run = max(i_run, fp_out_idx) i_run += 1 # on prend le num\u00e9ro d'ex\u00e9cution suivant # r\u00e9sultat: fichiers g\u00e9n\u00e9r\u00e9s par cette ex\u00e9cution out_files = { x: out_dir_csv / f\"paquet_{x}_{date_proc_dash}_{i_run:>02}.csv\" for x in OUT_BASENAMES } # 2. d\u00e9terminer le premier identifiant unique (idu) des prochaines entr\u00e9es: # il suit le dernier idu g\u00e9n\u00e9r\u00e9 par les ex\u00e9cutions pr\u00e9c\u00e9dentes le m\u00eame jour i_idu = 0 # init for fp_prevrun in out_prevruns: # ouvrir le fichier, lire les idus, prendre le dernier, extraire l'index s_idus = pd.read_csv( fp_prevrun, usecols=[\"idu\"], dtype={\"idu\": \"string\"}, sep=\";\" )[\"idu\"] max_idx = s_idus.str.rsplit(\"-\", n=1, expand=True)[1].astype(\"int32\").max() i_idu = max(i_idu, max_idx) i_idu += 1 # on prend le num\u00e9ro d'arr\u00eat\u00e9 suivant # 3. filtrer les arr\u00eat\u00e9s # - filtrer les documents hors p\u00e9rim\u00e8tre th\u00e9matique ou g\u00e9ographique ? # TODO v\u00e9rifier si ok sans liste d'exclusion ici ; sinon corriger avant d\u00e9ploiement? # df_in[\"pdf\"].str.split(\"-\", 1)[0] not in set(EXCLUDE_FILES + EXCLUDE_FIXME_FILES) # + EXCLUDE_HORS_AMP) # # - filtrer les fichiers d\u00e9j\u00e0 trait\u00e9s: ne garder que les PDF qui ne # sont pas d\u00e9j\u00e0 pr\u00e9sents dans un \"paquet_arrete_*.csv\" pdfs_in_old = set(df_in[\"pdf\"].tolist()).intersection(pdfs_old) # v\u00e9rifier que le fichier PDF existe bien dans les dossiers destination, # pdf_a_reclasser ou un dossier de commune, # sinon il faut le traiter comme s'il \u00e9tait compl\u00e8tement nouveau already_proc = [] # sous-dossiers par code commune (INSEE), sur 5 chiffres out_dir_pdf_communes = out_dir / \"[0-9][0-9][0-9][0-9][0-9]\" for fn in pdfs_in_old: areclass = sorted(out_dir_pdf_areclass.rglob(fn)) bienclas = sorted(out_dir_pdf_communes.rglob(fn)) if areclass or bienclas: logging.warning( f\"Fichier \u00e0 ignorer car d\u00e9j\u00e0 trait\u00e9: {areclass[0] if areclass else bienclas[0]}\" ) already_proc.append(fn) already_proc = set(already_proc) # s_dups = df_in[\"pdf\"].isin(already_proc) if any(s_dups): logging.info( f\"{s_dups.sum()} fichiers seront d\u00e9plac\u00e9s dans 'doublons/'\" + \" et ne seront pas retrait\u00e9s, car ils sont d\u00e9j\u00e0 pr\u00e9sents\" + \" dans un fichier 'paquet_arrete_*.csv' de 'csv_historique/'\" + \" et dans un dossier de commune\" + \" ou 'pdf_a_reclasser' .\" ) # d\u00e9placer les fichiers d\u00e9j\u00e0 trait\u00e9s dans doublons/ # (plus prudent que de les supprimer d'embl\u00e9e) out_dups = out_dir / \"doublons\" out_dups.mkdir(exist_ok=True) # df_dups = df_in[s_dups] for df_row in df_dups.itertuples(): fp = Path(df_row.fullpath) fp_dst = out_dups / fp.name shutil.move(fp, fp_dst) # si le move a r\u00e9ussi, on peut supprimer le fichier dans le dossier d'entr\u00e9e if fp_dst.is_file(): fp_orig = Path(df_row.origpath) fp_orig.unlink() # df_in = df_in[~s_dups] # si apr\u00e8s filtrage, df_in est vide, aucun fichier CSV ne sera produit # et on peut sortir imm\u00e9diatement if df_in.empty: return {} # 4 tables de sortie rows_adresse = [] rows_arrete = [] rows_notifie = [] rows_parcelle = [] # date de traitement, en 2 formats date_proc = date_exec.strftime(\"%Y%m%d\") # pour \"idu\" (id uniques des arr\u00eat\u00e9s) datemaj = date_exec.strftime(\"%d/%m/%Y\") # pour \"datemaj\" des 4 tables # identifiant des entr\u00e9es dans les fichiers de sortie: <type arr\u00eat\u00e9>-<date du traitement>-<index> # it\u00e9rer sur les fichiers PDF et TXT for i, df_row in enumerate(df_in.itertuples(), start=i_idu): # fichier PDF fp_pdf = Path(df_row.fullpath) if not fp_pdf.is_file(): raise ValueError(f\"{fp_pdf}: fichier PDF introuvable ({fp_txt})\") # fichier TXT (OCR sinon natif) fp_txt = Path(df_row.fullpath_txt) if not fp_txt.is_file(): raise ValueError(f\"{fp_pdf}: fichier TXT introuvable ({fp_txt})\") # type d'arr\u00eat\u00e9 ; \u00e0 date, seulement des arr\u00eat\u00e9s de p\u00e9ril \"AP\" ; # \u00e0 l'avenir, pourrait \u00eatre pr\u00e9dit \u00e0 partir du texte, avec un classifieur type_arr = \"AP\" # identifiant unique du document dans les tables de sortie (paquet_*.csv): # TODO d\u00e9tecter le ou les \u00e9ventuels fichiers d\u00e9j\u00e0 produits ce jour, \u00e9carter les doublons (blake2b?) # et initialiser le compteur \u00e0 la prochaine valeur # format: {type d'arr\u00eat\u00e9}-{date}-{id relatif, sur 4 chiffres} idu = f\"{type_arr}-{date_proc}-{i:04}\" # analyser le texte doc_data = parse_arrete(fp_pdf, fp_txt) # ajouter des entr\u00e9es dans les 4 tables rows_adresse.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"adresses\"] ) rows_arrete.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"arretes\"] ) rows_notifie.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"notifies\"] ) rows_parcelle.extend( ({\"idu\": idu} | x | {\"datemaj\": datemaj}) for x in doc_data[\"parcelles\"] ) # cr\u00e9er les 4 DataFrames et les exporter en CSV for key, rows, dtype in [ (\"adresse\", rows_adresse, DTYPE_ADRESSE), (\"arrete\", rows_arrete, DTYPE_ARRETE), (\"notifie\", rows_notifie, DTYPE_NOTIFIE), (\"parcelle\", rows_parcelle, DTYPE_PARCELLE), ]: out_file = out_files[key] df = pd.DataFrame.from_records(rows) for dtype_key, _ in dtype.items(): if dtype_key not in df.columns: df[dtype_key] = np.nan df = df.astype(dtype=dtype) df.to_csv(out_file, index=False, sep=\";\") # d\u00e9placer les fichiers PDF trait\u00e9s ; # le code est redondant avec celui utilis\u00e9 pour remplir le champ d'URL # mais on fait les d\u00e9placements de fichiers apr\u00e8s l'\u00e9criture du dataframe # pour \u00e9viter de d\u00e9placer le fichier si les CSV ne sont finalement pas # produits (eg. \u00e0 cause d'un \u00e9chec sur un autre document) df_arr = pd.read_csv(out_files[\"arrete\"], dtype=DTYPE_ARRETE, sep=\";\") for df_row in df_arr.itertuples(): # nom du PDF (incluant hash) fn = df_row.pdf # d\u00e9terminer le dossier destination if pd.notna(df_row.codeinsee): commune = df_row.codeinsee if pd.notna(df_row.date): # code correct # year = str(datetime.strptime(df_row.date, \"%d/%m/%Y\").date().year) # mais ne fonctionne pas sur des dates mal reconnues (OCR) ex: \"00/02/2022\" # alors qu'on peut extraire l'ann\u00e9e year = df_row.date.rsplit(\"/\", 1)[1] dest_dir = out_dir / \"pdf_analyses\" / commune / year else: dest_dir = out_dir / \"pdf_analyses/pdf_a_reclasser\" / commune else: dest_dir = out_dir / \"pdf_analyses/pdf_a_reclasser\" # cr\u00e9er le dossier destination si besoin dest_dir.mkdir(parents=True, exist_ok=True) # retrouver l'entr\u00e9e correspondance dans df_in, pour avoir le # chemin complet de sa copie (\u00e0 d\u00e9placer) et du fichier original # (\u00e0 supprimer) # .head(1) car normalement il y a *exactement une* entr\u00e9e correspondante # et .itertuples() pour avoir facilement un namedtuple for df_row_in in df_in.loc[df_in[\"pdf\"] == fn].head(1).itertuples(): # chemin du fichier trait\u00e9 (copi\u00e9 depuis dir_in vers le dossier de travail) fp = Path(df_row_in.fullpath) # chemin du fichier d'origine (pour suppression apr\u00e8s move) fp_orig = Path(df_row_in.origpath) # chemin destination du fichier trait\u00e9 print(fp.name) fp_dst = dest_dir / create_file_name_url(fp.name) print(fp_dst) print() shutil.move(fp, fp_dst) # si le move a r\u00e9ussi, on peut supprimer le fichier dans le dossier d'entr\u00e9e if fp_dst.is_file(): fp_orig.unlink() # chemin du fichier TXT (OCR sinon natif) fp_txt = Path(df_row_in.fullpath_txt) shutil.copy2(fp_txt, out_dir_txt / fp_txt.name) # faire une copie des 4 fichiers g\u00e9n\u00e9r\u00e9s avec les noms de base (\u00e9craser chaque fichier # pr\u00e9-existant ayant le nom de base) for fp_out in out_files.values(): # retirer la date et le num\u00e9ro d'ex\u00e9cution pour retrouver le nom de base fp_copy = ( out_dir / fp_out.with_stem(f\"{fp_out.stem.rsplit('_', maxsplit=2)[0]}\").name ) shutil.copy2(fp_out, fp_copy) return out_files","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc--analyse-le-document-dans-son-ensemble","text":"Extrait des empans de texte correspondant aux en-t\u00eates, pieds-de-page, autorit\u00e9, vus, correspondants, articles, signature...","title":"Analyse le document dans son ensemble."},{"location":"Code%20Source/process/#src.process.parse_doc.has_one","text":"D\u00e9tecte si la liste contient au moins un empan d'un type donn\u00e9. Si la liste est vide, renvoie None.","title":"has_one()"},{"location":"Code%20Source/process/#src.process.parse_doc.has_one--parameters","text":"spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.has_one--returns","text":"has_span: boolean True si au moins un empan de la liste est du type recherch\u00e9. src\\process\\parse_doc.py def has_one(spans: list[dict], span_typ: str) -> str: \"\"\"D\u00e9tecte si la liste contient au moins un empan d'un type donn\u00e9. Si la liste est vide, renvoie None. Parameters ---------- spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9 Returns ------- has_span: boolean True si au moins un empan de la liste est du type recherch\u00e9. \"\"\" if not spans: return None return any(x for x in spans if x[\"span_typ\"] == span_typ)","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_arrete_pages","text":"Analyse les pages de texte d'un arr\u00eat\u00e9.","title":"parse_arrete_pages()"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_arrete_pages--parameters","text":"fn_pdf: str Nom du fichier PDF. pages: list[str] Liste de pages de texte \u00e0 analyser.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_arrete_pages--returns","text":"doc_content: list[dict] Contenu du document, par page d\u00e9coup\u00e9e en zones de texte. src\\process\\parse_doc.py def parse_arrete_pages(fn_pdf: str, pages: list[str]) -> list: \"\"\"Analyse les pages de texte d'un arr\u00eat\u00e9. Parameters ---------- fn_pdf: str Nom du fichier PDF. pages: list[str] Liste de pages de texte \u00e0 analyser. Returns ------- doc_content: list[dict] Contenu du document, par page d\u00e9coup\u00e9e en zones de texte. \"\"\" doc_content = [] # valeur de retour # FIXME on ne traite pas une poign\u00e9e de documents qui posent diff\u00e9rents probl\u00e8mes if fn_pdf in EXCLUDE_SET: return doc_content # end FIXME # m\u00e9tadonn\u00e9es du document mdata_doc = { \"pdf\": fn_pdf, } # print(fn_pdf) # DEBUG # traiter les pages # TODO \u00e9tats alternatifs? [\"preambule\", \"vu\", \"considerant\", \"arrete\", \"article\", \"postambule\" ou \"signature\", \"apres_signature\" ou \"annexes\"] ? cur_state = \"avant_vucons\" # init ; \"avant_vucons\" < \"avant_articles\" < \"avant_signature\" # TODO ajouter \"avant_considerant\" ? latest_span = None # init for i, page in enumerate(pages, start=1): # m\u00e9tadonn\u00e9es de la page mdata_page = mdata_doc | {\"page_num\": i} if pd.isna(page): # * la page n'a pas de texte page_content = mdata_page | { \"template\": None, # empans de template \"body\": None, # texte (sans le texte du template) \"content\": None, # empans de contenu (paragraphes et donn\u00e9es): vide } doc_content.append(page_content) continue # NEW normalisation du texte # spaces=False sinon on perd les retours \u00e0 la ligne ! page = normalize_string(page, num=True, apos=True, hyph=True, spaces=False) # end NEW # rep\u00e9rer et effacer les \u00e9l\u00e9ments de template, pour ne garder que le contenu de chaque page pg_template, pg_txt_body = parse_page_template(page) pg_content = [] # initialisation de la liste des \u00e9l\u00e9ments de contenu # d\u00e9tecter et traiter sp\u00e9cifiquement les pages vides, de bordereau ou d'annexes if pg_txt_body.strip() == \"\": # * la page est vide de texte (hors template), donc aucun empan de contenu ne pourra \u00eatre reconnu page_content = mdata_page | { \"template\": pg_template, # empans de template \"body\": pg_txt_body, # texte (sans le texte du template) \"content\": pg_content, # empans de contenu (paragraphes et donn\u00e9es): vide } doc_content.append(page_content) continue elif P_BORDEREAU.search(pg_txt_body): # * page de bordereau de formalit\u00e9s (Aix-en-Provence) # TODO extraire le contenu (date de l'acte, num\u00e9ro, titre) pour v\u00e9rifier la correction des donn\u00e9es extraites ailleurs? page_content = mdata_page | { \"template\": pg_template, # empans de template \"body\": pg_txt_body, # texte (sans le texte du template) \"content\": pg_content, # empans de contenu (paragraphes et donn\u00e9es): vide } doc_content.append(page_content) continue # TODO pages d'annexe # TODO si la signature est d\u00e9j\u00e0 pass\u00e9e, on peut consid\u00e9rer que le document est termin\u00e9 et stopper tout le traitement? => ajouter un \u00e9tat cur_state == \"apres_signature\" ? # NB: certains fichiers PDF contiennent un arr\u00eat\u00e9 modificatif puis l'arr\u00eat\u00e9 d'origine (ex: \"modif 39 rue Tapis Vert 13001.pdf\"), on ignore le 2e ? # la page n'est pas vide de texte main_end = len(pg_txt_body) # 1. pr\u00e9ambule du document: avant le 1er \"Vu\", contient la commune, l'autorit\u00e9 prenant l'arr\u00eat\u00e9, parfois le num\u00e9ro de l'arr\u00eat\u00e9 if cur_state == \"avant_vucons\": fst_vucons = [] if fst_vu := P_VU.search(pg_txt_body): fst_vucons.append(fst_vu) if fst_cons := P_CONSIDERANT.search(pg_txt_body): fst_vucons.append(fst_cons) if fst_vucons: fst_vu_or_cons = sorted(fst_vucons, key=lambda x: x.start())[0] pream_beg = 0 pream_end = fst_vu_or_cons.start() pream_content = parse_doc_preamble( fn_pdf, pg_txt_body, pream_beg, pream_end ) pg_content.extend(pream_content) if pream_content: latest_span = None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible cur_state = \"avant_articles\" else: # la 1re page ne contient ni \"vu\" ni \"consid\u00e9rant\", ce doit \u00eatre une page de courrier # ex: \"21, rue Martinot Aubagne.pdf\" logging.warning( f\"{fn_pdf}: page {i}: ni 'vu' ni 'consid\u00e9rant' donc page ignor\u00e9e\" ) continue main_beg = pream_end else: # p. 2 et suivantes: la zone \u00e0 analyser commence en haut de la page (les \u00e9l\u00e9ments de # template ayant \u00e9t\u00e9 effac\u00e9s au pr\u00e9alable) main_beg = 0 # TODO si tout le texte a d\u00e9j\u00e0 \u00e9t\u00e9 reconnu, ajouter le contenu de la page au doc et passer \u00e0 la page suivante # 2. les \"vu\" et \"consid\u00e9rant\" if cur_state == \"avant_articles\": vucons_beg = main_beg # la page contient-elle un \"Article\" ? (le 1er) if m_article := P_ARTICLE.search(pg_txt_body, main_beg): # si oui, les \"Vu\" et \"Consid\u00e9rant\" de cette page, puis \"Arr\u00eate\", # sont \u00e0 chercher avant le 1er \"Article\" # print(f\"m_article={m_article}\") # DEBUG vucons_end = m_article.start() else: # si non, les \"Vu\" et \"Consid\u00e9rant\" sont sur toute la page vucons_end = main_end # rep\u00e9rer les \"Vu\" et \"Consid\u00e9rant\", et \"Arr\u00eate\" si pr\u00e9sent # print(f\"avant parse_page_content/Vucons: pg_content={pg_content}\") # DEBUG vucons_content = parse_page_content( pg_txt_body, vucons_beg, vucons_end, cur_state, latest_span ) # FIXME sp\u00e9cialiser la fonction pour restreindre aux \"Vu\" et \"Consid\u00e9rant\" et/ou passer cur_state? ; NB: ces deux types de paragraphes admettent des continuations pg_content.extend(vucons_content) # print(f\"apr\u00e8s parse_page_content/Vucons: pg_content={pg_content}\") # DEBUG if vucons_content: latest_span = ( None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible ) # si \"Arr\u00eate\" \u00e9tait bien sur la page, il faut ajouter l'empan reconnu, d\u00e9placer le curseur et changer d'\u00e9tat if pg_content: spans_arrete = [x for x in pg_content if x[\"span_typ\"] == \"par_arrete\"] if spans_arrete: assert len(spans_arrete) == 1 span_arrete = spans_arrete[0] # main_beg = span_arrete[\"span_end\"] latest_span = None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible cur_state = \"avant_signature\" # WIP 2023-05-09 else: logging.warning(f\"{fn_pdf} / {i}: parse_doc: pas de 'par_arrete'\") # end WIP 2023-05-09 # TODO si tout le texte a d\u00e9j\u00e0 \u00e9t\u00e9 reconnu, ajouter le contenu de la page au doc et passer \u00e0 la page suivante # TODO d\u00e9tecter la signature m\u00eame si \"Arr\u00eate\" n'a pas \u00e9t\u00e9 d\u00e9tect\u00e9 (simplification code + am\u00e9lioration robustesse?) # 3. les \"article\" et le postambule if cur_state == \"avant_signature\": # le corps du document s'arr\u00eate \u00e0 la signature ou la date de prise de l'arr\u00eat\u00e9 # FIXME attraper le 1er qui appara\u00eet: date de signature ou signataire artic_beg = main_beg if m_sign := P_DATE_SIGNAT.search(pg_txt_body, main_beg): # si la page contient la signature de fin de l'acte, l'analyse du contenu # principal doit s'arr\u00eater \u00e0 la signature (ici avec date) artic_end = m_sign.start() elif m_sign := P_LIEU_SIGNAT.search(pg_txt_body, main_beg): # si la page contient la signature de fin de l'acte, l'analyse du contenu # principal doit s'arr\u00eater \u00e0 la signature (ici avec lieu seul, date absente # ou non-reconnue) artic_end = m_sign.start() else: artic_end = main_end # rep\u00e9rer les articles # print(f\"avant parse_page_content/Articles: pg_content={pg_content}\") # DEBUG try: artic_content = parse_page_content( pg_txt_body, artic_beg, artic_end, cur_state, latest_span ) # FIXME sp\u00e9cialiser la fonction pour restreindre aux \"Vu\" et \"Consid\u00e9rant\" et/ou passer cur_state? ; NB: ces deux types de paragraphes admettent des continuations except TypeError: print(f\"Fichier fautif: {fn_pdf}, p. {i}\") raise pg_content.extend(artic_content) if artic_content: latest_span = ( None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible ) if m_sign: # analyser le postambule et changer l'\u00e9tat posta_beg = m_sign.start() posta_end = main_end posta_content = parse_doc_postamble(pg_txt_body, posta_beg, posta_end) pg_content.extend(posta_content) if posta_content: latest_span = None # le dernier empan de la page pr\u00e9c\u00e9dente n'est plus disponible cur_state = \"apres_signature\" logging.warning(f\"{fn_pdf}: parse_doc: apr\u00e8s m_sign\") # DEBUG # TODO si tout le texte a d\u00e9j\u00e0 \u00e9t\u00e9 reconnu, ajouter le contenu de la page au doc et passer \u00e0 la page suivante if cur_state == \"apres_signature\": pass # FIXME faire quelque chose? v\u00e9rifier s'il reste du texte? # r\u00e9cup\u00e9rer le dernier paragraphe de la page, car il peut \u00eatre continu\u00e9 # en d\u00e9but de page suivante if pg_content: try: latest_span = [ x for x in pg_content if x[\"span_typ\"].startswith(\"par_\") ][-1] except IndexError: print( f\"{fn_pdf} / p.{i} : pas de paragraphe sur l'empan {main_beg}:{main_end}\\ncontenu:{pg_content}\\ntexte:\\n{pg_txt_body}\" ) raise # accumulation au niveau du document page_content = mdata_page | { \"template\": pg_template, # empans de template \"body\": pg_txt_body, # texte (sans le texte du template) \"content\": pg_content, # empans de contenu (paragraphes et donn\u00e9es) } doc_content.append(page_content) if False: # DEBUG print(\"<<<<<<<<<<<<<<<<\") print(pg_content) # DEBUG print(\"----------------\") print(pg_txt_body) # DEBUG print(\"~~~~~~~~~~~~~~~~\") print(pg_txt_body[main_beg:main_end]) # DEBUG print(\"================\") # TODO arr\u00eater le traitement \u00e0 la fin du postambule et tronquer le texte / le PDF si possible? (utile pour l'OCR) # v\u00e9rifier que le r\u00e9sultat est bien form\u00e9 examine_doc_content(fn_pdf, doc_content) # return doc_content","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_doc_postamble","text":"Analyse le postambule d'un document, sur la derni\u00e8re page (hors annexes). Le postambule correspond \u00e0 la zone de signature: date, lieu \u00e9ventuel et signataire.","title":"parse_doc_postamble()"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_doc_postamble--parameters","text":"txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\".","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_doc_postamble--returns","text":"content: list Liste d'empans de contenu src\\process\\parse_doc.py def parse_doc_postamble(txt_body: str, pream_beg: int, pream_end: int) -> list[dict]: \"\"\"Analyse le postambule d'un document, sur la derni\u00e8re page (hors annexes). Le postambule correspond \u00e0 la zone de signature: date, lieu \u00e9ventuel et signataire. Parameters ---------- txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\". Returns ------- content: list Liste d'empans de contenu \"\"\" content = [] # a. extraire la date de signature if m_signature := P_DATE_SIGNAT.search(txt_body, pream_beg, pream_end): logging.warning(f\"parse_doc_postamble: signature: {m_signature}\") # stocker la zone reconnue content.append( { \"span_beg\": m_signature.start(), \"span_end\": m_signature.end(), \"span_txt\": m_signature.group(0), \"span_typ\": \"par_sign_date\", } ) # stocker la donn\u00e9e content.append( { \"span_beg\": m_signature.start(\"arr_date\"), \"span_end\": m_signature.end(\"arr_date\"), \"span_txt\": m_signature.group(\"arr_date\"), \"span_typ\": \"arr_date\", } ) # b. extraire la ville de signature if m_signature.group(\"arr_ville_signat\"): # stocker la donn\u00e9e content.append( { \"span_beg\": m_signature.start(\"arr_ville_signat\"), \"span_end\": m_signature.end(\"arr_ville_signat\"), \"span_txt\": m_signature.group(\"arr_ville_signat\"), \"span_typ\": \"adr_ville\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) elif m_signature := P_LIEU_SIGNAT.search(txt_body, pream_beg, pream_end): logging.warning(f\"parse_doc_postamble: signature (lieu): {m_signature}\") # stocker la zone reconnue content.append( { \"span_beg\": m_signature.start(), \"span_end\": m_signature.end(), \"span_txt\": m_signature.group(0), \"span_typ\": \"par_sign_lieu\", } ) # b. extraire la ville de signature # TODO ne capture pas toutes les villes (probl\u00e8me de named group avec contexte trop diff\u00e9rent) if m_signature.group(\"arr_ville_signat\"): # stocker la donn\u00e9e content.append( { \"span_beg\": m_signature.start(\"arr_ville_signat\"), \"span_end\": m_signature.end(\"arr_ville_signat\"), \"span_txt\": m_signature.group(\"arr_ville_signat\"), \"span_typ\": \"adr_ville\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) # TODO c. extraire l'identit\u00e9 et la qualit\u00e9 du signataire? (eg. d\u00e9l\u00e9gation de signature) # else: logging.warning( f\"parse_doc_postamble: aucune signature ? {txt_body[pream_beg:pream_end]}\" ) return content","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_doc_preamble","text":"Analyse le pr\u00e9ambule d'un document, sur la 1re page, avant le 1er \"Vu\".","title":"parse_doc_preamble()"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_doc_preamble--parameters","text":"fn_pdf: string Nom du fichier PDF txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\".","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_doc_preamble--returns","text":"content: list Liste d'empans de contenu src\\process\\parse_doc.py def parse_doc_preamble( fn_pdf: str, txt_body: str, pream_beg: int, pream_end: int ) -> list[dict]: \"\"\"Analyse le pr\u00e9ambule d'un document, sur la 1re page, avant le 1er \"Vu\". Parameters ---------- fn_pdf: string Nom du fichier PDF txt_body: string Corps de texte de la page \u00e0 analyser pream_beg: int D\u00e9but de l'empan \u00e0 analyser. pream_end: int Fin de l'empan \u00e0 analyser, correspondant au d\u00e9but du 1er \"Vu\". Returns ------- content: list Liste d'empans de contenu \"\"\" content = [] rem_txt = \"\" # s'il reste du texte apr\u00e8s l'autorit\u00e9 (WIP) # cr\u00e9er une copie du texte du pr\u00e9ambule, de m\u00eame longueur que le texte complet pour que les empans soient bien positionn\u00e9s # le texte sera effac\u00e9 au fur et \u00e0 mesure qu'il sera \"consomm\u00e9\" txt_copy = txt_body[:] txt_copy = ( \" \" * (pream_beg - 0) + txt_copy[pream_beg:pream_end] + \" \" * (len(txt_copy) - pream_end) ) assert len(txt_copy) == len(txt_body) # a. ce pr\u00e9ambule contient (vers la fin) l'intitul\u00e9 de l'autorit\u00e9 prenant l'arr\u00eat\u00e9 # TODO est-ce obligatoire? exceptions: La Ciotat if matches := list(P_MAIRE_COMMUNE.finditer(txt_copy, pream_beg, pream_end)): # on garde la premi\u00e8re occurrence, normalement la seule match = matches[0] # * toute la zone reconnue span_beg, span_end = match.span() content.append( { \"span_beg\": span_beg, \"span_end\": span_end, \"span_txt\": match.group(0), \"span_typ\": \"par_autorite\", } ) if match.group(\"commune\"): # * stocker la donn\u00e9e de la commune content.append( { \"span_beg\": match.start(\"commune\"), \"span_end\": match.end(\"commune\"), \"span_txt\": match.group(\"commune\"), \"span_typ\": \"adr_ville\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) # * effacer l'empan reconnu txt_copy = ( txt_copy[:span_beg] + \" \" * (span_end - span_beg) + txt_copy[span_end:] ) # la ou les \u00e9ventuelles autres occurrences sont des doublons if len(matches) > 1: logging.warning( f\"{fn_pdf}: > 1 mention d'autorit\u00e9 trouv\u00e9e dans le pr\u00e9ambule: {matches}\" ) for match_dup in matches[1:]: # toute la zone reconnue span_dup_beg, span_dup_end = match_dup.span() content.append( { \"span_beg\": span_dup_beg, \"span_end\": span_dup_end, \"span_txt\": match_dup.group(0), \"span_typ\": \"par_autorite_dup\", } ) if match.group(\"commune\"): # stocker la donn\u00e9e de la commune content.append( { \"span_beg\": match_dup.start(\"commune\"), \"span_end\": match_dup.end(\"commune\"), \"span_txt\": match_dup.group(\"commune\"), \"span_typ\": \"adr_ville_dup\", # TODO utiliser un autre nom pour \u00e9viter le conflit? } ) # effacer l'empan reconnu txt_copy = ( txt_copy[:span_dup_beg] + \" \" * (span_dup_end - span_dup_beg) + txt_copy[span_dup_end:] ) # v\u00e9rifier que la zone de l'autorit\u00e9 est bien en fin de pr\u00e9ambule try: rem_txt = txt_copy[span_end:pream_end].strip() assert rem_txt == \"\" except AssertionError: logging.warning( f\"{fn_pdf}: Texte apr\u00e8s l'autorit\u00e9, en fin de pr\u00e9ambule: {rem_txt}\" ) if len(rem_txt) < 2: # s'il ne reste qu'un caract\u00e8re, c'est probablement une typo => avertir et effacer logging.warning( f\"{fn_pdf}: Ignorer le fragment de texte en fin de pr\u00e9ambule, probablement une typo: {rem_txt}\" ) txt_copy = ( txt_copy[:span_end] + \" \" * (pream_end - span_end) + txt_copy[pream_end:] ) else: # pas d'autorit\u00e9 d\u00e9tect\u00e9e: anormal logging.warning(f\"{fn_pdf}: pas d'autorit\u00e9 d\u00e9tect\u00e9e dans le pr\u00e9ambule\") # b. ce pr\u00e9ambule peut contenir le num\u00e9ro de l'arr\u00eat\u00e9 (si pr\u00e9sent, absent dans certaines communes) # NB: ce num\u00e9ro d'arr\u00eat\u00e9 peut se trouver avant ou apr\u00e8s l'autorit\u00e9 (ex: Gardanne) match = P_NUM_ARR.search(txt_copy, pream_beg, pream_end) if match is None: # si la capture pr\u00e9cise \u00e9choue, utiliser une capture plus permissive (mais risque d'attrape-tout) match = P_NUM_ARR_FALLBACK.search(txt_copy, pream_beg, pream_end) if match is not None: # marquer toute la zone reconnue (contexte + num\u00e9ro de l'arr\u00eat\u00e9) span_beg, span_end = match.span() content.append( { \"span_beg\": span_beg, \"span_end\": span_end, \"span_txt\": match.group(0), \"span_typ\": \"par_num_arr\", # paragraphe contenant le num\u00e9ro de l'arr\u00eat\u00e9 } ) # stocker le num\u00e9ro de l'arr\u00eat\u00e9 content.append( { \"span_beg\": match.start(\"num_arr\"), \"span_end\": match.end(\"num_arr\"), \"span_txt\": match.group(\"num_arr\"), \"span_typ\": \"num_arr\", } ) # effacer le texte reconnu txt_copy = ( txt_copy[:span_beg] + \" \" * (span_end - span_beg) + txt_copy[span_end:] ) # print(f\"num arr: {content[-1]['span_txt']}\") # DEBUG else: # pas de num\u00e9ro d'arr\u00eat\u00e9 (ex: Aubagne) logging.warning( f\"{fn_pdf}: Pas de num\u00e9ro d'arr\u00eat\u00e9 trouv\u00e9: \" + '\"' + txt_copy[pream_beg:pream_end].replace(\"\\n\", \" \").strip() + '\"' ) pass # c. entre les deux doit se trouver le titre ou objet de l'arr\u00eat\u00e9 (obligatoire) if match := P_NOM_ARR.search(txt_copy, pream_beg, pream_end): span_beg, span_end = match.span() # stocker la zone reconnue content.append( { \"span_beg\": span_beg, \"span_end\": span_end, \"span_txt\": match.group(0), \"span_typ\": \"par_nom_arr\", } ) # stocker la donn\u00e9e content.append( { \"span_beg\": match.start(\"nom_arr\"), \"span_end\": match.end(\"nom_arr\"), \"span_txt\": match.group(\"nom_arr\"), \"span_typ\": \"nom_arr\", } ) # effacer l'empan reconnu txt_copy = ( txt_copy[:span_beg] + \" \" * (span_end - span_beg) + txt_copy[span_end:] ) else: # hypoth\u00e8se: sans marquage explicite comme \"Objet:\", le titre est tout le texte restant # dans cette zone (entre le num\u00e9ro et l'autorit\u00e9) if (not P_LINE.fullmatch(txt_copy, pream_beg, pream_end)) and ( match := P_STRIP.fullmatch(txt_copy, pream_beg, pream_end) ): # stocker la zone reconnue content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": \"par_nom_arr\", } ) # stocker la donn\u00e9e content.append( { \"span_beg\": match.start(\"outstrip\"), \"span_end\": match.end(\"outstrip\"), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": \"nom_arr\", } ) else: logging.warning( f\"{fn_pdf}: Pas de texte restant pour le nom de l'arr\u00eat\u00e9: \" + '\"' + txt_copy[pream_beg:pream_end].replace(\"\\n\", \" \").strip() + '\"' ) # WIP if rem_txt and content[-1][\"span_typ\"] == \"nom_arr\": arr_nom = content[-1][\"span_txt\"].replace(\"\\n\", \" \") logging.warning(f\"{fn_pdf}: texte restant et nom: {arr_nom}\") # end WIP # print(content) # WIP # TODO remplacer les zones reconnues par des espaces, et afficher le texte non-captur\u00e9? return content","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_page_content","text":"Analyse une page pour rep\u00e9rer les zones de contenus.","title":"parse_page_content()"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_page_content--parameters","text":"txt_body: string Corps de texte de la page \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser. cur_state: str \u00c9tat actuel: \"avant_articles\", \"avant_signature\", \"apres_signature\" latest_span: dict, optional Dernier empan de contenu rep\u00e9r\u00e9 sur la page pr\u00e9c\u00e9dente. Vaut None pour la premi\u00e8re page.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_page_content--returns","text":"content: list Liste d'empans de contenu src\\process\\parse_doc.py def parse_page_content( txt_body: str, main_beg: int, main_end: int, cur_state: str, latest_span: Optional[dict], ) -> list: \"\"\"Analyse une page pour rep\u00e9rer les zones de contenus. Parameters ---------- txt_body: string Corps de texte de la page \u00e0 analyser main_beg: int D\u00e9but de l'empan \u00e0 analyser. main_end: int Fin de l'empan \u00e0 analyser. cur_state: str \u00c9tat actuel: \"avant_articles\", \"avant_signature\", \"apres_signature\" latest_span: dict, optional Dernier empan de contenu rep\u00e9r\u00e9 sur la page pr\u00e9c\u00e9dente. Vaut `None` pour la premi\u00e8re page. Returns ------- content: list Liste d'empans de contenu \"\"\" # print(f\"parse_page_content: {(main_beg, main_end, cur_state, latest_span)}\") # DEBUG if cur_state not in (\"avant_articles\", \"avant_signature\"): raise ValueError( f\"\u00c9tat inattendu {cur_state}\\n{main_beg}:{main_end}\\n{txt_body[main_beg:main_end]}\" ) if txt_body[main_beg:main_end].strip() == \"\": # la zone de texte \u00e0 analyser est vide return [] content = [] # rep\u00e9rer les d\u00e9buts de paragraphes: \"Vu\", \"Consid\u00e9rant\", \"Arr\u00eate\", \"Article\" if cur_state == \"avant_articles\": # \"Vu\" et \"Consid\u00e9rant\" par_begs = sorted( [(m.start(), \"par_vu\") for m in P_VU.finditer(txt_body, main_beg, main_end)] + [ (m.start(), \"par_considerant\") for m in P_CONSIDERANT.finditer(txt_body, main_beg, main_end) ] ) # \u00e9ventuellement, \"Arr\u00eatons|Arr\u00eate|Arr\u00eat\u00e9\" entre les \"Vu\" \"Consid\u00e9rant\" d'une part, # les \"Article\" d'autre part # si la page en cours contient le dernier \"Vu\" ou \"Consid\u00e9rant\" du document, ou # la fin de ce dernier \"Vu\" ou \"Consid\u00e9rant\", \"Arr\u00eatons\" doit \u00eatre l\u00e0 if par_begs: # il y a au moins un \"Vu\" ou \"Consid\u00e9rant\" sur la page: # le dernier de la page est-il aussi le dernier du document? searchzone_beg = par_begs[-1][0] else: # le dernier \"Vu\" ou \"Consid\u00e9rant\" peut avoir commenc\u00e9 sur la page pr\u00e9c\u00e9dente, # auquel cas on cherche \"Arr\u00eatons|Arr\u00eate|Arr\u00eat\u00e9\" sur toute la zone en cours # d'analyse searchzone_beg = main_beg # print(f\"Cherche ARRETE dans:\\n{txt_body[searchzone_beg:main_end]}\") # DEBUG if m_arretons := P_ARRETONS.search(txt_body, searchzone_beg, main_end): par_begs.append((m_arretons.start(), \"par_arrete\")) elif cur_state == \"avant_signature\": par_begs = [ (m.start(), \"par_article\") for m in P_ARTICLE.finditer(txt_body, main_beg, main_end) ] else: raise ValueError(f\"cur_state: {cur_state}?\") # 1. traiter le texte avant le 1er d\u00e9but de paragraphe if not par_begs: # aucun d\u00e9but de paragraphe d\u00e9tect\u00e9 sur la page ; cela peut arriver lorsqu'un empan # court sur plusieurs pages, eg. un \"Consid\u00e9rant\" tr\u00e8s long incluant la liste des # copropri\u00e9taires # # ce n'est possible que s'il y a bien un latest_span, et d'un type admettant une continuation if (latest_span is None) or ( latest_span[\"span_typ\"] not in ( \"par_vu\", \"par_considerant\", \"par_article\", # une continuation peut \u00eatre elle-m\u00eame continu\u00e9e \"par_vu_suite\", \"par_considerant_suite\", \"par_article_suite\", ) ): raise ValueError( f\"Aucun paragraphe continuable sur cette page sans nouveau paragraphe?\\ncur_state={cur_state}, latest_span={latest_span}, (main_beg, main_end)=({main_beg}, {main_end})\\n{txt_body[main_beg:main_end]}\" ) # analyser jusqu'en bas de la page, sans visibilit\u00e9 sur le type du prochain empan (absent de la page) nxt_beg = main_end nxt_typ = None else: # s'il y a du texte avant le 1er d\u00e9but de paragraphe, c'est la continuation du # dernier paragraphe de la page pr\u00e9c\u00e9dente ; # le mettre dans un empan de type span_typ + \"_suite\" nxt_beg, nxt_typ = par_begs[0] # r\u00e9cup\u00e9rer ce texte et le mettre dans un empan sp\u00e9cial _suite if (not P_LINE.fullmatch(txt_body, main_beg, nxt_beg)) and ( match := P_STRIP.fullmatch(txt_body, main_beg, nxt_beg) ): txt_dang = match.group(\"outstrip\") if txt_dang: # print(f\"txt_dang: {txt_dang}\") # DEBUG try: lst_typ = latest_span[\"span_typ\"] except TypeError: print( f\"cur_state={cur_state}\\npar_begs={par_begs}\\n(main_beg, nxt_beg)=({main_beg}, {nxt_beg})\\n{txt_body[main_beg:nxt_beg]}\" ) raise # un empan peut courir sur plus d'une page compl\u00e8te (ex: \"Consid\u00e9rant\" tr\u00e8s long, incluant la liste des copropri\u00e9taires) cur_typ = lst_typ if lst_typ.endswith(\"_suite\") else lst_typ + \"_suite\" # stocker la zone reconnue content.append( { \"span_beg\": match.start(\"outstrip\"), \"span_end\": match.end(\"outstrip\"), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": cur_typ, } ) if nxt_typ is not None: # v\u00e9rifier que la transition autoris\u00e9e est correcte # TODO d\u00e9placer cette v\u00e9rification en amont ou en aval? try: assert (cur_typ, nxt_typ) in ( # les \"Vu\" sont g\u00e9n\u00e9ralement avant les \"Consid\u00e9rant\" mais certains arr\u00eat\u00e9s m\u00ealent les deux types de paragraphes (\"par_vu_suite\", \"par_vu\"), (\"par_vu_suite\", \"par_considerant\"), (\"par_vu_suite\", \"par_arrete\"), # NEW 2023-03-23 (\"par_considerant_suite\", \"par_vu\"), # NEW 2023-03-23 (\"par_considerant_suite\", \"par_considerant\"), (\"par_considerant_suite\", \"par_arrete\"), # (\"par_arrete_suite\", \"par_article\"), # \"Arr\u00eate\" ne peut pas \u00eatre coup\u00e9 par un saut de page car il est toujours sur une seule ligne # les articles forment un bloc homog\u00e8ne, sans retour vers des \"Vu\" ou \"Consid\u00e9rant\" (s'il y en a, ce sont des citations de passage dans un article...) (\"par_article_suite\", \"par_article\"), ) except AssertionError: print( f\"Transition inattendue: ({cur_typ}, {nxt_typ})\\n{latest_span}\\n{txt_body}\" ) raise # 2. pour chaque d\u00e9but de paragraphe, cr\u00e9er un empan allant jusqu'au prochain d\u00e9but for (cur_beg, cur_typ), (nxt_beg, nxt_typ) in zip(par_begs[:-1], par_begs[1:]): # extraire le texte hors espaces de d\u00e9but et fin if (not P_LINE.fullmatch(txt_body, cur_beg, nxt_beg)) and ( match := P_STRIP.fullmatch(txt_body, cur_beg, nxt_beg) ): # stocker la zone reconnue content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": cur_typ, } ) # v\u00e9rifier que la transition autoris\u00e9e est correcte # TODO d\u00e9placer cette v\u00e9rification en amont ou en aval? try: assert (cur_typ, nxt_typ) in ( (\"par_vu\", \"par_vu\"), (\"par_vu\", \"par_considerant\"), # (consid\u00e9rant, vu): transition rare mais qui arrive (\"par_considerant\", \"par_vu\"), # (vu, arr\u00eate): transition rare mais qui arrive (ex: abrogation d'arr\u00eat\u00e9 dont la raison est donn\u00e9e dans un autre arr\u00eat\u00e9...) (\"par_vu\", \"par_arrete\"), (\"par_considerant\", \"par_considerant\"), (\"par_considerant\", \"par_arrete\"), (\"par_arrete\", \"par_article\"), (\"par_article\", \"par_article\"), ) except AssertionError: print(f\"Transition impr\u00e9vue: ({cur_typ, nxt_typ})\\n{content}\") raise # 3. pour le dernier d\u00e9but de paragraphe, cr\u00e9er un empan allant jusqu'\u00e0 la fin du texte if par_begs: cur_beg, cur_typ = par_begs[-1] nxt_beg = main_end nxt_typ = None # extraire le texte hors espaces de d\u00e9but et fin if (not P_LINE.fullmatch(txt_body, cur_beg, nxt_beg)) and ( match := P_STRIP.fullmatch(txt_body, cur_beg, nxt_beg) ): # stocker la zone reconnue content.append( { \"span_beg\": match.start(), \"span_end\": match.end(), \"span_txt\": match.group(\"outstrip\"), \"span_typ\": cur_typ, } ) # on ne peut pas v\u00e9rifier si la transition autoris\u00e9e est correcte puisque # le prochain empan n'est pas connu (page suivante) ; cette v\u00e9rification # sera faite de toute fa\u00e7on lors du traitement du haut de la prochaine page # rep\u00e9rer, dans chaque paragraphe, les r\u00e9f\u00e9rences au cadre r\u00e9glementaire # TODO seulement pour les \"vu\"? ou utile pour les autres? # TODO certaines r\u00e9f\u00e9rences peuvent-elles \u00eatre coup\u00e9es par des sauts de page ? => concat\u00e9ner latest_span[\"span_txt\"] et content[0][\"span_txt\"] ? content_reg = [] for par in content: par_reg = parse_refs_reglement(txt_body, par[\"span_beg\"], par[\"span_end\"]) content_reg.extend(par_reg) content.extend(content_reg) return content","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_page_template","text":"Analyse une page pour rep\u00e9rer le template. Rep\u00e8re les en-t\u00eates, pieds-de-page, tampons, et renvoie les empans correspondants, ainsi que le texte d\u00e9barrass\u00e9 de ces \u00e9l\u00e9ments de template.","title":"parse_page_template()"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_page_template--parameters","text":"txt: str Texte d'origine de la page.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.parse_page_template--returns","text":"content: list Liste d'empans rep\u00e9r\u00e9s sur la page. txt_body: string Corps de texte, d\u00e9fini comme le texte en entr\u00e9e dans lequel les empans d'en-t\u00eates, pieds-de-page et tampons de content ont \u00e9t\u00e9 effac\u00e9s (remplac\u00e9s par des espaces de m\u00eame longueur). src\\process\\parse_doc.py def parse_page_template(txt: str) -> tuple[list, str]: \"\"\"Analyse une page pour rep\u00e9rer le template. Rep\u00e8re les en-t\u00eates, pieds-de-page, tampons, et renvoie les empans correspondants, ainsi que le texte d\u00e9barrass\u00e9 de ces \u00e9l\u00e9ments de template. Parameters ---------- txt: str Texte d'origine de la page. Returns ------- content: list Liste d'empans rep\u00e9r\u00e9s sur la page. txt_body: string Corps de texte, d\u00e9fini comme le texte en entr\u00e9e dans lequel les empans d'en-t\u00eates, pieds-de-page et tampons de `content` ont \u00e9t\u00e9 effac\u00e9s (remplac\u00e9s par des espaces de m\u00eame longueur). \"\"\" content = [] # en-t\u00eate # TODO expectation: n=0..2 par page if m_headers := P_HEADER.finditer(txt): for match in m_headers: content.append( { \"span_beg\": match.span()[0], \"span_end\": match.span()[1], \"span_txt\": match.group(0), \"span_typ\": \"header\", } ) # print(f\"<<<<< template:headers={content}\") # DEBUG # pied-de-page # TODO expectation: n=0..2 par page if m_footers := P_FOOTER.finditer(txt): for match in m_footers: content.append( { \"span_beg\": match.span()[0], \"span_end\": match.span()[1], \"span_txt\": match.group(0), \"span_typ\": \"footer\", } ) # tampon de transmission \u00e0 actes if m_stamps := P_STAMP.finditer(txt): for match in m_stamps: m_beg, m_end = match.span() content.append( { \"span_beg\": m_beg, \"span_end\": m_end, \"span_txt\": match.group(0), \"span_typ\": \"stamp\", } ) # corps du texte # d\u00e9fini comme le texte d'origine, dans lequel on a effac\u00e9 les empans rep\u00e9r\u00e9s # (en-t\u00eates, pieds-de-page, tampons) ; # remplacer les empans par des espaces permet de conserver les indices d'origine # et \u00e9viter les d\u00e9calages spans = list((x[\"span_beg\"], x[\"span_end\"]) for x in content) txt_body = txt[:] for sp_beg, sp_end in spans: txt_body = txt_body[:sp_beg] + \" \" * (sp_end - sp_beg) + txt_body[sp_end:] return content, txt_body","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc.process_files","text":"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes.","title":"process_files()"},{"location":"Code%20Source/process/#src.process.parse_doc.process_files--parameters","text":"df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.process_files--returns","text":"df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. src\\process\\parse_doc.py def process_files( df_meta: pd.DataFrame, df_txts: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters ---------- df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns ------- df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. \"\"\" indics_struct = [] for _, df_doc_pages in df_txts.groupby(\"fullpath\"): # RESUME HERE ~exclude # m\u00e9ta \u00e0 passer en fin df_doc_meta = df_doc_pages[[\"pdf\", \"fullpath\", \"pagenum\"]] # fn_pdf = df_doc_pages[\"pdf\"].iat[0] pages = df_doc_pages[\"pagetxt\"].values exclude = df_doc_pages[\"exclude\"].values # actes try: has_stamp_pages = [ (P_STAMP.search(x) is not None) if pd.notna(x) else None for x in pages ] except TypeError: print(repr(pages)) raise # rep\u00e9rer les pages d'accus\u00e9 de r\u00e9ception d'actes, elles seront marqu\u00e9es et non pass\u00e9es au parser # TODO v\u00e9rifier si la page d'AR actes appara\u00eet seulement en derni\u00e8re page, sinon on peut couper le doc et passer moins de pages au parser # TODO timer et r\u00e9\u00e9crire les deux instructions en pandas[pyarrow] pour am\u00e9liorer la vitesse? is_ar_pages = [ (P_ACCUSE.match(x) is not None) if pd.notna(x) else None for x in pages ] # filtrer les pages filt_pages = [] for x, excl, is_ar in zip(pages, exclude, is_ar_pages): if excl: # flag d'exclusion de la page filt_p = None elif is_ar: # page d'accus\u00e9 de r\u00e9ception de t\u00e9l\u00e9transmission actes filt_p = \"\" else: filt_p = x filt_pages.append(filt_p) # analyser les pages doc_content = parse_arrete_pages(fn_pdf, filt_pages) # filtrer les empans de donn\u00e9es, et laisser de c\u00f4t\u00e9 les empans de structure for page_cont, has_st, is_ar, page_meta in zip( doc_content, has_stamp_pages, is_ar_pages, df_doc_meta.itertuples() ): pg_content = page_cont[\"content\"] pg_txt_body = page_cont[\"body\"] # donn\u00e9es if pg_txt_body: # adresse(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 if pg_adrs_doc := get_adr_doc(pg_txt_body): # on s\u00e9lectionne arbitrairement la 1re zone d'adresse(s) (FIXME?) pg_adr_doc = pg_adrs_doc[0][\"adresse_brute\"] # temporairement: on prend la 1re adresse pr\u00e9cise extraite de cette zone adr_fields = pg_adrs_doc[0][\"adresses\"][0] # end WIP else: pg_adr_doc = None adr_fields = { \"adr_num\": None, # num\u00e9ro de la voie \"adr_ind\": None, # indice de r\u00e9p\u00e9tition \"adr_voie\": None, # nom de la voie \"adr_compl\": None, # compl\u00e9ment d'adresse \"adr_cpostal\": None, # code postal \"adr_ville\": None, # ville } # parcelle(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 if pg_parcelle := get_parcelles(pg_txt_body): pg_parcelle = pg_parcelle[0] # get_parcelles:list[str] else: pg_parcelle = None else: pg_adr_doc = None adr_fields = { \"adr_num\": None, # num\u00e9ro de la voie \"adr_ind\": None, # indice de r\u00e9p\u00e9tition \"adr_voie\": None, # nom de la voie \"adr_compl\": None, # compl\u00e9ment d'adresse \"adr_cpostal\": None, # code postal \"adr_ville\": None, # ville } pg_parcelle = None # rassembler les donn\u00e9es dans un dict rec_struct = { # @ctes \"has_stamp\": has_st, \"is_accusedereception_page\": is_ar, # tous arr\u00eat\u00e9s \"commune_maire\": unique_txt(pg_content, \"adr_ville\"), \"has_vu\": has_one(pg_content, \"par_vu\"), \"has_considerant\": has_one(pg_content, \"par_considerant\"), \"has_arrete\": has_one(pg_content, \"par_arrete\"), \"has_article\": has_one(pg_content, \"par_article\"), # arr\u00eat\u00e9s sp\u00e9cifiques # - r\u00e9glementaires \"has_cgct\": contains_cgct(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cgct_art\": contains_cgct_art(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch\": contains_cch(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L111\": contains_cch_L111(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L511\": contains_cch_L511(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L521\": contains_cch_L521(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_L541\": contains_cch_L541(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cch_R511\": contains_cch_R511(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cc\": contains_cc(pg_txt_body) if pg_txt_body is not None else None, # TODO \"has_cc_art\": contains_cc_art(pg_txt_body) if pg_txt_body is not None else None, # TODO # - donn\u00e9es \"adresse\": pg_adr_doc, # TODO urgent # refactor 2023-03-31: remonter l'extraction de l'adresse pr\u00e9cise \"adr_num\": adr_fields[\"adr_num\"], # num\u00e9ro de la voie \"adr_ind\": adr_fields[\"adr_ind\"], # indice de r\u00e9p\u00e9tition \"adr_voie\": adr_fields[\"adr_voie\"], # nom de la voie \"adr_compl\": adr_fields[\"adr_compl\"], # compl\u00e9ment d'adresse \"adr_cpostal\": adr_fields[\"adr_cpostal\"], # code postal \"adr_ville\": adr_fields[\"adr_ville\"], # ville # end refactor 2023-03-31 \"parcelle\": pg_parcelle, # TODO urgent \"proprio\": get_proprio(pg_txt_body) if pg_txt_body is not None else None, # WIP \"syndic\": get_syndic(pg_txt_body) if pg_txt_body is not None else None, # TODO urgent- \"gest\": get_gest(pg_txt_body) if pg_txt_body is not None else None, # TODO urgent- \"date\": unique_txt(pg_content, \"arr_date\"), # * arr\u00eat\u00e9 \"num_arr\": unique_txt(pg_content, \"num_arr\"), \"nom_arr\": unique_txt(pg_content, \"nom_arr\"), \"classe\": get_classe(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"urgence\": get_urgence(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"demo\": get_demo(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"int_hab\": get_int_hab(pg_txt_body) if pg_txt_body is not None else None, # TODO improve \"equ_com\": get_equ_com(pg_txt_body) if pg_txt_body is not None else None, # TODO improve } indics_struct.append( { \"pdf\": page_meta.pdf, \"fullpath\": page_meta.fullpath, \"pagenum\": page_meta.pagenum, } | rec_struct # python >= 3.9 (dict union) ) df_indics = pd.DataFrame.from_records(indics_struct) df_proc = pd.merge(df_meta, df_indics, on=[\"pdf\", \"fullpath\"]) df_proc = df_proc.astype(dtype=DTYPE_META_NTXT_PROC) return df_proc","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_doc.unique_txt","text":"Cherche l'unique empan d'un type donn\u00e9 et renvoie son texte. Si plusieurs empans de la liste en entr\u00e9e sont du type recherch\u00e9, une exception est lev\u00e9e. Si aucun empan de la liste n'est du type recherch\u00e9, renvoie None.","title":"unique_txt()"},{"location":"Code%20Source/process/#src.process.parse_doc.unique_txt--parameters","text":"spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_doc.unique_txt--returns","text":"span_txt: str Texte de l'unique empan du type recherch\u00e9. src\\process\\parse_doc.py def unique_txt(spans: list[dict], span_typ: str) -> str: \"\"\"Cherche l'unique empan d'un type donn\u00e9 et renvoie son texte. Si plusieurs empans de la liste en entr\u00e9e sont du type recherch\u00e9, une exception est lev\u00e9e. Si aucun empan de la liste n'est du type recherch\u00e9, renvoie None. Parameters ---------- spans: list(dict) Liste d'empans extraits span_typ: str Type d'empan recherch\u00e9 Returns ------- span_txt: str Texte de l'unique empan du type recherch\u00e9. \"\"\" if not spans: return None cands = [x for x in spans if x[\"span_typ\"] == span_typ] if not cands: return None elif len(cands) > 1: raise ValueError(f\"Plusieurs empans de type {span_typ}: {cands}\") else: return cands[0][\"span_txt\"]","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_native_pages--extrait-la-structure-des-documents","text":"D\u00e9coupe chaque arr\u00eat\u00e9 en zones: * pr\u00e9ambule (?), * VUs, * CONSIDERANTs, * ARTICLES, * postambule (?)","title":"Extrait la structure des documents."},{"location":"Code%20Source/process/#src.process.parse_native_pages.process_files","text":"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes.","title":"process_files()"},{"location":"Code%20Source/process/#src.process.parse_native_pages.process_files--parameters","text":"df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter.","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_native_pages.process_files--returns","text":"df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. src\\process\\parse_native_pages.py def process_files( df_meta: pd.DataFrame, df_txts: pd.DataFrame, ) -> pd.DataFrame: \"\"\"Traiter un ensemble d'arr\u00eat\u00e9s: rep\u00e9rer des \u00e9l\u00e9ments de structure des textes. Parameters ---------- df_meta: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des fichiers \u00e0 traiter. df_txts: pd.DataFrame Liste de pages de documents \u00e0 traiter. Returns ------- df_proc: pd.DataFrame Liste de m\u00e9tadonn\u00e9es des pages trait\u00e9es, avec indications des \u00e9l\u00e9ments de structure d\u00e9tect\u00e9s. \"\"\" indics_struct = [] for df_row in df_txts.itertuples(): # pour chaque page de document, rep\u00e9rer des indications de structure rec_struct = spot_text_structure(df_row) indics_struct.append( { \"pdf\": df_row.pdf, \"fullpath\": df_row.fullpath, \"pagenum\": df_row.pagenum, } | rec_struct # python >= 3.9 (dict union) ) df_indics = pd.DataFrame.from_records(indics_struct) df_proc = pd.merge(df_meta, df_indics, on=[\"pdf\", \"fullpath\"]) df_proc = df_proc.astype(dtype=DTYPE_META_NTXT_PROC) return df_proc","title":"Returns"},{"location":"Code%20Source/process/#src.process.parse_native_pages.spot_text_structure","text":"D\u00e9tecte la pr\u00e9sence d'\u00e9l\u00e9ments de structure dans une page d'arr\u00eat\u00e9. D\u00e9tecte la pr\u00e9sence de tampons, pages d'accus\u00e9 de r\u00e9ception, VU, CONSIDERANT, ARTICLE.","title":"spot_text_structure()"},{"location":"Code%20Source/process/#src.process.parse_native_pages.spot_text_structure--parameters","text":"df_row: NamedTuple Page de document","title":"Parameters"},{"location":"Code%20Source/process/#src.process.parse_native_pages.spot_text_structure--returns","text":"rec_struct: dict Dictionnaire de valeurs bool\u00e9ennes ou nulles, selon que les \u00e9l\u00e9ments de structure ont \u00e9t\u00e9 d\u00e9tect\u00e9s. Les cl\u00e9s et les types de valeurs sont sp\u00e9cifi\u00e9s dans DTYPE_PARSE . Si df_row ne contient pas de texte, toutes les valeurs de sortie sont None. src\\process\\parse_native_pages.py def spot_text_structure( df_row: NamedTuple, ) -> pd.DataFrame: \"\"\"D\u00e9tecte la pr\u00e9sence d'\u00e9l\u00e9ments de structure dans une page d'arr\u00eat\u00e9. D\u00e9tecte la pr\u00e9sence de tampons, pages d'accus\u00e9 de r\u00e9ception, VU, CONSIDERANT, ARTICLE. Parameters ---------- df_row: NamedTuple Page de document Returns ------- rec_struct: dict Dictionnaire de valeurs bool\u00e9ennes ou nulles, selon que les \u00e9l\u00e9ments de structure ont \u00e9t\u00e9 d\u00e9tect\u00e9s. Les cl\u00e9s et les types de valeurs sont sp\u00e9cifi\u00e9s dans `DTYPE_PARSE`. Si df_row ne contient pas de texte, toutes les valeurs de sortie sont None. \"\"\" if pd.notna(df_row.pagetxt) and ( not df_row.exclude ): # WIP \" and (not df_row.exclude)\" logging.warning(f\"{df_row.pdf} / {df_row.pagenum}\") # WIP # adresse(s) vis\u00e9e(s) par l'arr\u00eat\u00e9 if pg_adrs_doc := get_adr_doc(df_row.pagetxt): # on s\u00e9lectionne arbitrairement la 1re zone d'adresse(s) (FIXME?) pg_adr_doc = pg_adrs_doc[0][\"adresse_brute\"] # temporairement: on prend la 1re adresse pr\u00e9cise extraite de cette zone adr_fields = pg_adrs_doc[0][\"adresses\"][0] # end WIP else: pg_adr_doc = None adr_fields = { \"adr_num\": None, # num\u00e9ro de la voie \"adr_ind\": None, # indice de r\u00e9p\u00e9tition \"adr_voie\": None, # nom de la voie \"adr_compl\": None, # compl\u00e9ment d'adresse \"adr_cpostal\": None, # code postal \"adr_ville\": None, # ville } # parcelle(s) vis\u00e9es par l'arr\u00eat\u00e9 parcelles = get_parcelles(df_row.pagetxt) # rec_struct = { # @ctes \"has_stamp\": is_stamped_page(df_row.pagetxt), \"is_accusedereception_page\": is_accusedereception_page(df_row.pagetxt), # tous arr\u00eat\u00e9s \"commune_maire\": get_commune_maire(df_row.pagetxt), \"has_vu\": contains_vu(df_row.pagetxt), \"has_considerant\": contains_considerant(df_row.pagetxt), \"has_arrete\": contains_arrete(df_row.pagetxt), \"has_article\": contains_article(df_row.pagetxt), # arr\u00eat\u00e9s sp\u00e9cifiques # - r\u00e9glementaires \"has_cgct\": contains_cgct(df_row.pagetxt), \"has_cgct_art\": contains_cgct_art(df_row.pagetxt), \"has_cch\": contains_cch(df_row.pagetxt), \"has_cch_L111\": contains_cch_L111(df_row.pagetxt), \"has_cch_L511\": contains_cch_L511(df_row.pagetxt), \"has_cch_L521\": contains_cch_L521(df_row.pagetxt), \"has_cch_L541\": contains_cch_L541(df_row.pagetxt), \"has_cch_R511\": contains_cch_R511(df_row.pagetxt), \"has_cc\": contains_cc(df_row.pagetxt), \"has_cc_art\": contains_cc_art(df_row.pagetxt), # - donn\u00e9es \"adresse\": pg_adr_doc, # refactor 2023-03-31: remonter l'extraction de l'adresse pr\u00e9cise \"adr_num\": adr_fields[\"adr_num\"], # num\u00e9ro de la voie \"adr_ind\": adr_fields[\"adr_ind\"], # indice de r\u00e9p\u00e9tition \"adr_voie\": adr_fields[\"adr_voie\"], # nom de la voie \"adr_compl\": adr_fields[\"adr_compl\"], # compl\u00e9ment d'adresse \"adr_cpostal\": adr_fields[\"adr_cpostal\"], # code postal \"adr_ville\": adr_fields[\"adr_ville\"], # ville # end refactor 2023-03-31 \"parcelle\": parcelles[0] if parcelles else None, # TODO si la page contient plusieurs empans d\u00e9signant une ou plusieurs parcelles \"proprio\": get_proprio(df_row.pagetxt), # WIP \"syndic\": get_syndic(df_row.pagetxt), \"gest\": get_gest(df_row.pagetxt), \"date\": get_date(df_row.pagetxt), # * arr\u00eat\u00e9 \"num_arr\": get_num(df_row.pagetxt), \"nom_arr\": get_nom(df_row.pagetxt), \"classe\": get_classe(df_row.pagetxt), \"urgence\": get_urgence(df_row.pagetxt), \"demo\": get_demo(df_row.pagetxt), \"int_hab\": get_int_hab(df_row.pagetxt), \"equ_com\": get_equ_com(df_row.pagetxt), } else: # tous les champs sont vides (\"None\") rec_struct = {x: None for x in DTYPE_PARSE} return rec_struct","title":"Returns"},{"location":"Code%20Source/quality/","text":"Quality Fonctions de validation des donn\u00e9es extraites. Valide les zones rep\u00e9r\u00e9es. Tous les en-t\u00eates commencent \u00e0 0 ; Tous les pieds-de-pages terminent \u00e0 la longueur du document ; En-t\u00eate et pied-de-page sont disjoints ; drop_no_errors_arr(df_arr) Supprime les arr\u00eat\u00e9s sans erreur. Parameters df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s sans erreurs. src\\quality\\validate_parses.py def drop_no_errors_arr(df_arr: pd.DataFrame) -> pd.DataFrame: \"\"\"Supprime les arr\u00eat\u00e9s sans erreur. Parameters ---------- df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s sans erreurs. \"\"\" df_arr = df_arr.copy() # if none of the keys is equal to 1, then the row has no error df_arr[\"has_error\"] = df_arr[ERROR_KEYS].sum(axis=1) > 0 # delete all the row without error df_arr = df_arr[df_arr[\"has_error\"]] return df_arr error_classe_manquante(df) Signale les arr\u00eat\u00e9s dont la classe n'a pu \u00eatre d\u00e9termin\u00e9e. Les causes les plus fr\u00e9quentes sont une erreur d'OCR sur un document mal num\u00e9ris\u00e9, ou une mise en page du document sur plusieurs colonnes qui n'est pas explicitement g\u00e9r\u00e9e par les scripts actuels, et dont le r\u00e9sultat ne permet pas la reconnaissance des motifs recherch\u00e9s. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_classe_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont la classe n'a pu \u00eatre d\u00e9termin\u00e9e. Les causes les plus fr\u00e9quentes sont une erreur d'OCR sur un document mal num\u00e9ris\u00e9, ou une mise en page du document sur plusieurs colonnes qui n'est pas explicitement g\u00e9r\u00e9e par les scripts actuels, et dont le r\u00e9sultat ne permet pas la reconnaissance des motifs recherch\u00e9s. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_classe\"] = df.apply( lambda row: 1 if pd.isnull(row.classe) else 0, axis=1 ) return df error_codeinsee_13055(df) Signale les arr\u00eat\u00e9s dont le code INSEE est 13055. 13055 est le code pour tout Marseille, alors que l'on devrait avoir le code propre \u00e0 l'arrondissement (13201 \u00e0 13216). Ignore les valeurs manquantes. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_codeinsee_13055(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont le code INSEE est 13055. 13055 est le code pour tout Marseille, alors que l'on devrait avoir le code propre \u00e0 l'arrondissement (13201 \u00e0 13216). Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"codeinsee_13055\"] = df.apply( lambda row: 1 if not pd.isnull(row.codeinsee) and row.codeinsee == \"13055\" else 0, axis=1, ) return df error_codeinsee_manquant(df) Signale les arr\u00eat\u00e9s dont le code INSEE est manquant. Le code INSEE est d\u00e9termin\u00e9 sur base du nom de la commune, crois\u00e9 avec la table des codes communes dans data/external/ (actuellement restreint au p\u00e9rim\u00e8tre de la m\u00e9tropole Aix-Marseille Provence). Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_codeinsee_manquant(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont le code INSEE est manquant. Le code INSEE est d\u00e9termin\u00e9 sur base du nom de la commune, crois\u00e9 avec la table des codes communes dans data/external/ (actuellement restreint au p\u00e9rim\u00e8tre de la m\u00e9tropole Aix-Marseille Provence). Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucun_codeinsee\"] = df.apply( lambda row: 1 if pd.isnull(row.codeinsee) else 0, axis=1 ) return df error_cpostal_manquant(df) Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_cpostal_manquant(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucun_cpostal\"] = df.apply( lambda row: 1 if pd.isnull(row.cpostal) else 0, axis=1 ) return df error_date_manquante(df) Signale les arr\u00eat\u00e9s dont la date n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une erreur d'OCR sur une date manuscrite ou tamponn\u00e9e, ou un document mal num\u00e9ris\u00e9 ; il est possible que le script \u00e9choue \u00e0 extraire la date dans certaines tournures de r\u00e9daction. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_date_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont la date n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une erreur d'OCR sur une date manuscrite ou tamponn\u00e9e, ou un document mal num\u00e9ris\u00e9 ; il est possible que le script \u00e9choue \u00e0 extraire la date dans certaines tournures de r\u00e9daction. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_date\"] = df.apply(lambda row: 1 if pd.isnull(row.date) else 0, axis=1) return df error_num_voie_manquant(df) Signale les adresses d'arr\u00eat\u00e9s sans num\u00e9ro de voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement ou totalement. Ignore les valeurs manquantes. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_num_voie_manquant(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans num\u00e9ro de voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement ou totalement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucun_num_voie\"] = df.apply(lambda row: 1 if pd.isnull(row.num) else 0, axis=1) return df error_urgence_manquante(df) Signale les arr\u00eat\u00e9s dont l'urgence n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une classe d'arr\u00eat\u00e9 qui ne donne pas explicitement cette information. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_urgence_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont l'urgence n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une classe d'arr\u00eat\u00e9 qui ne donne pas explicitement cette information. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"manque_urgence\"] = df.apply( lambda row: 1 if pd.isnull(row.urgence) else 0, axis=1 ) return df error_ville_manquante(df) Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_ville_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_ville\"] = df.apply(lambda row: 1 if pd.isnull(row.ville) else 0, axis=1) return df error_voie_manquante(df) Signale les adresses d'arr\u00eat\u00e9s sans voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_voie_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_voie\"] = df.apply(lambda row: 1 if pd.isnull(row.voie) else 0, axis=1) return df examine_doc_content(fn_pdf, doc_content) V\u00e9rifie des hypoth\u00e8ses de bonne formation sur le contenu extrait du document. Parameters doc_content: list[dict] Empans de contenu extraits du document src\\quality\\validate_parses.py def examine_doc_content(fn_pdf: str, doc_content: \"list[dict]\"): \"\"\"V\u00e9rifie des hypoth\u00e8ses de bonne formation sur le contenu extrait du document. Parameters ---------- doc_content: list[dict] Empans de contenu extraits du document \"\"\" # filtrer les pages absentes pg_conts = [x for x in doc_content if (pd.notna(x) and x[\"content\"] is not None)] # paragraphes pars = [ x for pg_content in pg_conts for x in pg_content[\"content\"] if (pd.notna(x) and x[\"span_typ\"].startswith(\"par_\")) ] par_typs = [x[\"span_typ\"] for x in pars] # \"consid\u00e9rant\" obligatoire sauf pour certains arr\u00eat\u00e9s? # TODO d\u00e9terminer si les assertions ne s'appliquent qu'\u00e0 certaines classes d'arr\u00eat\u00e9s if par_typs: # chaque arr\u00eat\u00e9 contient au moins un \"vu\" if \"par_vu\" not in par_typs: logging.warning( f\"{fn_pdf}: pas de 'vu' trouv\u00e9 (v\u00e9rifier la nature du document ?)\" ) # chaque arr\u00eat\u00e9 contient au moins un \"consid\u00e9rant\" # * sauf dans les mainlev\u00e9es et abrogations o\u00f9 dans la pratique ce n'est pas le cas if \"par_considerant\" not in par_typs: # FIXME d\u00e9tecter la classe => ne pas appliquer pour abrogations et mainlev\u00e9es logging.warning( f\"{fn_pdf}: pas de 'consid\u00e9rant' trouv\u00e9 (v\u00e9rifier la nature du document ?)\" ) # chaque arr\u00eat\u00e9 contient exactement 1 \"Arr\u00eate\" try: assert len([x for x in par_typs if x == \"par_arrete\"]) == 1 except AssertionError: logging.warning( f\"{fn_pdf}: pas de 'Arr\u00eate' trouv\u00e9 (v\u00e9rifier la qualit\u00e9 de l'OCR ?)\" ) expect_footer_end_len(df) V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9). Parameters df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents. Returns success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. src\\quality\\validate_parses.py def expect_footer_end_len(df: pd.DataFrame) -> bool: \"\"\"V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9). Parameters ---------- df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents. Returns ------- success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. \"\"\" return (df[\"footer_end\"].dropna() == len()).all() expect_header_beg_zero(df) V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9). Parameters df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents. Returns success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. src\\quality\\validate_parses.py def expect_header_beg_zero(df: pd.DataFrame) -> bool: \"\"\"V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9). Parameters ---------- df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents. Returns ------- success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. \"\"\" return (df[\"header_beg\"].dropna() == 0).all() generate_html_report(run, df_adr, df_arr, df_not, df_par) G\u00e9n\u00e9rer un rapport d'erreurs en HTML Parameters run: string Identifiant de l'ex\u00e9cution df_adr: pd.DataFrame Adresses df_arr: pd.DataFrame Arr\u00eat\u00e9s df_not: pd.DataFrame Notifi\u00e9s df_par: pd.DataFrame Parcelles Returns html_report: string Rapport HTML src\\quality\\validate_parses.py def generate_html_report( run: str, df_adr: pd.DataFrame, df_arr: pd.DataFrame, df_not: pd.DataFrame, df_par: pd.DataFrame, ) -> str: \"\"\"G\u00e9n\u00e9rer un rapport d'erreurs en HTML Parameters ---------- run: string Identifiant de l'ex\u00e9cution df_adr: pd.DataFrame Adresses df_arr: pd.DataFrame Arr\u00eat\u00e9s df_not: pd.DataFrame Notifi\u00e9s df_par: pd.DataFrame Parcelles Returns ------- html_report: string Rapport HTML \"\"\" # --- TODO bricolage pour fusionner en aval --- # # Merge dataframes based on 'idu' column merged_df = ( df_adr.merge(df_arr, on=\"idu\", how=\"outer\", suffixes=(\"_df1\", \"_df2\")) .merge(df_not, on=\"idu\", how=\"outer\", suffixes=(\"_df12\", \"_df3\")) .merge(df_par, on=\"idu\", how=\"outer\", suffixes=(\"_df123\", \"_df4\")) ) # fuse duplicate columns for index, row in merged_df.iterrows(): for col in merged_df.columns: # Check if the column matches the pattern r\".*_df.*\" if re.match(r\".*_df\\d+\", col): # Extract the column name excluding the \"df\" number col_name = re.sub(r\"_df\\d+\", \"\", col) # Check if there are other columns with the same name excluding the \"df\" number matching_cols = [ c for c in merged_df.columns if re.match(col_name + r\"_df\\d+\", c) ] # Find the first non-null value among the matching columns non_null_cell = next( (c for c in matching_cols if row[c] is not None), None ) # Assign the value from the first non-null column to the corresponding col_name column merged_df.at[index, col_name] = row[non_null_cell] # Drop the other matching columns merged_df.drop( [c for c in matching_cols if c != non_null_cell], axis=1, inplace=True, ) # drop row with duplicate idu merged_df.drop_duplicates(subset=\"idu\", keep=\"first\", inplace=True) # reset ID merged_df.reset_index(drop=True, inplace=True) nb_arretes = len(merged_df) # options de mise en forme render_links = True res = [] # d\u00e9but et bloc de titre res.append(\"<html>\") res.append(f\"<title>Rapport d'erreurs {run}</title>\") res.append(f\"<h1>Rapport d'erreurs {run}</h1>\") # informations g\u00e9n\u00e9rales sur le lot analys\u00e9 res.append(\"<div>\") res.append(f\"Nombre d'arr\u00eat\u00e9s analys\u00e9s: {nb_arretes}\") res.append(\"</div>\") # adding error columns to the dataframe, 1 = there is an error, 0 = no error # --- aucune adresse --- # merged_df = warn_adresse_empty(merged_df) # --- aucune parcelle --- # merged_df = warn_par_ref_cad_empty(merged_df) # --- aucune date --- # merged_df = error_date_manquante(merged_df) # --- aucune classe --- # merged_df = error_classe_manquante(merged_df) # --- aucune voie --- # merged_df = error_voie_manquante(merged_df) # --- aucun code postal --- # merged_df = error_cpostal_manquant(merged_df) # --- aucune ville --- # merged_df = error_ville_manquante(merged_df) # --- aucun code INSEE --- # merged_df = error_codeinsee_manquant(merged_df) # --- code INSEE 13055 --- # merged_df = error_codeinsee_13055(merged_df) # --- aucun num\u00e9ro de voie --- # merged_df = error_num_voie_manquant(merged_df) # --- manquance_urgence --- # merged_df = error_urgence_manquante(merged_df) # drop rows without errors merged_df = drop_no_errors_arr(merged_df) # only keep the columns we want to display merged_df = merged_df[[\"idu\", *ERROR_KEYS, \"url\"]] res.append(\"<h1>Infos manquantes</h1>\") \"\"\" ## points d'attention # plusieurs parcelles # FL: 2023-06-29: inutile? # plusieurs adresses # FL: 2023-06-29: inutile? \"\"\" # --- adding red to errors --- # # Apply the styling to the DataFrame styled_df = merged_df.applymap(highlight_value_red) # remove any 0 and make the cell empty instead styled_df = styled_df.replace(0, \"\") # Convert the styled DataFrame to HTML with red highlighting styled_df = styled_df.rename_axis(\"id\", axis=1) html_table = styled_df.to_html(escape=False, render_links=render_links) res.append(html_table) # fin du document res.append(\"</html>\") return \"\\n\".join(res) warn_adresse_empty(df) Signale les arr\u00eat\u00e9s sans aucune adresse. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: mainlev\u00e9e, abrogation), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que l'adresse ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune adresse n'a \u00e9t\u00e9 extraite, est relativement \u00e9lev\u00e9e. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def warn_adresse_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s sans aucune adresse. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: mainlev\u00e9e, abrogation), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que l'adresse ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune adresse n'a \u00e9t\u00e9 extraite, est relativement \u00e9lev\u00e9e. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" # r\u00e9cup\u00e9rer toutes les adresses df[\"aucune_adresse\"] = df.apply( lambda row: 1 if pd.isnull(row.ad_brute) else 0, axis=1 ) return df warn_par_ref_cad_empty(df) Signale les arr\u00eat\u00e9s sans aucune r\u00e9f\u00e9rence de parcelle cadastrale. Certains arr\u00eat\u00e9s ne contiennent pas de r\u00e9f\u00e9rence cadastrale, auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs r\u00e9f\u00e9rences cadastrales que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que la r\u00e9f\u00e9rence ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune r\u00e9f\u00e9rence n'a \u00e9t\u00e9 extraite, est \u00e9lev\u00e9e. Parameters df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def warn_par_ref_cad_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s sans aucune r\u00e9f\u00e9rence de parcelle cadastrale. Certains arr\u00eat\u00e9s ne contiennent pas de r\u00e9f\u00e9rence cadastrale, auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs r\u00e9f\u00e9rences cadastrales que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que la r\u00e9f\u00e9rence ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune r\u00e9f\u00e9rence n'a \u00e9t\u00e9 extraite, est \u00e9lev\u00e9e. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_parcelle\"] = df.apply( lambda row: 1 if pd.isnull(row.ref_cad) else 0, axis=1 ) return df","title":"Quality"},{"location":"Code%20Source/quality/#quality","text":"Fonctions de validation des donn\u00e9es extraites.","title":"Quality"},{"location":"Code%20Source/quality/#src.quality.validate_parses--valide-les-zones-reperees","text":"Tous les en-t\u00eates commencent \u00e0 0 ; Tous les pieds-de-pages terminent \u00e0 la longueur du document ; En-t\u00eate et pied-de-page sont disjoints ;","title":"Valide les zones rep\u00e9r\u00e9es."},{"location":"Code%20Source/quality/#src.quality.validate_parses.drop_no_errors_arr","text":"Supprime les arr\u00eat\u00e9s sans erreur.","title":"drop_no_errors_arr()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.drop_no_errors_arr--parameters","text":"df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.drop_no_errors_arr--returns","text":"df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s sans erreurs. src\\quality\\validate_parses.py def drop_no_errors_arr(df_arr: pd.DataFrame) -> pd.DataFrame: \"\"\"Supprime les arr\u00eat\u00e9s sans erreur. Parameters ---------- df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df_arr: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s sans erreurs. \"\"\" df_arr = df_arr.copy() # if none of the keys is equal to 1, then the row has no error df_arr[\"has_error\"] = df_arr[ERROR_KEYS].sum(axis=1) > 0 # delete all the row without error df_arr = df_arr[df_arr[\"has_error\"]] return df_arr","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_classe_manquante","text":"Signale les arr\u00eat\u00e9s dont la classe n'a pu \u00eatre d\u00e9termin\u00e9e. Les causes les plus fr\u00e9quentes sont une erreur d'OCR sur un document mal num\u00e9ris\u00e9, ou une mise en page du document sur plusieurs colonnes qui n'est pas explicitement g\u00e9r\u00e9e par les scripts actuels, et dont le r\u00e9sultat ne permet pas la reconnaissance des motifs recherch\u00e9s.","title":"error_classe_manquante()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_classe_manquante--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_classe_manquante--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_classe_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont la classe n'a pu \u00eatre d\u00e9termin\u00e9e. Les causes les plus fr\u00e9quentes sont une erreur d'OCR sur un document mal num\u00e9ris\u00e9, ou une mise en page du document sur plusieurs colonnes qui n'est pas explicitement g\u00e9r\u00e9e par les scripts actuels, et dont le r\u00e9sultat ne permet pas la reconnaissance des motifs recherch\u00e9s. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_classe\"] = df.apply( lambda row: 1 if pd.isnull(row.classe) else 0, axis=1 ) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_codeinsee_13055","text":"Signale les arr\u00eat\u00e9s dont le code INSEE est 13055. 13055 est le code pour tout Marseille, alors que l'on devrait avoir le code propre \u00e0 l'arrondissement (13201 \u00e0 13216). Ignore les valeurs manquantes.","title":"error_codeinsee_13055()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_codeinsee_13055--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_codeinsee_13055--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_codeinsee_13055(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont le code INSEE est 13055. 13055 est le code pour tout Marseille, alors que l'on devrait avoir le code propre \u00e0 l'arrondissement (13201 \u00e0 13216). Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"codeinsee_13055\"] = df.apply( lambda row: 1 if not pd.isnull(row.codeinsee) and row.codeinsee == \"13055\" else 0, axis=1, ) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_codeinsee_manquant","text":"Signale les arr\u00eat\u00e9s dont le code INSEE est manquant. Le code INSEE est d\u00e9termin\u00e9 sur base du nom de la commune, crois\u00e9 avec la table des codes communes dans data/external/ (actuellement restreint au p\u00e9rim\u00e8tre de la m\u00e9tropole Aix-Marseille Provence).","title":"error_codeinsee_manquant()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_codeinsee_manquant--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_codeinsee_manquant--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_codeinsee_manquant(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont le code INSEE est manquant. Le code INSEE est d\u00e9termin\u00e9 sur base du nom de la commune, crois\u00e9 avec la table des codes communes dans data/external/ (actuellement restreint au p\u00e9rim\u00e8tre de la m\u00e9tropole Aix-Marseille Provence). Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucun_codeinsee\"] = df.apply( lambda row: 1 if pd.isnull(row.codeinsee) else 0, axis=1 ) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_cpostal_manquant","text":"Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes.","title":"error_cpostal_manquant()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_cpostal_manquant--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_cpostal_manquant--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_cpostal_manquant(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucun_cpostal\"] = df.apply( lambda row: 1 if pd.isnull(row.cpostal) else 0, axis=1 ) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_date_manquante","text":"Signale les arr\u00eat\u00e9s dont la date n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une erreur d'OCR sur une date manuscrite ou tamponn\u00e9e, ou un document mal num\u00e9ris\u00e9 ; il est possible que le script \u00e9choue \u00e0 extraire la date dans certaines tournures de r\u00e9daction.","title":"error_date_manquante()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_date_manquante--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_date_manquante--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_date_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont la date n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une erreur d'OCR sur une date manuscrite ou tamponn\u00e9e, ou un document mal num\u00e9ris\u00e9 ; il est possible que le script \u00e9choue \u00e0 extraire la date dans certaines tournures de r\u00e9daction. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_date\"] = df.apply(lambda row: 1 if pd.isnull(row.date) else 0, axis=1) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_num_voie_manquant","text":"Signale les adresses d'arr\u00eat\u00e9s sans num\u00e9ro de voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement ou totalement. Ignore les valeurs manquantes.","title":"error_num_voie_manquant()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_num_voie_manquant--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_num_voie_manquant--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_num_voie_manquant(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans num\u00e9ro de voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement ou totalement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucun_num_voie\"] = df.apply(lambda row: 1 if pd.isnull(row.num) else 0, axis=1) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_urgence_manquante","text":"Signale les arr\u00eat\u00e9s dont l'urgence n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une classe d'arr\u00eat\u00e9 qui ne donne pas explicitement cette information.","title":"error_urgence_manquante()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_urgence_manquante--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_urgence_manquante--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_urgence_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s dont l'urgence n'a pu \u00eatre d\u00e9termin\u00e9e. La cause la plus fr\u00e9quente est une classe d'arr\u00eat\u00e9 qui ne donne pas explicitement cette information. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"manque_urgence\"] = df.apply( lambda row: 1 if pd.isnull(row.urgence) else 0, axis=1 ) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_ville_manquante","text":"Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes.","title":"error_ville_manquante()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_ville_manquante--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_ville_manquante--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_ville_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans ville. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), ou pas d'adresse incluant la ville, auquel cas la ville est d\u00e9termin\u00e9e selon d'autres indices (ex: lieu de signature), sinon recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_ville\"] = df.apply(lambda row: 1 if pd.isnull(row.ville) else 0, axis=1) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_voie_manquante","text":"Signale les adresses d'arr\u00eat\u00e9s sans voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes.","title":"error_voie_manquante()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_voie_manquante--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.error_voie_manquante--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def error_voie_manquante(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les adresses d'arr\u00eat\u00e9s sans voie. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: certaines mainlev\u00e9es ou abrogations), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer ou \u00e0 analyser correctement. Ignore les valeurs manquantes. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_voie\"] = df.apply(lambda row: 1 if pd.isnull(row.voie) else 0, axis=1) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.examine_doc_content","text":"V\u00e9rifie des hypoth\u00e8ses de bonne formation sur le contenu extrait du document.","title":"examine_doc_content()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.examine_doc_content--parameters","text":"doc_content: list[dict] Empans de contenu extraits du document src\\quality\\validate_parses.py def examine_doc_content(fn_pdf: str, doc_content: \"list[dict]\"): \"\"\"V\u00e9rifie des hypoth\u00e8ses de bonne formation sur le contenu extrait du document. Parameters ---------- doc_content: list[dict] Empans de contenu extraits du document \"\"\" # filtrer les pages absentes pg_conts = [x for x in doc_content if (pd.notna(x) and x[\"content\"] is not None)] # paragraphes pars = [ x for pg_content in pg_conts for x in pg_content[\"content\"] if (pd.notna(x) and x[\"span_typ\"].startswith(\"par_\")) ] par_typs = [x[\"span_typ\"] for x in pars] # \"consid\u00e9rant\" obligatoire sauf pour certains arr\u00eat\u00e9s? # TODO d\u00e9terminer si les assertions ne s'appliquent qu'\u00e0 certaines classes d'arr\u00eat\u00e9s if par_typs: # chaque arr\u00eat\u00e9 contient au moins un \"vu\" if \"par_vu\" not in par_typs: logging.warning( f\"{fn_pdf}: pas de 'vu' trouv\u00e9 (v\u00e9rifier la nature du document ?)\" ) # chaque arr\u00eat\u00e9 contient au moins un \"consid\u00e9rant\" # * sauf dans les mainlev\u00e9es et abrogations o\u00f9 dans la pratique ce n'est pas le cas if \"par_considerant\" not in par_typs: # FIXME d\u00e9tecter la classe => ne pas appliquer pour abrogations et mainlev\u00e9es logging.warning( f\"{fn_pdf}: pas de 'consid\u00e9rant' trouv\u00e9 (v\u00e9rifier la nature du document ?)\" ) # chaque arr\u00eat\u00e9 contient exactement 1 \"Arr\u00eate\" try: assert len([x for x in par_typs if x == \"par_arrete\"]) == 1 except AssertionError: logging.warning( f\"{fn_pdf}: pas de 'Arr\u00eate' trouv\u00e9 (v\u00e9rifier la qualit\u00e9 de l'OCR ?)\" )","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.expect_footer_end_len","text":"V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9).","title":"expect_footer_end_len()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.expect_footer_end_len--parameters","text":"df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.expect_footer_end_len--returns","text":"success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. src\\quality\\validate_parses.py def expect_footer_end_len(df: pd.DataFrame) -> bool: \"\"\"V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9). Parameters ---------- df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents. Returns ------- success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. \"\"\" return (df[\"footer_end\"].dropna() == len()).all()","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.expect_header_beg_zero","text":"V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9).","title":"expect_header_beg_zero()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.expect_header_beg_zero--parameters","text":"df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.expect_header_beg_zero--returns","text":"success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. src\\quality\\validate_parses.py def expect_header_beg_zero(df: pd.DataFrame) -> bool: \"\"\"V\u00e9rifie que les en-t\u00eates commencent tous \u00e0 0. Ignore les valeurs manquantes (aucun en-t\u00eate d\u00e9tect\u00e9). Parameters ---------- df: pd.DataFrame DataFrame contenant les zones rep\u00e9r\u00e9es dans les documents. Returns ------- success: bool True si tous les en-t\u00eates d\u00e9tect\u00e9s commencent \u00e0 0. \"\"\" return (df[\"header_beg\"].dropna() == 0).all()","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.generate_html_report","text":"G\u00e9n\u00e9rer un rapport d'erreurs en HTML","title":"generate_html_report()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.generate_html_report--parameters","text":"run: string Identifiant de l'ex\u00e9cution df_adr: pd.DataFrame Adresses df_arr: pd.DataFrame Arr\u00eat\u00e9s df_not: pd.DataFrame Notifi\u00e9s df_par: pd.DataFrame Parcelles","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.generate_html_report--returns","text":"html_report: string Rapport HTML src\\quality\\validate_parses.py def generate_html_report( run: str, df_adr: pd.DataFrame, df_arr: pd.DataFrame, df_not: pd.DataFrame, df_par: pd.DataFrame, ) -> str: \"\"\"G\u00e9n\u00e9rer un rapport d'erreurs en HTML Parameters ---------- run: string Identifiant de l'ex\u00e9cution df_adr: pd.DataFrame Adresses df_arr: pd.DataFrame Arr\u00eat\u00e9s df_not: pd.DataFrame Notifi\u00e9s df_par: pd.DataFrame Parcelles Returns ------- html_report: string Rapport HTML \"\"\" # --- TODO bricolage pour fusionner en aval --- # # Merge dataframes based on 'idu' column merged_df = ( df_adr.merge(df_arr, on=\"idu\", how=\"outer\", suffixes=(\"_df1\", \"_df2\")) .merge(df_not, on=\"idu\", how=\"outer\", suffixes=(\"_df12\", \"_df3\")) .merge(df_par, on=\"idu\", how=\"outer\", suffixes=(\"_df123\", \"_df4\")) ) # fuse duplicate columns for index, row in merged_df.iterrows(): for col in merged_df.columns: # Check if the column matches the pattern r\".*_df.*\" if re.match(r\".*_df\\d+\", col): # Extract the column name excluding the \"df\" number col_name = re.sub(r\"_df\\d+\", \"\", col) # Check if there are other columns with the same name excluding the \"df\" number matching_cols = [ c for c in merged_df.columns if re.match(col_name + r\"_df\\d+\", c) ] # Find the first non-null value among the matching columns non_null_cell = next( (c for c in matching_cols if row[c] is not None), None ) # Assign the value from the first non-null column to the corresponding col_name column merged_df.at[index, col_name] = row[non_null_cell] # Drop the other matching columns merged_df.drop( [c for c in matching_cols if c != non_null_cell], axis=1, inplace=True, ) # drop row with duplicate idu merged_df.drop_duplicates(subset=\"idu\", keep=\"first\", inplace=True) # reset ID merged_df.reset_index(drop=True, inplace=True) nb_arretes = len(merged_df) # options de mise en forme render_links = True res = [] # d\u00e9but et bloc de titre res.append(\"<html>\") res.append(f\"<title>Rapport d'erreurs {run}</title>\") res.append(f\"<h1>Rapport d'erreurs {run}</h1>\") # informations g\u00e9n\u00e9rales sur le lot analys\u00e9 res.append(\"<div>\") res.append(f\"Nombre d'arr\u00eat\u00e9s analys\u00e9s: {nb_arretes}\") res.append(\"</div>\") # adding error columns to the dataframe, 1 = there is an error, 0 = no error # --- aucune adresse --- # merged_df = warn_adresse_empty(merged_df) # --- aucune parcelle --- # merged_df = warn_par_ref_cad_empty(merged_df) # --- aucune date --- # merged_df = error_date_manquante(merged_df) # --- aucune classe --- # merged_df = error_classe_manquante(merged_df) # --- aucune voie --- # merged_df = error_voie_manquante(merged_df) # --- aucun code postal --- # merged_df = error_cpostal_manquant(merged_df) # --- aucune ville --- # merged_df = error_ville_manquante(merged_df) # --- aucun code INSEE --- # merged_df = error_codeinsee_manquant(merged_df) # --- code INSEE 13055 --- # merged_df = error_codeinsee_13055(merged_df) # --- aucun num\u00e9ro de voie --- # merged_df = error_num_voie_manquant(merged_df) # --- manquance_urgence --- # merged_df = error_urgence_manquante(merged_df) # drop rows without errors merged_df = drop_no_errors_arr(merged_df) # only keep the columns we want to display merged_df = merged_df[[\"idu\", *ERROR_KEYS, \"url\"]] res.append(\"<h1>Infos manquantes</h1>\") \"\"\" ## points d'attention # plusieurs parcelles # FL: 2023-06-29: inutile? # plusieurs adresses # FL: 2023-06-29: inutile? \"\"\" # --- adding red to errors --- # # Apply the styling to the DataFrame styled_df = merged_df.applymap(highlight_value_red) # remove any 0 and make the cell empty instead styled_df = styled_df.replace(0, \"\") # Convert the styled DataFrame to HTML with red highlighting styled_df = styled_df.rename_axis(\"id\", axis=1) html_table = styled_df.to_html(escape=False, render_links=render_links) res.append(html_table) # fin du document res.append(\"</html>\") return \"\\n\".join(res)","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.warn_adresse_empty","text":"Signale les arr\u00eat\u00e9s sans aucune adresse. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: mainlev\u00e9e, abrogation), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que l'adresse ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune adresse n'a \u00e9t\u00e9 extraite, est relativement \u00e9lev\u00e9e.","title":"warn_adresse_empty()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.warn_adresse_empty--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.warn_adresse_empty--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def warn_adresse_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s sans aucune adresse. Certains arr\u00eat\u00e9s ne contiennent pas d'adresse (ex: mainlev\u00e9e, abrogation), auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs adresses que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que l'adresse ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune adresse n'a \u00e9t\u00e9 extraite, est relativement \u00e9lev\u00e9e. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" # r\u00e9cup\u00e9rer toutes les adresses df[\"aucune_adresse\"] = df.apply( lambda row: 1 if pd.isnull(row.ad_brute) else 0, axis=1 ) return df","title":"Returns"},{"location":"Code%20Source/quality/#src.quality.validate_parses.warn_par_ref_cad_empty","text":"Signale les arr\u00eat\u00e9s sans aucune r\u00e9f\u00e9rence de parcelle cadastrale. Certains arr\u00eat\u00e9s ne contiennent pas de r\u00e9f\u00e9rence cadastrale, auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs r\u00e9f\u00e9rences cadastrales que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que la r\u00e9f\u00e9rence ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune r\u00e9f\u00e9rence n'a \u00e9t\u00e9 extraite, est \u00e9lev\u00e9e.","title":"warn_par_ref_cad_empty()"},{"location":"Code%20Source/quality/#src.quality.validate_parses.warn_par_ref_cad_empty--parameters","text":"df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s.","title":"Parameters"},{"location":"Code%20Source/quality/#src.quality.validate_parses.warn_par_ref_cad_empty--returns","text":"df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. src\\quality\\validate_parses.py def warn_par_ref_cad_empty(df: pd.DataFrame) -> pd.DataFrame: \"\"\"Signale les arr\u00eat\u00e9s sans aucune r\u00e9f\u00e9rence de parcelle cadastrale. Certains arr\u00eat\u00e9s ne contiennent pas de r\u00e9f\u00e9rence cadastrale, auquel cas cette information doit \u00eatre recherch\u00e9e puis renseign\u00e9e manuellement. D'autres arr\u00eat\u00e9s contiennent une ou plusieurs r\u00e9f\u00e9rences cadastrales que les scripts \u00e9chouent \u00e0 rep\u00e9rer. Ignore les valeurs manquantes. C'est une erreur pour l'utilisateur final mais un warning du point de vue du script, car la probabilit\u00e9 que la r\u00e9f\u00e9rence ne soit pas dans l'arr\u00eat\u00e9, sachant qu'aucune r\u00e9f\u00e9rence n'a \u00e9t\u00e9 extraite, est \u00e9lev\u00e9e. Parameters ---------- df: pd.DataFrame DataFrame contenant les arr\u00eat\u00e9s. Returns ------- df: pd.DataFrame DataFrame contenant avec une colonne indiquant si cette erreur est pr\u00e9sente. \"\"\" df[\"aucune_parcelle\"] = df.apply( lambda row: 1 if pd.isnull(row.ref_cad) else 0, axis=1 ) return df","title":"Returns"},{"location":"Code%20Source/utils/","text":"Utils Diverses fonctions utilitaires. Fonctions utiles pour la gestion des fichiers. get_file_digest(fp_pdf, digest='blake2b', digest_size=10) Extraire le hachage d'un fichier avec la fonction digest . Fonctionne pour Python >= 3.8, mais le code pourra \u00eatre simplifi\u00e9 pour Python >= 3.11 quand ce sera la version minimale requise par les principaux projets. Parameters fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Nom de la fonction de hachage \u00e0 utiliser, \"sha1\" par d\u00e9faut. digest_size : int Taille du digest (blake2b, sinon ignor\u00e9). Returns fd_hexdigest : str Hachage du fichier. src\\utils\\file_utils.py def get_file_digest( fp_pdf: Path, digest: str = \"blake2b\", digest_size: int = 10 ) -> str: \"\"\"Extraire le hachage d'un fichier avec la fonction `digest`. Fonctionne pour Python >= 3.8, mais le code pourra \u00eatre simplifi\u00e9 pour Python >= 3.11 quand ce sera la version minimale requise par les principaux projets. Parameters ---------- fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Nom de la fonction de hachage \u00e0 utiliser, \"sha1\" par d\u00e9faut. digest_size : int Taille du digest (blake2b, sinon ignor\u00e9). Returns ------- fd_hexdigest : str Hachage du fichier. \"\"\" with open(fp_pdf, mode=\"rb\") as f: # python >= 3.8 if digest == \"blake2b\": # constructeur direct privil\u00e9gi\u00e9 car plus rapide (doc module hashlib) # et digest_size configurable pour les algos blake2 f_digest = hashlib.blake2b(digest_size=digest_size) else: f_digest = hashlib.new(digest) while chunk := f.read(8192): f_digest.update(chunk) # alternative en 1 ligne (remplace les 3 lignes pr\u00e9c\u00e9dentes) pour python >= 3.11: # f_digest = hashlib.file_digest(f, digest) fd_hexdigest = f_digest.hexdigest() return fd_hexdigest Reconnaissance et mise en forme des dates. process_date_brute(arr_date) Extraire les diff\u00e9rents champs d'une date brute et la normaliser. Parameters arr_date: str Date brute Returns arr_date_norm: str Date normalis\u00e9e dd/mm/yyyy src\\utils\\str_date.py def process_date_brute(arr_date: str) -> Dict: \"\"\"Extraire les diff\u00e9rents champs d'une date brute et la normaliser. Parameters ---------- arr_date: str Date brute Returns ------- arr_date_norm: str Date normalis\u00e9e dd/mm/yyyy \"\"\" if m_date_p := P_DATE_PREC.search(arr_date): m_dict = m_date_p.groupdict() # traitement sp\u00e9cifique pour le mois, qui peut \u00eatre \u00e9crit en lettres mm_norm = MAP_MOIS.get( m_dict[\"mm\"].lower().replace(\"\u00e9\", \"e\").replace(\"\u00fb\", \"u\"), m_dict[\"mm\"] ) # TODO ajouter au log une erreur si la date est incorrecte return f\"{m_dict['dd']:>02}/{mm_norm:>02}/{m_dict['yyyy']}\" else: return None Fonctions utilitaires g\u00e9n\u00e9riques pour le texte. normalize_string(raw_str, num=False, apos=False, hyph=False, spaces=False) Normaliser une cha\u00eene de caract\u00e8res. Remplacer les s\u00e9quences d'espaces par une unique espace. Parameters raw_str: str Cha\u00eene de caract\u00e8res \u00e0 normaliser Returns nor_str: str Cha\u00eene de caract\u00e8res normalis\u00e9e src\\utils\\text_utils.py def normalize_string( raw_str: str, num: bool = False, apos: bool = False, hyph: bool = False, spaces: bool = False, ) -> str: \"\"\"Normaliser une cha\u00eene de caract\u00e8res. Remplacer les s\u00e9quences d'espaces par une unique espace. Parameters ---------- raw_str: str Cha\u00eene de caract\u00e8res \u00e0 normaliser Returns ------- nor_str: str Cha\u00eene de caract\u00e8res normalis\u00e9e \"\"\" nor_str = raw_str if num: # graphies de \"num\u00e9ro\" nor_str = re.sub(RE_NO, \"n\u00b0\", nor_str, flags=re.MULTILINE | re.IGNORECASE) if hyph: # remplace les tirets (hyphens) par un \"en dash\" et les moins (minus) par un simple \"-\" # observation sur le stock: les PDF texte contiennent g\u00e9n\u00e9ralement des \"en dash\" (\\u2013), # les PDF OCRis\u00e9s contiennent \"em dash\" (\\u2014) for hyphen in HYPHENS: nor_str = nor_str.replace(hyphen, \"-\") # was: \"\u2013\") # \\u2013 en dash for minus in MINUSES: nor_str = nor_str.replace(minus, \"-\") # supprimer les \"soft hyphen\" (qui sont invisibles au rendu) nor_str = nor_str.replace(\"\\u00ad\", \"\") if apos: # apostrophes for single_quote in APOSTROPHES: nor_str = nor_str.replace(single_quote, \"'\") # \\u0027 # et supprimer les \u00e9ventuels espaces inutiles apr\u00e8s une apostrophe nor_str = re.sub(r\"[']\\s*\", \"'\", nor_str, flags=re.MULTILINE) # end TODO if spaces: # remplacer toutes les suites d'espaces (de tous types) par une espace simple nor_str = re.sub(r\"\\s+\", \" \", nor_str, flags=re.MULTILINE).strip() return nor_str remove_accents(str_in) Enl\u00e8ve les accents d'une cha\u00eene de caract\u00e8res. cf. https://stackoverflow.com/a/517974 Parameters str_in: string Cha\u00eene de caract\u00e8res pouvant contenir des caract\u00e8res combinants (accents, c\u00e9dille etc.). Returns str_out: string Cha\u00eene de caract\u00e8res sans caract\u00e8re combinant. src\\utils\\text_utils.py def remove_accents(str_in: str) -> str: \"\"\"Enl\u00e8ve les accents d'une cha\u00eene de caract\u00e8res. cf. <https://stackoverflow.com/a/517974> Parameters ---------- str_in: string Cha\u00eene de caract\u00e8res pouvant contenir des caract\u00e8res combinants (accents, c\u00e9dille etc.). Returns ------- str_out: string Cha\u00eene de caract\u00e8res sans caract\u00e8re combinant. \"\"\" nfkd_form = unicodedata.normalize(\"NFKD\", str_in) return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)]) Charger les fichiers de texte extraits des PDF, natifs ou non. load_pages_text(fp_txt, page_break='\\x0c') Charge le texte d'un document, d\u00e9coup\u00e9 en pages. Parameters fp_txt: Path Chemin du fichier contenant le texte d'un document. page_break: string, defaults to \" \" S\u00e9parateur de pages. Les fichiers PDF texte produits par l'export direct depuis les logiciels de traitement de texte contiennent d\u00e9j\u00e0 un \"form feed\" (\" \"), comme les fichiers \"sidecar\" produits par ocrmypdf (pour les fichiers PDF image). Returns doc_txt: List[str] Texte du document, par page. src\\utils\\txt_format.py def load_pages_text(fp_txt: Path, page_break: str = \"\\f\") -> List[str]: \"\"\"Charge le texte d'un document, d\u00e9coup\u00e9 en pages. Parameters ---------- fp_txt: Path Chemin du fichier contenant le texte d'un document. page_break: string, defaults to \"\\f\" S\u00e9parateur de pages. Les fichiers PDF texte produits par l'export direct depuis les logiciels de traitement de texte contiennent d\u00e9j\u00e0 un \"form feed\" (\"\\f\"), comme les fichiers \"sidecar\" produits par ocrmypdf (pour les fichiers PDF image). Returns ------- doc_txt: List[str] Texte du document, par page. \"\"\" with open(fp_txt) as f_txt: doc_txt = f_txt.read().split(page_break) return doc_txt","title":"Utils"},{"location":"Code%20Source/utils/#utils","text":"Diverses fonctions utilitaires.","title":"Utils"},{"location":"Code%20Source/utils/#src.utils.file_utils--fonctions-utiles-pour-la-gestion-des-fichiers","text":"","title":"Fonctions utiles pour la gestion des fichiers."},{"location":"Code%20Source/utils/#src.utils.file_utils.get_file_digest","text":"Extraire le hachage d'un fichier avec la fonction digest . Fonctionne pour Python >= 3.8, mais le code pourra \u00eatre simplifi\u00e9 pour Python >= 3.11 quand ce sera la version minimale requise par les principaux projets.","title":"get_file_digest()"},{"location":"Code%20Source/utils/#src.utils.file_utils.get_file_digest--parameters","text":"fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Nom de la fonction de hachage \u00e0 utiliser, \"sha1\" par d\u00e9faut. digest_size : int Taille du digest (blake2b, sinon ignor\u00e9).","title":"Parameters"},{"location":"Code%20Source/utils/#src.utils.file_utils.get_file_digest--returns","text":"fd_hexdigest : str Hachage du fichier. src\\utils\\file_utils.py def get_file_digest( fp_pdf: Path, digest: str = \"blake2b\", digest_size: int = 10 ) -> str: \"\"\"Extraire le hachage d'un fichier avec la fonction `digest`. Fonctionne pour Python >= 3.8, mais le code pourra \u00eatre simplifi\u00e9 pour Python >= 3.11 quand ce sera la version minimale requise par les principaux projets. Parameters ---------- fp_pdf : Path Chemin du fichier PDF \u00e0 traiter. digest : str Nom de la fonction de hachage \u00e0 utiliser, \"sha1\" par d\u00e9faut. digest_size : int Taille du digest (blake2b, sinon ignor\u00e9). Returns ------- fd_hexdigest : str Hachage du fichier. \"\"\" with open(fp_pdf, mode=\"rb\") as f: # python >= 3.8 if digest == \"blake2b\": # constructeur direct privil\u00e9gi\u00e9 car plus rapide (doc module hashlib) # et digest_size configurable pour les algos blake2 f_digest = hashlib.blake2b(digest_size=digest_size) else: f_digest = hashlib.new(digest) while chunk := f.read(8192): f_digest.update(chunk) # alternative en 1 ligne (remplace les 3 lignes pr\u00e9c\u00e9dentes) pour python >= 3.11: # f_digest = hashlib.file_digest(f, digest) fd_hexdigest = f_digest.hexdigest() return fd_hexdigest","title":"Returns"},{"location":"Code%20Source/utils/#src.utils.str_date--reconnaissance-et-mise-en-forme-des-dates","text":"","title":"Reconnaissance et mise en forme des dates."},{"location":"Code%20Source/utils/#src.utils.str_date.process_date_brute","text":"Extraire les diff\u00e9rents champs d'une date brute et la normaliser.","title":"process_date_brute()"},{"location":"Code%20Source/utils/#src.utils.str_date.process_date_brute--parameters","text":"arr_date: str Date brute","title":"Parameters"},{"location":"Code%20Source/utils/#src.utils.str_date.process_date_brute--returns","text":"arr_date_norm: str Date normalis\u00e9e dd/mm/yyyy src\\utils\\str_date.py def process_date_brute(arr_date: str) -> Dict: \"\"\"Extraire les diff\u00e9rents champs d'une date brute et la normaliser. Parameters ---------- arr_date: str Date brute Returns ------- arr_date_norm: str Date normalis\u00e9e dd/mm/yyyy \"\"\" if m_date_p := P_DATE_PREC.search(arr_date): m_dict = m_date_p.groupdict() # traitement sp\u00e9cifique pour le mois, qui peut \u00eatre \u00e9crit en lettres mm_norm = MAP_MOIS.get( m_dict[\"mm\"].lower().replace(\"\u00e9\", \"e\").replace(\"\u00fb\", \"u\"), m_dict[\"mm\"] ) # TODO ajouter au log une erreur si la date est incorrecte return f\"{m_dict['dd']:>02}/{mm_norm:>02}/{m_dict['yyyy']}\" else: return None","title":"Returns"},{"location":"Code%20Source/utils/#src.utils.text_utils--fonctions-utilitaires-generiques-pour-le-texte","text":"","title":"Fonctions utilitaires g\u00e9n\u00e9riques pour le texte."},{"location":"Code%20Source/utils/#src.utils.text_utils.normalize_string","text":"Normaliser une cha\u00eene de caract\u00e8res. Remplacer les s\u00e9quences d'espaces par une unique espace.","title":"normalize_string()"},{"location":"Code%20Source/utils/#src.utils.text_utils.normalize_string--parameters","text":"raw_str: str Cha\u00eene de caract\u00e8res \u00e0 normaliser","title":"Parameters"},{"location":"Code%20Source/utils/#src.utils.text_utils.normalize_string--returns","text":"nor_str: str Cha\u00eene de caract\u00e8res normalis\u00e9e src\\utils\\text_utils.py def normalize_string( raw_str: str, num: bool = False, apos: bool = False, hyph: bool = False, spaces: bool = False, ) -> str: \"\"\"Normaliser une cha\u00eene de caract\u00e8res. Remplacer les s\u00e9quences d'espaces par une unique espace. Parameters ---------- raw_str: str Cha\u00eene de caract\u00e8res \u00e0 normaliser Returns ------- nor_str: str Cha\u00eene de caract\u00e8res normalis\u00e9e \"\"\" nor_str = raw_str if num: # graphies de \"num\u00e9ro\" nor_str = re.sub(RE_NO, \"n\u00b0\", nor_str, flags=re.MULTILINE | re.IGNORECASE) if hyph: # remplace les tirets (hyphens) par un \"en dash\" et les moins (minus) par un simple \"-\" # observation sur le stock: les PDF texte contiennent g\u00e9n\u00e9ralement des \"en dash\" (\\u2013), # les PDF OCRis\u00e9s contiennent \"em dash\" (\\u2014) for hyphen in HYPHENS: nor_str = nor_str.replace(hyphen, \"-\") # was: \"\u2013\") # \\u2013 en dash for minus in MINUSES: nor_str = nor_str.replace(minus, \"-\") # supprimer les \"soft hyphen\" (qui sont invisibles au rendu) nor_str = nor_str.replace(\"\\u00ad\", \"\") if apos: # apostrophes for single_quote in APOSTROPHES: nor_str = nor_str.replace(single_quote, \"'\") # \\u0027 # et supprimer les \u00e9ventuels espaces inutiles apr\u00e8s une apostrophe nor_str = re.sub(r\"[']\\s*\", \"'\", nor_str, flags=re.MULTILINE) # end TODO if spaces: # remplacer toutes les suites d'espaces (de tous types) par une espace simple nor_str = re.sub(r\"\\s+\", \" \", nor_str, flags=re.MULTILINE).strip() return nor_str","title":"Returns"},{"location":"Code%20Source/utils/#src.utils.text_utils.remove_accents","text":"Enl\u00e8ve les accents d'une cha\u00eene de caract\u00e8res. cf. https://stackoverflow.com/a/517974","title":"remove_accents()"},{"location":"Code%20Source/utils/#src.utils.text_utils.remove_accents--parameters","text":"str_in: string Cha\u00eene de caract\u00e8res pouvant contenir des caract\u00e8res combinants (accents, c\u00e9dille etc.).","title":"Parameters"},{"location":"Code%20Source/utils/#src.utils.text_utils.remove_accents--returns","text":"str_out: string Cha\u00eene de caract\u00e8res sans caract\u00e8re combinant. src\\utils\\text_utils.py def remove_accents(str_in: str) -> str: \"\"\"Enl\u00e8ve les accents d'une cha\u00eene de caract\u00e8res. cf. <https://stackoverflow.com/a/517974> Parameters ---------- str_in: string Cha\u00eene de caract\u00e8res pouvant contenir des caract\u00e8res combinants (accents, c\u00e9dille etc.). Returns ------- str_out: string Cha\u00eene de caract\u00e8res sans caract\u00e8re combinant. \"\"\" nfkd_form = unicodedata.normalize(\"NFKD\", str_in) return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])","title":"Returns"},{"location":"Code%20Source/utils/#src.utils.txt_format--charger-les-fichiers-de-texte-extraits-des-pdf-natifs-ou-non","text":"","title":"Charger les fichiers de texte extraits des PDF, natifs ou non."},{"location":"Code%20Source/utils/#src.utils.txt_format.load_pages_text","text":"Charge le texte d'un document, d\u00e9coup\u00e9 en pages.","title":"load_pages_text()"},{"location":"Code%20Source/utils/#src.utils.txt_format.load_pages_text--parameters","text":"fp_txt: Path Chemin du fichier contenant le texte d'un document. page_break: string, defaults to \" \" S\u00e9parateur de pages. Les fichiers PDF texte produits par l'export direct depuis les logiciels de traitement de texte contiennent d\u00e9j\u00e0 un \"form feed\" (\" \"), comme les fichiers \"sidecar\" produits par ocrmypdf (pour les fichiers PDF image).","title":"Parameters"},{"location":"Code%20Source/utils/#src.utils.txt_format.load_pages_text--returns","text":"doc_txt: List[str] Texte du document, par page. src\\utils\\txt_format.py def load_pages_text(fp_txt: Path, page_break: str = \"\\f\") -> List[str]: \"\"\"Charge le texte d'un document, d\u00e9coup\u00e9 en pages. Parameters ---------- fp_txt: Path Chemin du fichier contenant le texte d'un document. page_break: string, defaults to \"\\f\" S\u00e9parateur de pages. Les fichiers PDF texte produits par l'export direct depuis les logiciels de traitement de texte contiennent d\u00e9j\u00e0 un \"form feed\" (\"\\f\"), comme les fichiers \"sidecar\" produits par ocrmypdf (pour les fichiers PDF image). Returns ------- doc_txt: List[str] Texte du document, par page. \"\"\" with open(fp_txt) as f_txt: doc_txt = f_txt.read().split(page_break) return doc_txt","title":"Returns"},{"location":"Notebooks/notebooks/","text":"Notebooks TODO","title":"Notebooks"},{"location":"Notebooks/notebooks/#notebooks","text":"TODO","title":"Notebooks"},{"location":"Scripts/scripts/","text":"Scripts Le dossier scripts contient des scripts shell pour automatiser certaines t\u00e2ches. Chacun de ces scripts contient des param\u00e8tres d'entr\u00e9e \u00e0 configurer en d\u00e9but de fichier. Le script principal permettant de lancer tout les traitements : process.sh Plusieurs scripts pour faciliter le nettoyage des donn\u00e9es en cas de probl\u00e8me ou pendant les d\u00e9veloppements : cleanall.sh : supprime les fichiers sources et les fichiers g\u00e9n\u00e9r\u00e9s par les scripts. cleanbest.sh : supprime les fichiers temporaires et les fichiers g\u00e9n\u00e9r\u00e9s par les scripts. cleanfast.sh : supprime uniquement les fichiers temporaires pouvant bloquer les prochains lancements de scripts. cleanpreprocess.sh : supprime les fichiers temporaraires. Des scripts pour lancer les scripts de pr\u00e9traitement des donn\u00e9es : parsebest.sh : lance les scripts de pr\u00e9traitements des donn\u00e9es avec les param\u00e8tres aux meilleures performances. parsefast.sh : lance les scripts de pr\u00e9traitements des donn\u00e9es avec les param\u00e8tres les plus rapides.","title":"Scripts"},{"location":"Scripts/scripts/#scripts","text":"Le dossier scripts contient des scripts shell pour automatiser certaines t\u00e2ches. Chacun de ces scripts contient des param\u00e8tres d'entr\u00e9e \u00e0 configurer en d\u00e9but de fichier. Le script principal permettant de lancer tout les traitements : process.sh Plusieurs scripts pour faciliter le nettoyage des donn\u00e9es en cas de probl\u00e8me ou pendant les d\u00e9veloppements : cleanall.sh : supprime les fichiers sources et les fichiers g\u00e9n\u00e9r\u00e9s par les scripts. cleanbest.sh : supprime les fichiers temporaires et les fichiers g\u00e9n\u00e9r\u00e9s par les scripts. cleanfast.sh : supprime uniquement les fichiers temporaires pouvant bloquer les prochains lancements de scripts. cleanpreprocess.sh : supprime les fichiers temporaraires. Des scripts pour lancer les scripts de pr\u00e9traitement des donn\u00e9es : parsebest.sh : lance les scripts de pr\u00e9traitements des donn\u00e9es avec les param\u00e8tres aux meilleures performances. parsefast.sh : lance les scripts de pr\u00e9traitements des donn\u00e9es avec les param\u00e8tres les plus rapides.","title":"Scripts"}]}